{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a mix of things from Ross & Pekoz, vol1 Karlin & Taylor, and perhaps other sources.  \n",
    "\n",
    "This will be updated, slowly, over time.  A *very* large amount of this is related to Azuma Hoeffding, and Hoeffding's Lemma in particular as your author way unhappy with available proofs on the internet as well as in Ross & Pekoz.  \n",
    "\n",
    "A very long (and tedious) original proof is first given and then under the heading of \"a radical streamlining\" the proof is given in merely a few lines, by a simple insight your author had upon finishing the first proof -- work in logspace and use cumulants, then wield linear lower bound property for convex functions-- problem solved.  Your author has, for now, chosen to leave in the long cumbersome original proof, though as it is frequently helpful for others to leave the 'scaffolding' in place.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azuma Hoeffding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Hoeffding's lemma:**   \n",
    "\n",
    "**note the approach is original, and rather long.  A sketch of key steps is as follows:**  \n",
    "\n",
    "1.) we want to prove, with centered (zero mean) Bernouli random variable $Z= X - \\bar{X}$  \n",
    "$E\\big[e^{tZ}\\big] \\leq \\exp\\big(\\frac{t^2}{8}\\big)$    \n",
    "\n",
    "for $t \\gt 0$, though we'll prove the slightly more general case for $t\\geq 0$  \n",
    "\n",
    "2.)  note that by convexity $1 = \\exp\\big({E[tZ]}\\big) \\leq E\\big[e^{tZ}\\big] $ and because $0\\leq \\frac{t^2}{8}$ we have $1 \\leq \\exp\\big(\\frac{t^2}{8}\\big)$  \n",
    "\n",
    "3.) further note that for $t=0$ we have $E\\big[e^{tZ}\\big] = \\exp\\big(\\frac{t^2}{8}\\big)$ -- i.e. the inequality is an equality and $= 1$ (or in logspace equals 0).  \n",
    "\n",
    "4.) Since both sides are always positive we may choose to work in logspace\n",
    "\n",
    "5.) In essence consider the function $f_p(t) =\\exp\\big(\\frac{t^2}{8}\\big) - E\\big[e^{tZ}\\big]$ and prove it is non-negative for some closed interval including zero -- in this case for $t \\in [0,2]$.  The method used for this will not directly generalize for all positive $t$ but this positive interval is enough to kickstart the process.  Note that $\\exp\\big(\\frac{t^2}{8}\\big)$ is invariant to choice of $p$.  If we want to show that the minimum of $f_p(t)\\geq 0$ for $t \\in [0,2]$ it's enough to do so when selecting the maximizing value of $p$.  We prove that the inequality is satisfied for the \"easy case\" of $q \\in [0, \\frac{1}{2}]$ and then prove that the maximum must be in $q \\in [0, \\frac{1}{2}]$ proves the claim for any choice of $p$ for any $t \\in [0,2]$.  \n",
    "\n",
    "6.)  Taking advantange of positivity, we recognize that we could also 'merely' prove \n",
    "$\\frac{\\exp\\big(\\frac{t^2}{8}\\big)}{E\\big[e^{tZ}\\big]}\\geq 1$.  We've already done this for $t\\in [0,2]$ in step 5 (i.e. both terms are know to be non-negative in step 3, and step 5 shows that the numerator is larger than the denominator at least for $t \\in [0,2]$.  So we work in logspace with cumulants, instead of MGF and focus on proving  \n",
    "\n",
    "$\\phi(t) =  \\log\\Big(\\frac{\\exp \\big(\\frac{t^2}{8}\\big)}{E\\big[e^{tZ}\\big]}\\Big) = \\frac{t^2}{8} - \\log\\Big(E\\big[e^{tZ}\\big]\\Big) = \\frac{t^2}{8} - \\gamma_Z(t)\\geq 0$  \n",
    "\n",
    "We can apply Mean Value Theorem to see that there exists a $u \\in (0,2)$ such that $\\phi'(u) \\geq 0$.  And by work in $5$ we know that for any $u \\in (0,2)$ $\\phi(u) \\geq 0$.  If $\\phi$ is convex, we thus have the makings of a linear lower bound that is non-negative for $t\\geq u$ which includes all of $t\\geq 2$ -- giving us the desired result for all $t\\geq 0$.  \n",
    "\n",
    "7.) To verify the convexity of $\\phi$ we differentiate twice -- some additional maneuvering is needed to tease out the positivity.  \n",
    "\n",
    "- - - - -\n",
    "\n",
    "consider a centered Bernouli random variable $Y = X - \\bar{X} = X - p$  and its negative $Z = -Y$ which is also a centered Bernouli random variable -- which is positive with probability $(1-p)$.  \n",
    "\n",
    "The key idea is, to first tackle $p \\in \\big[\\frac{1}{2}, 1]$ then do the other portion later. \n",
    "\n",
    "So for $t \\gt 0$, \n",
    "\n",
    "though we start by considering $t\\in [0,2]$:  \n",
    "\n",
    "$E\\big[e^{tZ}\\big] = E\\big[e^{-t Y}\\big]$    \n",
    "$=E\\big[f(-t)\\big] $  \n",
    "$=  E\\big[f(0) +f'(0)(-t-0)+ \\frac{f''(0)(-t-0)^2}{2!} + R_3\\big] $  \n",
    "$=  E\\big[f(0) +f'(0)(-t)+ \\frac{f''(0)(t)^2}{2!} + R_3\\big] $  \n",
    "$=  E\\big[f(0)\\big] +E\\big[f'(0)\\big](-t)+ E\\big[f''(0)\\big]\\frac{(t)^2}{2!} + E\\big[R_3\\big]$  \n",
    "$=  1 + 0 + E\\big[Y^2\\big]\\frac{t^2}{2} + E\\big[R_3\\big]$  \n",
    "$=  1 + \\text{var}\\big(Y^2\\big)\\frac{t^2}{2} + E\\big[R_3\\big]$  \n",
    "$\\leq  1 + \\frac{t^2}{8} + E\\big[R_3\\big]$  \n",
    "(by Grues's Inequality)  \n",
    "\n",
    "what we want to show is that for the cubic remainder term for our Taylor Polynomial $E\\big[R_3\\big]\\leq 0$ \n",
    "\n",
    "giving us \n",
    "\n",
    "$E\\big[e^{tZ}\\big] \\leq  1 + \\frac{t^2}{8} + E\\big[R_3\\big] \\leq 1 + \\frac{t^2}{8} \\leq e^{\\frac{t^2}{8}}$  \n",
    "\n",
    "which is the Hoeffding Lemma, finished off by the familiar fact that $1 + s \\leq e^s$  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E\\big[R_3\\big] = E\\big[f'''(u)\\frac{(-t)^3}{3!}\\big]= -\\frac{t^3}{3!} E\\big[f'''(u)\\big]\\leq 0$   \n",
    "for some $ u\\in [-t, 0]$  \n",
    "hence our goal is to prove that \n",
    "\n",
    "$E\\big[f'''(u)\\big] \\leq 0$  \n",
    "**potential cleanup item: is this backwards? should I have said the below?**  \n",
    "$E\\big[f'''(u)\\big] \\geq 0$  \n",
    "\n",
    "so we have \n",
    "\n",
    "$E\\big[f'''(u)\\big] = p\\cdot(-(1-\\bar{X}))^3e^{(1-\\bar {X})u} + (1-p)\\cdot(0--\\bar{X})^3e^{(0--\\bar {X})u}$  \n",
    "$ = -p\\cdot(1-p)^3e^{-(1-p)\\vert u\\vert} + (1-p)\\cdot(p)^3e^{-p\\vert u\\vert}$   \n",
    "$= p(1-p)\\big(-(1-p)^2 e^{-(1-p)\\vert u\\vert} + p^2 e^{-p\\vert u\\vert}\\big)$   \n",
    "$= p(1-p)e^{-p\\vert u\\vert}\\big(-(1-p)^2 e^{(2p-1)\\vert u\\vert} + p^2 \\big)$   \n",
    "$= p(1-p)e^{-p\\vert u\\vert}\\big(p -(1-p) e^{\\frac{1}{2}(2p-1)\\vert u\\vert}  \\big)\\big(p +(1-p) e^{\\frac{1}{2}(2p-1)\\vert u\\vert}  \\big)$   \n",
    "\n",
    "hence the claim reduces to proving  \n",
    "$p(1+e^{\\frac{1}{2}(2p-1)\\vert u\\vert}) -e^{\\frac{1}{2}(2p-1)\\vert u\\vert}= p -(1-p) e^{\\frac{1}{2}(2p-1)\\vert u\\vert}\\geq 0$   \n",
    "\n",
    "or via positive re-scaling, that \n",
    "\n",
    "$ p\\big(e^{\\frac{1}{2}(1-2p)\\vert u\\vert} - \\frac{1}{p}+ 1\\big) \\geq 0$   \n",
    "\n",
    "which, for instance, gives us \n",
    "\n",
    "$ p\\big(e^{\\frac{1}{2}(1-2p)\\vert u\\vert} - \\frac{1}{p}+ 1\\big) \\geq p\\big(1 + \\frac{1}{2}(1-2p)\\vert u\\vert - \\frac{1}{p}+ 1\\big) \\geq p\\big(1 + \\frac{1}{2}(1-2p) - \\frac{1}{p}+ 1\\big)  =  \\big(\\frac{5p}{2}-p^2 - 1 \\big)   \\geq 0$   \n",
    "\n",
    "\n",
    "noting that $\\big(\\frac{5p}{2}-p^2 - 1 \\big) $ is concave with roots at $p = \\frac{1}{2}$ and $p=2$ and hence non-negative for $p \\in [0,1]$.  With $t\\in [0,1]$ we know $\\vert u \\vert \\leq 1$ and have thus proven the desired result.\n",
    "\n",
    "\n",
    "note: because we ran this analysis on $Z = -Y$ we in fact have proven that it holds for $t\\in [0,1]$ for a centered Bernouli with success parameter $q \\in [0, \\frac{1}{2}]$.  \n",
    "\n",
    "*another look: why this approach can't be directly used to prove for all t*  \n",
    "re-visiting the line:  \n",
    "$p(1-p)\\big(-(1-p)^2 e^{-(1-p)\\vert u\\vert} + p^2 e^{-p\\vert u\\vert}\\big)$   \n",
    "\n",
    "we want to show that this is always non-negative for $p\\in [\\frac{1}{2}, 1]$.  Equivalently, we want to show \n",
    "\n",
    "$p^2 e^{-p\\vert u\\vert}\\geq (1-p)^2 e^{-(1-p)\\vert u\\vert}$  \n",
    "\n",
    "or, taking advantage of positivity, we may re-arrange terms to see  \n",
    "\n",
    "$e^{-p\\vert u\\vert}e^{(1-p)\\vert u\\vert}\\geq \\big(\\frac{1-p}{p}\\big)^2 $   \n",
    "$e^{(1-2p)\\vert u\\vert}  \\geq  \\big(\\frac{1-p}{p}\\big)^2 $   \n",
    "\n",
    "since we've constrained ourself to $p \\in [\\frac{1}{2}, 1]$  we have \n",
    "\n",
    "if $p = \\frac{1}{2}$ then we have   \n",
    "$e^{(1-2p)}=e^{(1-2p)\\vert u \\vert} = 1= \\big(\\frac{1-p}{p}\\big)^2 $   \n",
    "\n",
    "for any $\\vert u\\vert$  \n",
    "\n",
    "now for $p \\in (\\frac{1}{2}, 1]$  we see \n",
    "\n",
    "$e^{(1-2p)}\\lt 1$ \n",
    "\n",
    "which qualitatively means that if we set $\\vert u \\vert \\gt 1$ (equivalently: loosening our restriction on the domain of $t$), we have \n",
    "\n",
    "$e^{(1-2p)\\vert u \\vert}\\leq e^{(1-2p)}\\leq 1$  \n",
    "\n",
    "which is a problem, because the Left Hand Side may be made arbitrarily close to zero by changing $u$ while $\\big(\\frac{1-p}{p}\\big)^2$ has some fixed positive value for a given choice of $p$.  Ultimately it tells us a different approach is needed for $t \\gt 1$.  We've thus proven the inequality holds for $t \\in [0,1]$ so long as $p\\in [1, \\frac{1}{2}]$.  \n",
    "\n",
    "- - - \n",
    "**leg 2**  \n",
    "we now seek to prove that $E\\big[e^{tZ}\\big]$ attains a maximum with a probability parameter $q \\in [0, \\frac{1}{2}]$ for any $t \\in [0,1]$.  Note that we have a compact domain (i.e. any $q \\in [0,1]$) and so we know that a maximum exists for any given $t$.  We can see that *any* choice of $q$ is ok for $t=0$ and the desired inequality holds -- i.e. we have $E\\big[e^{tZ}\\big] = 1\\leq 1 = \\exp\\big(\\frac{0^2}{8}\\big)$.  Now for all other choices of $t \\in (0,1]$  we need to prove \n",
    "\n",
    "$M(t) = E\\big[e^{tZ}\\big] \\leq E\\big[e^{tZ^*}\\big]  = \\text{max  } M(t) \\leq \\exp\\big(\\frac{t^2}{8}\\big)$  \n",
    "where $Z^*$ is the centered Bernoui random variable with maximum MGF value for some chosen $t \\in (0,1]$.  Notation could perhaps be cleaned up around here.      \n",
    "\n",
    "Since we know that the maximum exists for any choice of $t \\in (0,1]$, the goal is to prove that this maximum cannot exist for $q \\in (\\frac{1}{2}, 1]$ and hence our earlier bound holds because $Z^*$ must have $q \\in [0, \\frac{1}{2}]$.  \n",
    "\n",
    "- - - - \n",
    "for avodiance of doubt, we have   \n",
    "$g_t(q) = E\\big[e^{tZ}\\big] = q \\cdot e^{t(1-q)} + (1-q) \\cdot e^{-tq}$  \n",
    "\n",
    "$g_t(1) = 1$  \n",
    "$g_t(0) = 1$   \n",
    "but by strict convexity of the exponential map, we know, that for any $t \\in (0,1]$ and $q \\in(0,1)$, we have   \n",
    "$ 1 = \\exp\\Big(t \\cdot 0\\Big) = \\exp\\Big(t \\cdot E\\big[Z\\big]\\Big)= \\exp\\Big(E\\big[tZ\\big]\\Big) \\lt E\\Big[\\exp\\big(tZ\\big)\\Big]=  E\\big[e^{tZ}\\big]$  \n",
    "\n",
    "so the maximum cannot occur with $q =1$ or $q=0$.  \n",
    "\n",
    "Put differently, what this tells us is that for any choice of $t \\gt 0$, \n",
    "\n",
    "$g_t(q)$  attains a maximum at $q \\in(0,1)$, and the maximum occurs when the derivative of $g$ with respect to $q$ is zero.  However we'll show that this derivative is always negative for $q\\in(\\frac{1}{2},1)$ which implies that $q^* \\in (0, \\frac{1}{2}]$  \n",
    "\n",
    "\n",
    "- - - - \n",
    "\n",
    "The above function has two terms and consists of multiplication and the exponential function -- it is differentiable as many times as we wish.  \n",
    "\n",
    "Our plan of attack is to consider $q \\in (\\frac{1}{2}, 1)$  by showing that the derivative with respect to $q$ is negative there, for any given $t$ and hence the function cannot attain a maximum there.  \n",
    "\n",
    "$g_t'(q) = e^{-tp}\\big(-(1-q)t +e^t(1-qt) -1 \\big)$  \n",
    "\n",
    "since $e^{-tp} \\gt 0$  we thus want to show that the component inside the parenthesis is negative for any $t \\gt 0 $ and $q \\in (\\frac{1}{2}, 1)$  \n",
    "\n",
    "$\\big(-(1-q)t +e^t(1-qt) -1 \\big) $  \n",
    "$= \\big(-t+qt +e^t(1-qt) -1 \\big) $  \n",
    "$= \\big(-t-1(1 - qt) +e^t(1-qt)  \\big) $  \n",
    "$= (e^t - 1)(1-qt) -t  $   \n",
    "$\\lt (e^t - 1)(1-\\frac{t}{2}) -t =\\text{upper bound} = h(t)$   \n",
    "\n",
    "i.e. $(e^t - 1)(1-\\frac{t}{2})  \\gt 0$  and monotone decreases as $q$ increases for any choice of $t$, and $q = \\frac{1}{2}$ is the infimum over our domain in consideration. This gives us a clue on why the partition of $q$ into $\\leq \\frac{1}{2}$ and $\\gt \\frac{1}{2}$ matters and it allows us to get rid of our dependence on $q$.    \n",
    "\n",
    "\n",
    "if we consider $t=0$ we have   \n",
    "$h(0) = (e^0 - 1)(1-\\frac{0}{2}) -0 =  0$   \n",
    "$h(1) = (e^1 - 1)(1-\\frac{1}{2}) -1 \\lt 0$   \n",
    "\n",
    "\n",
    "We aim to show that the upper bound is never positive for $t \\gt 0$, because it's maximum occurs at 0.  We show that by examining the derivative of the $h$, showing that it is always negative for $t \\gt 0$, and hence $h(t)$ is never increasing. (Equivalently, for $t \\in [0,1]$ we consider that we have a compact domain of $t \\in[0,1]$ and we know that $h(t)$ obtains a maximum. It is either in the interior where $h'(t) = 0$ for $t \\in(0,1)$ or it is at $\\max\\big(h(0), h(1)\\big) = h(0)$.  But as we show below, $h'(t)\\lt 0$ for all $t \\in(0,1)$ hence the maximum occurs as $h(0) = 0$ -- i.e. we have $h(t) \\leq 0$ for $t \\in [0,1]$.)  \n",
    "\n",
    "for $t \\gt 0$:  \n",
    "$h'(t) = \\frac{1}{2}\\big(-e^t(t-1) -1\\big)$  \n",
    "$= \\frac{1}{2}\\big(e^t(1-t) -1\\big)$  \n",
    "$\\lt \\frac{1}{2}\\big(e^{t-t} -1\\big)$  \n",
    "$=\\frac{1}{2}\\big(1 -1\\big)$  \n",
    "$ = 0$  \n",
    "\n",
    "\n",
    "where we used   \n",
    "$(1+a) \\lt e^a$  for $a \\neq 0$, then rescaled by $e^t$   \n",
    "$e^t(1+a) \\lt e^t e^a = e^{t+a}$  \n",
    "and set $a = -t$   \n",
    "$e^t(1-t) \\lt e^{t-t} = 1$   \n",
    "- - - - \n",
    "This completes the proof that for $0 \\lt t \\leq 1$, for all $q \\in (\\frac{1}{2},1)$  \n",
    "\n",
    "$g_t'(q) = e^{-tp}\\big(-(1-q)t +e^t(1-qt) -1 \\big) \\lt h(t) \\lt 0$  \n",
    "\n",
    "and that $\\text{max  } M(t)$ occurs for any $t\\in (0,1]$ with $q \\in (0, \\frac{1}{2}]$.  And again for avoidance of doubt, with $t=0$. $\\text{max  } M(0)$ occurs for every choice of $q$ including $q \\in (0, \\frac{1}{2}]$ (which is in effect weak dominance).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generalizing the result for  **$t\\gt 1$  \n",
    "Thus far we've proven for $t\\in[0,1]$ \n",
    "\n",
    "$1 \\leq E\\big[\\exp\\big(t Z\\big)\\big]\\leq \\exp\\big(\\frac{t^2}{8}\\big)$  \n",
    "\n",
    "However, it becomes convenient to work in logspace here.   \n",
    "\n",
    "$\\gamma_Z(t) = \\log\\Big(E\\big[\\exp\\big(t Z\\big)\\big]\\Big) = \\log\\Big(E\\big[\\exp\\big(-t \\bar{X}\\big)\\big]\\Big) +\\log\\Big(E\\big[\\exp\\big(t X\\big)\\big]\\Big) = -pt+\\log\\Big(\\big[\\exp\\big(t X\\big)\\big]\\Big) \\leq \\log\\Big(\\exp\\big(\\frac{t^2}{8}\\big)\\Big)=\\frac{t^2}{8}$   \n",
    "\n",
    "- - - - \n",
    "where $-\\bar{X}$ is a deterministic random variable and hence independent from $X$.  By independence, their cumulants add.  \n",
    "\n",
    "$\\gamma_Z(t) = \\gamma_{-\\bar{X}}(t) + \\gamma_X(t)$  \n",
    "- - - - \n",
    "\n",
    "while we have proven this is true for $t \\in[0,1]$, what we want to prove is that for *all* $t$ \n",
    "\n",
    "$\\gamma_Z(t) \\leq \\frac{t^2}{8}$  \n",
    "notice that both sides are convex functions of $t$ -- cumulants always are, and so are quadratics.  \n",
    "\n",
    "re-arranging terms:  \n",
    "$0 \\leq \\frac{t^2}{8}- \\gamma_Z(t)$  \n",
    "\n",
    "\n",
    "\n",
    "We proceed indirectly when we consider the function $\\phi$ given by \n",
    "\n",
    "$\\phi(t) =  \\frac{t^2}{8} - \\gamma_Z(t)$  \n",
    "\n",
    "we know \n",
    "\n",
    "$\\phi(0) = \\log(1) - \\log(1) = 0$  \n",
    "\n",
    "and  \n",
    "\n",
    "$\\phi(1) = \\log\\Big(\\exp\\big(\\frac{1}{8}\\big)\\Big)- \\log\\Big(E\\big[\\exp\\big(t Z\\big)\\big]\\Big) = m \\geq 0$  \n",
    "\n",
    "however, by Mean Value Theorem we know that \n",
    "\n",
    "$0 \\leq m = \\frac{\\phi(1)-\\phi(0)}{1- 0} = \\phi'(u)$  for some $u \\in(0,1)$  \n",
    "\n",
    "but we also know that $\\phi(u) = b\\geq 0$  hence we have \n",
    "\n",
    "$0\\leq  m\\cdot t + b$ \n",
    "\n",
    "for \n",
    "\n",
    "$u \\lt 1 \\leq t$  \n",
    "  \n",
    "The final linkage is, we need to prove that $\\phi$ is convex, and hence the above tangent line at $u$ is a linear lower bound for the entire function.  We've already proven $0\\leq \\phi(t)$ for $t\\in [0,1]$ and this non-negative linear lower bound then extends the non-negative of $\\phi$ to all $t\\in [1, \\infty)$ and the union gives the result for all $t \\geq 0$.  \n",
    "\n",
    "We prove the convexity of $\\phi$ by careful examination of the second derivative. We wish to prove\n",
    "\n",
    "$0 \\leq \\phi''(t) $ or equivalently  \n",
    "$\\gamma_Z''(t)\\leq \\frac{d^2}{dt^2}\\big(\\frac{t^2}{8}\\big) $     \n",
    "\n",
    "After twice differentiating, the right hand side\n",
    "$\\frac{d^2}{dt^2}\\frac{t^2}{8} \\to \\frac{1}{4}$   \n",
    "which is a nice result so characteristic of quadratics:  the second derivative is constant and no longer depends on $t$.  \n",
    "\n",
    "Hence we must prove $\\gamma_Z''(t) \\leq \\frac{1}{4}$   \n",
    "\n",
    "\n",
    "note that by linearity \n",
    "\n",
    "$\\frac{d^2}{dt^2}\\gamma_Z(t) = \\frac{d^2}{dt^2}\\big(\\gamma_{-\\bar{X}}(t) + \\gamma_X(t)\\big)= \\frac{d^2}{dt^2}\\gamma_{-\\bar{X}}(t) + \\frac{d^2}{dt^2}\\gamma_X(t) = \\gamma_X(t)$  \n",
    "\n",
    "(where the first term is linear in $t$ and hence gets mapped to zero.) \n",
    "\n",
    "Differentiating is mechanically straightforward\n",
    "\n",
    "https://www.wolframalpha.com/input/?i=d%5E2%2Fdx%5E2+log(p+exp(x)+%2B(1-p))   \n",
    "\n",
    "(and e.g. in chp9_DiscStochProcesses.ipynb we see a general formula for the second derivative of cumulants)  \n",
    "\n",
    "\n",
    "$\\frac{d^2}{dt^2}\\gamma_Z(t) = \\frac{(1-p)pe^t}{(p(e^t-1)+1)^2}= \\frac{(1-p)pe^t}{(pe^t+1-p)^2}= \\frac{(1-p)pe^t}{\\big((1-p)+pe^t\\big)^2}$  \n",
    "\n",
    "however it is not immediately clear why this must be less than $\\frac{1}{4}$.  \n",
    "\n",
    "Note: the below works through a long messy polynomial factorization to confirm the result.  \n",
    "- - - -  \n",
    "But first an **interlude:** a much better approach is to do a simple sanity check and try to confirm \n",
    "\n",
    "$\\frac{(1-p)pe^t}{\\big( (1-p)+pe^t\\big)^2}\\leq 1$  \n",
    "\n",
    "for $p\\in(0,1)$.  How doe we know this? In essence because of $\\text{GM}\\leq \\text{AM}$.    \n",
    "\n",
    "in particular, consider \n",
    "\n",
    "$0\\leq (1-p)^\\frac{1}{2}(pe^t)^\\frac{1}{2} \\leq \\frac{1}{2}\\big((1-p)+(pe^t)\\big)$  \n",
    "\n",
    "which gives us a way of bounding the numerator in terms of the denominator.  Taking advantage of positivity, we may square both sides to see  \n",
    "\n",
    "$0\\leq (1-p)(pe^t) \\leq \\frac{1}{4}\\big((1-p)+(pe^t)\\big)^2$  \n",
    "and dividing by $\\big((1-p)+(pe^t)\\big)^2$  gives the desired upper bound  \n",
    "\n",
    "$0\\leq \\frac{(1-p)(pe^t)}{\\big((1-p)+(pe^t)\\big)^2} \\leq \\frac{1}{4}$  \n",
    "\n",
    "this completes the proof of the Hoeffding Lemma.  \n",
    "- - - -  \n",
    "**Detour:  A much better finish**  \n",
    "recalling the work through in 'chp9_DiscStochProcesses.ipynb' \n",
    "\n",
    "we can see that \n",
    "$\\frac{d^2}{dt^2}\\gamma_Z(t) =  \\text{var}_g\\big(Z\\big)  = E_g\\big[Z^2\\big] - E_g\\big[Z\\big]^2$  \n",
    "\n",
    "i.e. it is variance with respect to the tilted probability measure of \n",
    "$g(z) = \\frac{f(z)\\exp(tZ)}{E [\\exp(tZ)]}$  \n",
    "\n",
    "instead of $f(x)$ (of course we recover the 'regular' variance by setting $t:=0$).  However, Gruess's Inequality tells us that *any* random variable bounded between $[a, A]$, which in our case, is an interval of length one (i.e. $A-a = 1$), which gives us      \n",
    "\n",
    "$\\frac{d^2}{dt^2}\\gamma_Z(t) = \\text{var}_g\\big(Z\\big) \\leq \\frac{1}{4}\\big(A-a\\big) = \\frac{1}{4}$  \n",
    "\n",
    "This is strongly suggestive that we may refine the bound in Hoeffding's Lemma via the use of a tighter bound on $\\text{var}_g\\big(Z\\big)$ for arbitrary bounded random variables (i.e. Gruess's Inequality is loose if the variable does not have all of its mass concentrated at end points).  \n",
    "\n",
    "# a radical streamlining of the proof  \n",
    "\n",
    "This is also suggestive of a *much* shorter proof of the existing setup.  \n",
    "\n",
    "chp9_DiscStochProcesses.ipynb also tells us  \n",
    "$\\gamma'_X(r) = \\frac{g'_X(r)}{g_X(r)} = E_g\\big[X\\big]$  \n",
    "\n",
    "The streamlined setup is thus:  Verify upper bound and lower bound are $\\geq 1$ in general and $=1$ when $t=0$.  Then work in logspace, using $\\phi(t)$. As a technical nit, since our random variable is bounded, we can verify that $\\phi(t)$ exists for $t \\in (-a,a)$ for some $a \\gt 0$ i.e. while we are interested in $t\\geq 0$ it is useful to extend the domain in question to $(-\\epsilon, \\infty)$ that way we have bonafide derivatives at $t=0$.  \n",
    "\n",
    "we know  \n",
    "$\\phi(0) = 0$  \n",
    "we *also* know \n",
    "\n",
    "$\\phi'(t) = \\frac{t}{4} - \\gamma'_X(r) = \\frac{t}{4} - E_g\\big[X\\big] $\n",
    "\n",
    "and thus  \n",
    "$\\phi'(0) = 0$   \n",
    "\n",
    "But using the above we can easily prove  \n",
    "$\\phi''(t) \\geq 0$ for all $t$   \n",
    "which confirms that $\\phi$ is convex.  \n",
    "\n",
    "This tells us that we have a linear lower bound given by the tangent line \n",
    "\n",
    "$y = mx + b$  \n",
    "\n",
    "where  \n",
    "$m= \\phi'(0) = 0$  \n",
    "$b= \\phi(0)$  \n",
    "hence  \n",
    "$\\phi(t) \\geq 0$ for all $t \\geq 0$  \n",
    "\n",
    "which proves the Hoeffding Lemma.  \n",
    "\n",
    "**end Detour**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other approach to finishing the proof comes from a clever decomposition curtiosy of Wikipedia  \n",
    "\n",
    "$\\frac{d^2}{dt^2}\\gamma_Z(t) = a\\cdot(1-a)  = \\frac{b}{(p(e^t-1)+1)}(1-a) = \\frac{b}{(p(e^t-1)+1)}\\big(1-\\frac{b}{(p(e^t-1)+1)}\\big) $   \n",
    "\n",
    "for some $a \\in (0,1)$  and hence by $\\text{GM}\\leq \\text{AM}$ we know \n",
    "\n",
    "$0\\leq a^\\frac{1}{2}(1-a)^\\frac{1}{2} \\leq \\frac{1}{2}\\big(a + 1-a) = \\frac{1}{2}$  \n",
    "and squaring both sides tells us that   \n",
    "$a(1-a)\\leq \\frac{1}{4}$  \n",
    "\n",
    "so we want to find $a$ via brute force polynomial factorization:  \n",
    "\n",
    "$\\frac{(1-p)pe^t}{(pe^t+1-p)^2} = \\frac{d}{c ^2} = a(1-a) = \\frac{b}{c}\\big(1-\\frac{b}{c}\\big)= \\frac{b}{c}\\big(\\frac{c-b}{c}\\big) $   \n",
    "\n",
    "which tells us that for the factorization to work, \n",
    "\n",
    "$(1-p)pe^t = d = bc - b^2 = b(p(e^t-1)+1) - b^2$    \n",
    "\n",
    "i.e. we need to solve for a positive root of \n",
    "\n",
    "$b^2 - b(p(e^t-1)+1) + (1-p)pe^t$  \n",
    "\n",
    "and confirm $\\frac{b}{c}^* \\in (0,1)$\n",
    "\n",
    "more simply consider \n",
    "\n",
    "$b^2 - bc + d = 0$  \n",
    "\n",
    "thankfully this is a quadratic which means a very simple root structure and it has a real roots at \n",
    "\n",
    "$b^* = \\frac{1}{2}\\big(c-\\sqrt{c^2 - 4d}\\big)$   \n",
    "and  \n",
    "$b^* = \\frac{1}{2}\\big(c+\\sqrt{c^2 - 4d}\\big)$   \n",
    "\n",
    "The symbol manipulations and simplifications are tedious however and sympy was not of much help in doing them.  \n",
    "\n",
    "However wolfram did a better job and \n",
    "https://www.wolframalpha.com/input/?i=(p*(exp(t)+-+1)+%2B+sqrt(-4*p*(-p+%2B+1)*exp(t)+%2B+(p*(exp(t)+-+1)+%2B+1)**2)+%2B+1)%2F2\n",
    "\n",
    "gives the desired result about halfway through.  Of course once we have the result, the messy means of finding it don't matter so much -- we can verify directly via algebraic manipulations that it holds true -- which appears to be what was done on wikipedia.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
