{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "These notes cover a few items from chapters 11, 12, 13 in Bremaud, as well as some associated items in Ash's *Information Theory*.  The general idea is to nail down some bounds and algebraic operations with entropy, with a particular focus on KL Divergence.  \n",
    "\n",
    "The focus is on discrete random variables, including those that may take on a countably infinite number of values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy Definition and Non-negativity\n",
    "\n",
    "For some random variable $X$ with probability distribution vector $\\mathbf q$, we have \n",
    "\n",
    "$H\\big(X\\big) = H\\big(\\mathbf p\\big) = - E\\big[\\log(p(X))\\big] = - \\sum_{x \\in \\mathcal{X}}p(x)\\log\\big(p(x)\\big) = - \\sum_{i}p_i\\log\\big(p_i\\big)$  \n",
    "\n",
    "note: the common convention that $0\\log\\big(0\\big) = 0$ \n",
    "\n",
    "given this, we may safely remove and zero probability items in the above summation /assume Without Loss of Generality that all $p_i \\gt 0$.  This simplifies mentioning of special cases in the above.  \n",
    "\n",
    "We also have:  $-\\alpha \\log\\big(0\\big) = \\infty$ for $\\alpha \\gt 0$.  \n",
    "\n",
    "\n",
    "The base of the logarithm needs to be specified as some particular number $\\gt 1$.  Depending on uses base $2$ and base $e$ are most common and are assumed below.  The result is the same, up to a positive re-scaling of course -- which crucially has no impact on the derived inequalities.  In the event it is important this writeup will distinguish between $2$ and $e$ as a base, though frequently this won't be directly addressed without impacting the inferences to be made.  \n",
    "\n",
    "- - -- \n",
    "\n",
    "\n",
    "To prove non-negativity, we need the following:  \n",
    "\n",
    "for any $p_i \\in (0,1]$ \n",
    "\n",
    "$0 \\lt p_i \\leq 1$  \n",
    "\n",
    "hence \n",
    "$0 \\lt p_i^2 \\leq p_i$  \n",
    "\n",
    "summing over the bound, over all $i$ gives \n",
    "\n",
    "$0 \\lt \\sum_i p_i^2 \\leq \\sum_i p_i = 1$  \n",
    "\n",
    "using applying the real logarithm (and interpretting it's limit from the right as being toward $- \\infty$), we have:  \n",
    "\n",
    "$-\\infty \\lt \\log\\big(\\sum_i p_i^2\\big) \\leq \\log\\big( 1\\big) = 0$  \n",
    "\n",
    "and if we negate the above, we get \n",
    "\n",
    "$  0 \\leq -\\log\\big(\\sum_i p_i^2\\big)  \\lt \\infty$  \n",
    "- - - - \n",
    "returning to our definition we have \n",
    "\n",
    "$0 \\leq -\\log\\big(\\sum_{i}p_i^2\\big) = -\\log\\big(\\sum_{i}p_i\\cdot p_i\\big) = - \\log\\Big(E\\big[p(X)\\big]\\Big) \\leq  - E\\big[\\log(p(X))\\big]  =  E\\big[-\\log(p(X))\\big]  =  H\\big(\\mathbf p\\big) =H\\big(X\\big) $  \n",
    "\n",
    "by application of Jensen's Inequality (given the convexity of the negative logarithm) and the above results.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometric Distribution Has Maximal Entropy for Random Variables distributed on the Positive Integers, given the constraint of having the same (finite) first moment\n",
    "\n",
    "consider two random variables distributed on the posivite integers: $X$ and $Y$.  Each has the same mean \n",
    "\n",
    "$\\bar{X} = \\bar{Y} = \\mu \\lt \\infty$ \n",
    "\n",
    "$X$ its probability distribution vector $\\mathbf p$ and is geometrically distributed \n",
    "(i.e. $p_k = Pr\\{X = k\\} = p_1(1-p_1)^{k-1} $) and \n",
    "\n",
    "$Y$ has distribution vector $\\mathbf q$ but is otherwise arbitrarily distributed.  \n",
    "** this may need tweaked**  \n",
    "\n",
    "apply the KL Divergence \n",
    "\n",
    "\n",
    "$0 \\leq D_{\\text{KL}}\\big(Y\\big \\Vert X\\big) = D\\big(\\mathbf q \\big \\Vert \\mathbf p \\big) = \\sum_{i=1}^\\infty q_i \\log\\big(\\frac{q_i}{p_i}\\big) = \\Big(\\sum_{i=1}^\\infty q_i \\log\\big(q_i\\big) \\Big)- \\Big(\\sum_{i=1}^\\infty q_i \\log\\big(p_i\\big) \\Big) $  \n",
    "\n",
    "** cleanup: did I write the divergence backwards?** \n",
    "\n",
    "hence \n",
    "\n",
    " $H(\\mathbf q\\big) = -\\sum_{i=1}^\\infty q_i \\log\\big(q_i\\big) \\leq -\\sum_{i=1}^\\infty q_i \\log\\big(p_i\\big) = -\\sum_{i=1}^\\infty q_i \\log\\big(p_1(1-p_1)^{i-1}\\big)$   \n",
    " \n",
    " $= -\\Big(\\sum_{i=1}^\\infty q_i \\log\\big(p_1\\big) + \\sum_{i=1}^\\infty q_i \\log\\big((1-p_1)^{i-1}\\big)\\Big) = -\\Big(\\log\\big(p_1\\big)\\cdot\\big(\\sum_{i=1}^\\infty q_i \\big)  +  \\big(\\sum_{i=1}^\\infty q_i (i-1)\\cdot \\log\\big(1-p_1\\big)\\big)\\Big) $  \n",
    " \n",
    " $= -\\Big(\\log\\big(p_1\\big) +  \\log\\big(1-p_1) \\cdot \\sum_{i=1}^\\infty q_i (i-1)\\Big)  = -\\Big(\\log\\big(p_1\\big) +  \\log\\big(1-p_1) \\cdot \\big(\\big(\\sum_{i=1}^\\infty i \\cdot q_i \\big)- \\big(\\sum_{i=1}^\\infty q_i\\big) \\big)\\Big)$  \n",
    " \n",
    " $=-\\Big(\\log\\big(p_1\\big) +  \\log\\big(1-p_1) \\cdot\\big(\\mu - 1\\big)\\Big) $   \n",
    " \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we recall the mean for the geometric distribution is given by:  \n",
    "$\\mu = \\frac{1}{p_1}$  \n",
    "$\\mu - 1 = \\frac{1 - p_1}{p_1}$ \n",
    "\n",
    "thus we have \n",
    "\n",
    "$H\\big(\\mathbf q\\big) \\leq -\\Big(\\log\\big(p_1\\big) +  \\big(\\mu - 1\\big)\\log\\big(1-p_1\\big)\\Big)  = -\\Big(\\frac{p_1}{p_1}\\log\\big(p_1\\big) +  \\frac{1 - p_1}{p_1} \\log\\big(1-p_1\\big)\\Big)= \\frac{-p_1 \\log\\big(p_1\\big) -  (1 - p_1) \\log\\big(1-p_1\\big)}{p_1} $  \n",
    "\n",
    "as will be shown, the right hand side is the entropy for a geometric random variable -- in this case $X$.   \n",
    "\n",
    "Thus the results can be equivalently stated as \n",
    "\n",
    "$H\\big(Y\\big) \\leq H\\big(X\\big)$  \n",
    "\n",
    "*Note 1:* we have not yet derived the entropy for the geometric distribution! However, the below cells under \"Putting it all together\" do the entropy calculation for the geometric distribution.   \n",
    "\n",
    "*Note 2:* A nice insight from above is that it shows that all random variables defined on the positive integers, with a finite first moment, have a finite entropy.  I.e. for any $Z$ meeting the above, we have $0 \\leq H\\big(Z\\big) \\lt \\infty$.    \n",
    "\n",
    "*Note 3:* The above results actually implicitly give the entropy for the geometric distribution.  If we recall that KL divergence over a discrete random variables is zero if and only if the distribution is the same (this is refined subsequently with Pinsker's Inequality) then we can re-run our argument by setting $Y$ to be a geometric random variable with success probability $q_1 = p_1$.  In such a case the inequality becomes an equality and we'd see \n",
    "\n",
    "$ H\\big(Y_{\\text{which has same distribution as X}}\\big) = \\frac{-p_1 \\log\\big(p_1\\big) -  (1 - p_1) \\log\\big(1-p_1\\big)}{p_1}$.  \n",
    "\n",
    "\n",
    "However, the below argument is a useful exercise involving conditional entropy, memorylessness, affine shifts, and so on.  Thus we derive the conditional entropy of the geometric random variable below.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**commentary** \n",
    "\n",
    "an interesting conclusion from the above is that **all** distributions defined on the positive integers have bounded (i.e. finite) entropy if they also have a first moment.  With some additional thought we should see that this also covers all distributions defined on real non-negative integers.  And after more reflection, we see that it includes distributions defined on real-nonegative integers plus finitely many negative integers (and also the 'opposite' of this of real non-positive integers plus finitely many positive integers).  \n",
    "\n",
    "Why?  Recall that $H\\big(X\\big) = H\\big(\\mathbf p\\big)$.  i.e. it is not a function of the values that $X$ takes on, only the underlying distribution.  Thus entropy is invariant to an affine shift and also invariant to rescaling (of course by some finite amount in each case).  Each of these transformations is in the linear/affine family and hence has easily understood impacts with the expectations operator; crucially if the mean is finite before these transformations, it remains so afterward.   \n",
    "\n",
    "*Regarding Geometric Distributions:*  if $X$ is a geometric random variable that counts trials until first success (i.e. for $k= 1, 2, 3, ...$ with probability of success $p$ and $W$ is geomteric, defined on the same probability space, and counts failures until first success (i.e. $k=0,1,2,3,...$ ) again with succes probability $p$, then we have $X = W + 1$.  **An immediate consequence is $X$ and $W$ must have the same entropy**.   \n",
    "\n",
    "*note: if we run an almost identical argument for continuous time random variables defined on real non-negative numbers with a finite first moment, we get the result that the exponential distribution maximizes entropy in that situation.  This is something which may be added to this posting, though the argument really is almost identical*  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Putting it all together: ** \n",
    "\n",
    "we can use an argument similar to a memoryless argument (and reminscent of a coupling argument) to derive the entropy of the geometric distribution i.e. $H\\big(X\\big)$  \n",
    "\n",
    "Consider $V$ a bernouli random variable with success probability $p_1$, $X$ which is geoemtric over positive integers with probability $p$ and $W$ which is geometric if $V{\\omega} = 1$ and otherwise is equal to zero.  All random variables exists in the same probability space and are generated by the same sequence iid bernouli trials (though $V$ only contemplates the first one).   \n",
    "\n",
    "After some thought we see that $W = X - 1$ -- i.e. $X = W +1$ i.e. $W$ is a geometric random variable that counts failures.  Hence \n",
    "\n",
    "$H\\big(W\\big) = H\\big(X\\big) = H\\big(W,V\\big)$  \n",
    "\n",
    "but by definition for conditional entropy, we have:  \n",
    "\n",
    "$H\\big(W,V\\big) = H\\big(V\\big) + H\\big(W\\big \\vert V\\big)$\n",
    "\n",
    "hence   \n",
    "$H\\big(X\\big) = H\\big(V\\big) + H\\big(W\\big \\vert V\\big) = -\\Big(p_1 \\cdot \\log\\big(p_1\\big) + (1-p_1)\\cdot \\log\\big(1-p_1\\big) \\Big) + H\\big(W\\big \\vert V\\big)$   \n",
    "\n",
    "now we have \n",
    "\n",
    "$H\\big(W\\big \\vert V\\big) = \\sum_{i=0}^\\infty \\sum_{k=0}^1 p\\big(w_i, v_k\\big)\\log\\big(w_i\\big \\vert v_k\\big) = \\sum_{i=0}^\\infty p\\big(w_i, v_1\\big)\\log\\big(w_i\\big \\vert v_1\\big)  +  \\sum_{i=0}^\\infty p\\big(w_i, v_0\\big)\\log\\big(w_i\\big \\vert v_0\\big)$  \n",
    "\n",
    "\n",
    "** some of the ideas and notation here need cleaned up a bit **  \n",
    "the second series vanishes -- because with $v_0$ is the probability of $success$ on the first Bernouli trial given by $p_1$, in with case $W=0$ with probability one and $\\log\\big(w_i\\big \\vert v_0\\big) = \\log\\big(1\\big)0$ -- all other terms may be interpretted as vanishing (i.e. the infinite series only exists formally) or may be viewed as identically zero because of our convention $0\\log(0\\big) = 0$.  Thus we have \n",
    "\n",
    "$H\\big(W\\big \\vert V\\big) = \\sum_{i=0}^\\infty p\\big(w_i, v_1\\big)\\log\\big(w_i\\big \\vert v_1\\big) = \\sum_{i=1}^\\infty (1-p_1)p\\big(w_i\\big \\vert v_1 \\big)\\log\\big(w_i\\big \\vert v_1\\big) = (1-p_1)\\sum_{i=1}^\\infty p\\big(w_i\\big \\vert v_1 \\big)\\log\\big(w_i\\big \\vert v_1\\big) $  \n",
    "\n",
    "taking advantage of memorylessness, we may re-write this below as:  \n",
    "\n",
    "$H\\big(W\\big \\vert V\\big) = (1-p_1)\\sum_{i=1}^\\infty p\\big(x_i\\big)\\log\\big(x_i\\big) = (1-p_1)H\\big(X\\big) $  \n",
    "\n",
    "plugging this back into our equation \n",
    "\n",
    "$H\\big(X\\big) = H\\big(V\\big) + H\\big(W\\big \\vert V\\big) = -\\Big(p_1 \\cdot \\log\\big(p_1\\big) + (1-p_1)\\cdot \\log\\big(1-p_1\\big) \\Big) + (1-p_1)H\\big(X\\big)$   \n",
    "\n",
    "\n",
    "$p_1 \\cdot H\\big(X\\big)  = H\\big(X\\big) - (1-p_1)H\\big(X\\big) = -\\Big(p_1 \\cdot \\log\\big(p_1\\big) + (1-p_1)\\cdot \\log\\big(1-p_1\\big) \\Big)$  \n",
    "\n",
    "$H\\big(X\\big)  = -1\\cdot\\frac{p_1 \\cdot \\log\\big(p_1\\big) + (1-p_1)\\cdot \\log\\big(1-p_1\\big)}{p_1} =  \\frac{-p_1 \\cdot \\log\\big(p_1\\big) - (1-p_1)\\cdot \\log\\big(1-p_1\\big)}{p_1}$   \n",
    "\n",
    "\n",
    "**We can thus recognize that entropy of the geoemtric distribution is the same as our entropy upper bound for all probability distributions defined on positive integers that have a finite first moment**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it may be worth running through the arguments on page 7 of \"entropypost_KeithConrad.pdf\"  \n",
    "\n",
    "as they are quite good derivations for normal distribution and note that in particular the exponential distribution (while re-parameterized, is almost a verbatim argument to what I did with the geometric distribution) ... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL Divergence is convex in both of its arguments:  \n",
    "\n",
    "for $p \\in (0,1)$  we hvae \n",
    "\n",
    "**claim**   \n",
    "$D_{\\text{KL}}\\big(p\\cdot \\mathbf w^{(1)} + (1-p) \\mathbf w^{(2)} \\big \\Vert p\\cdot \\mathbf q^{(1)} + (1-p) \\mathbf q^{(2)}\\big) \\leq p \\cdot D_{\\text{KL}}\\big(\\mathbf w^{(1)} \\big \\Vert \\mathbf q^{(1)}\\big) + (1-p)\\cdot D_{\\text{KL}}\\big( \\mathbf w^{(2)} \\big \\Vert  \\mathbf q^{(2)}\\big) $ \n",
    "\n",
    "where $\\mathbf w^{(k)}$ and $\\mathbf q^{(k)}$ are discrete probability distributions \n",
    "\n",
    "We go about proving this via 2 different approaches.  One is to show that the result is implied by the log sum inequality.  The other approach is to examing an assocaited Hessian.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**proof 1**  \n",
    "by the log-sum inequality, we have the identity:  \n",
    "\n",
    "$  \\Big( p \\cdot w_i^{(1)} + (1-p) \\cdot w_i^{(2)}\\Big) \\log \\Big(\\frac{ p \\cdot w_i^{(1)} + (1-p) \\cdot w_i^{(2)} }{ p \\cdot q_i^{(1)} + (1-p) \\cdot q_i^{(2)}}\\Big)  \\leq   p \\cdot w_i^{(1)} \\log \\Big(\\frac{p \\cdot w_i^{(1)} }{p \\cdot q_i^{(1)}}\\Big) +  (1-p) \\cdot w_i^{(2)} \\log \\Big(\\frac{(1-p) \\cdot w_i^{(2)} }{(1-p) \\cdot q_i^{(2)}}\\Big)$  \n",
    "\n",
    "which simplifies to   \n",
    "$\\Big( p \\cdot w_i^{(1)} + (1-p) \\cdot w_i^{(2)}\\Big) \\log \\Big(\\frac{ p \\cdot w_i^{(1)} + (1-p) \\cdot w_i^{(2)} }{ p \\cdot q_i^{(1)} + (1-p) \\cdot q_i^{(2)}}\\Big)   \\leq p \\cdot w_i^{(1)} \\log \\Big(\\frac{ w_i^{(1)} }{  q_i^{(1)}}\\Big) +  (1-p) \\cdot w_i^{(2)} \\log \\Big(\\frac{ w_i^{(2)} }{q_i^{(2)}}\\Big) $ \n",
    "\n",
    "from prior work on the log sum inequality we know that the above inequality is strict unless \n",
    "\n",
    "$\\frac{w_i^{(1)} }{p \\cdot w_i^{(1)} + (1-p) \\cdot w_i^{(2)}} = \\frac{ q_i^{(1)}}{p\\cdot q_i^{(1)} + (1-p) \\cdot q_i^{(2)}}$  \n",
    "\n",
    "and \n",
    "\n",
    "$\\frac{w_i^{(2)} }{p \\cdot w_i^{(1)} + (1-p) \\cdot w_i^{(2)}} = \\frac{ q_i^{(2)}}{p\\cdot q_i^{(1)} + (1-p) \\cdot q_i^{(2)}}$  \n",
    "\n",
    "or where \n",
    "\n",
    "$\\alpha_i := \\frac{p\\cdot q_i^{(1)} + (1-p) \\cdot q_i^{(2)}}{p \\cdot w_i^{(1)} + (1-p) \\cdot w_i^{(2)}}$  \n",
    "\n",
    "we can simplify the equality conditions to   \n",
    "$\\alpha_i \\cdot w_i^{(1)} = q_i^{(1)}$   \n",
    "$\\alpha_i \\cdot w_i^{(2)} = q_i^{(2)}$ \n",
    "\n",
    "however with a little insight, we may see that \n",
    "\n",
    "$\\alpha_i \\cdot\\big( w_i^{(1)} + w_i^{(2)}\\big) = q_i^{(1)}   + q_i^{(2)}$  \n",
    "\n",
    "is trivially true. But it does *not* reveal that $\\alpha$ must be zero.  There is another layer of dimensionality here.  However, there are some easy results, e.g. if we had $\\mathbf q^{(1)} = \\mathbf q^{(2)}$ then as a result we could see that the inequality is strictly unless $\\mathbf w^{(1)} = \\mathbf w^{(2)}$.  However there is one additional concern that was not an issue in the log sum inequality writeup: the case where KL Divergence, evaluates to infinity.     \n",
    "\n",
    "\n",
    "**additional technial nit:**  where we have zero in the denominator of the logriathm: then we have divergence and the inequality is meaningful (in particular in comparing two positive infinities is not meaningful in standard analysis)  restricting our selves to the case where every probability is positive (even if vanishly small) cleans up all the results considerably.  \n",
    "\n",
    "\n",
    "- - - - - \n",
    "\n",
    "summing over all $i$ we see  \n",
    "\n",
    "\n",
    "$ \\sum_i  \\Big( p \\cdot w_i^{(1)} + (1-p) \\cdot w_i^{(2)}\\Big) \\log \\Big(\\frac{ p \\cdot w_i^{(1)} + (1-p) \\cdot w_i^{(2)} }{ p \\cdot q_i^{(1)} + (1-p) \\cdot q_i^{(2)}}\\Big)  \\leq  p \\cdot \\Big(\\sum_i  w_i^{(1)} \\log \\big(\\frac{ w_i^{(1)} }{  q_i^{(1)}}\\big)\\Big) +  (1-p) \\cdot \\Big( \\sum_i w_i^{(2)} \\log \\big(\\frac{ w_i^{(2)} }{q_i^{(2)}}\\big)\\Big) $ \n",
    "\n",
    "- - - - \n",
    "now we simply notice that:   \n",
    "\n",
    "$\\sum_i  \\Big( p \\cdot w_i^{(1)} + (1-p) \\cdot w_i^{(2)}\\Big) \\log \\Big(\\frac{ p \\cdot w_i^{(1)} + (1-p) \\cdot w_i^{(2)} }{ p \\cdot q_i^{(1)} + (1-p) \\cdot q_i^{(2)}}\\Big) = D_{\\text{KL}}\\big(p\\cdot \\mathbf w^{(1)} + (1-p) \\mathbf w^{(2)} \\big \\Vert p\\cdot \\mathbf q^{(1)} + (1-p) \\mathbf q^{(2)}\\big)$ \n",
    "\n",
    "and \n",
    "\n",
    "$p \\cdot \\Big(\\sum_i  w_i^{(1)} \\log \\big(\\frac{ w_i^{(1)} }{  q_i^{(1)}}\\big)\\Big) +  (1-p) \\cdot \\Big( \\sum_i w_i^{(2)} \\log \\big(\\frac{ w_i^{(2)} }{q_i^{(2)}}\\big)\\Big) = p \\cdot D_{\\text{KL}}\\big(\\mathbf w^{(1)} \\big \\Vert \\mathbf q^{(1)}\\big) + (1-p)\\cdot D_{\\text{KL}}\\big( \\mathbf w^{(2)} \\big \\Vert  \\mathbf q^{(2)}\\big)$  \n",
    "- - - - \n",
    "which proves that \n",
    "\n",
    "$D_{\\text{KL}}\\big(p\\cdot \\mathbf w^{(1)} + (1-p) \\mathbf w^{(2)} \\big \\Vert p\\cdot \\mathbf q^{(1)} + (1-p) \\mathbf q^{(2)}\\big) \\leq p \\cdot D_{\\text{KL}}\\big(\\mathbf w^{(1)} \\big \\Vert \\mathbf q^{(1)}\\big) + (1-p)\\cdot D_{\\text{KL}}\\big( \\mathbf w^{(2)} \\big \\Vert  \\mathbf q^{(2)}\\big)$  \n",
    "\n",
    "Note: supposing that the LHS is finite (KL Divergence, does unfortunately diverge when zero probabilities show up) then the inequality is strict unless  -- we know this because of the sharpness conditions associated with the Log Sum Inequality  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**proof 2**  \n",
    "An alternative approach is to consider $x^{(1)}, x^{(2)}  \\in \\big(0, \\infty \\big)$ \n",
    "\n",
    "and the function $f$ given by \n",
    "\n",
    "$f\\big(x^{(1)},x^{(2)}\\big) = x^{(1)} \\log\\frac{x^{(1)}}{x^{(2)}}$ \n",
    "\n",
    "This has a Hessian of \n",
    "\n",
    "$\\mathbf H = \\begin{bmatrix}\n",
    "\\frac{1}{x^{(1)}} & -\\frac{1}{x^{(2)}}\\\\ \n",
    "-\\frac{1}{x^{(2)}} & \\frac{x^{(1)}}{x^{(2)}\\cdot x^{(2)}}\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "The matrix is real symmetric, with   \n",
    "$\\text{trace}\\big(\\mathbf H\\big) \\gt 0$  \n",
    "$\\det\\big(\\mathbf H \\big) = 0$   \n",
    "\n",
    "which confirms that the matrix is postive semi-definite, and in turn our function $f$ is convex (using taylor polynomial arguments).  \n",
    "\n",
    "hence we have \n",
    "\n",
    "$f\\big(E\\big[\\mathbf x\\big]\\big) \\leq E\\big[f\\big(\\mathbf x\\big)\\big]$    \n",
    "\n",
    "letting $x^{(1)} = w^{(k)}$ and $x^{(2)} = q^{(k)}$, taking the expectation with respect to $p, (1-p)$ and summing over all $i$ gives the result, as in the above.  \n",
    "\n",
    "Your author prefers the first proof to this one because the former uses information theoretic tools / shows it is implied by earlier information theoretic tools, and has clear equality conditions that it inherits from them.  In this second case, the approach is analytic and as a result does not give as much information theoretic insights, and further, since the function is not strictly convex, the equality conditions are a bit harder to interpret.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
