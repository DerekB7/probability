{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This section really is a mixture of key topics from chapter 13 and chapter 15 of Feller volume 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Renewal Matrix - Age\n",
    "This is example 2l) from chp 15, page 382 of Feller Vol. 1, 3rd edition. There are more moving parts to this matrix than the residual life matrix.  This is also page 184 of Ross & Pekoz, though they index at zero, not one there.  (In fact indexing at zero is a lot more convenient in this case.  However to keep with the presentation elsewhere in this writeup, and more importantly, to tie in to into the various extensions cited in Feller, this writeup will use indexing at 1.)  \n",
    "\n",
    "\n",
    "$\\mathbf A =\n",
    "\\left[\\begin{matrix}\n",
    "q_1 & p_1 & 0 & 0 & 0 & 0 & \\dots\\\\\n",
    "q_2 & 0 & p_2& 0 & 0 & 0 & \\dots\\\\\n",
    "q_3 & 0 & 0 & p_3 & 0 & 0 & \\dots\\\\\n",
    "q_4 & 0 & 0 & 0 & p_4 & 0 & \\dots\\\\\n",
    "q_5 & 0 & 0 & 0 & 0 & p_5 & \\dots\\\\\n",
    "q_6 & 0 & 0 & 0 & 0 & 0 & \\ddots\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots\\end{matrix}\\right]$  \n",
    "\n",
    "where for avoidance of doubt $q_k + p_k = 1$  and the top row refers to age zero \n",
    "\n",
    "hence row $i$ of the matrix refers to state $i - 1$ \n",
    "\n",
    "recall from page 308 of Feller that \n",
    "\n",
    "$f_k = Pr\\{\\text{Renewal occurs for the first time at the kth trial}\\}$  \n",
    "\n",
    "for $k = 1, 2, 3, ...$  \n",
    "- - - - \n",
    "\n",
    "Thus if we are on say row 3, this is state 2, and we want the conditional probability of a renewal given that we are in state 2.  So -- again with a pinch of awkwardness on 0 vs 1 indexing-- focus on the row number, this reads as \n",
    "\n",
    "$q_3 = Pr\\{ \\text{renewal at next iteration given at state 2 (row 3)}\\} = Pr\\{f_3 \\big \\vert f_j \\geq 3\\} = \\frac{f_3}{\\sum_{j \\geq 3} f_j}  = \\frac{f_3}{1- (f_1  + f_2)}$  \n",
    "\n",
    "and for some general state we have \n",
    "\n",
    "$q_k = Pr\\{ \\text{renewal at next iteration given at state k-1 (row k)}\\} = Pr\\{ f_k \\big \\vert f_j \\geq k\\} = \\frac{f_k}{\\sum_{j \\geq k} f_j} = \\frac{f_k}{1- (f_1  + f_2 + ... + f_{k-1})}$  \n",
    "\n",
    "and of course: \n",
    "\n",
    "$p_k = 1-q_k = 1 - \\frac{f_k}{1- (f_1  + f_2 + ... + f_{k-1})}  = \\frac{1- (f_1  + f_2 + ... + f_{k-1})}{1- (f_1  + f_2 + ... + f_{k-1})} - \\frac{f_k}{1- (f_1  + f_2 + ... + f_{k-1})} = \\frac{1- (f_1  + f_2 + ... + f_{k-1} + f_k)}{1- (f_1  + f_2 + ... + f_{k-1})} $\n",
    "\n",
    "which agrees with equation (2.5) (on page 382) of Feller.  \n",
    "\n",
    "\n",
    "**Technical Note: The above sums are fine for a non-defective renewal process.  However positing that they sum to one does not immediately follow for defective renewal processes. That is conditional probabilities for defective random variables is rather pecuiliar concept.  We address that the above modelling in fact correctly addresses defective renewal processes, via the below writeup.**   \n",
    "\n",
    "$p_k= \\frac{1- (f_1  + f_2 + ... + f_{k-1} + f_k)}{1- (f_1  + f_2 + ... + f_{k-1})}$ \n",
    "\n",
    "not that the probability of no renewal after $n$ trials is given by   \n",
    "$Pr\\{\\text{no renewal at n trials}\\} = \\prod_{k=1}^n p_k = \\prod_{k=1}^n \\frac{1- (\\sum_{i=1}^1 f_k)}{1 - (\\sum_{j=1}^{k-1} f_j)} =  \\frac{1- (\\sum_{i=1}^k f_k)}{1 - (\\sum_{j=1}^{0} f_j)}\\cdot\\frac{1- (\\sum_{i=1}^2 f_k)}{1 - (\\sum_{i=1}^{1} f_j)}\\cdot\\frac{1- (\\sum_{i=1}^3 f_k)}{1 - (\\sum_{i=1}^{2} f_j)}\\cdot\\frac{1- (\\sum_{i=1}^4 f_k)}{1 - (\\sum_{i=1}^{3} f_j)}\\cdot...\\cdot\\frac{1- (\\sum_{i=1}^n f_i)}{1 - (\\sum_{i=1}^{n-1} f_j)}$  \n",
    "\n",
    "we could take advantage of positivity and explicitly work in logspace here -- in such a case we'd identify the above relation as a telescoping sum.  The point is that the numerator of term $k-1$ cancels (is equivalent to) the denominator of term $k$.  Thus we can simplify the product as:  \n",
    "\n",
    "$Pr\\{\\text{no renewal at n trials}\\} = \\prod_{k=1}^n p_k = \\frac{1}{1 - (\\sum_{j=1}^{0} f_j)} \\cdot \\frac{1- (\\sum_{i=1}^n f_i)}{1 }= 1- (\\sum_{i=1}^n f_i)$  \n",
    "\n",
    "$Pr\\{\\text{no renewals}\\} = \\lim_{n \\to \\infty} Pr\\{\\text{no renewal at n trials}\\} =  \\lim_{n \\to \\infty} \\Big(1- (\\sum_{i=1}^n f_i)\\Big) = 1- (\\sum_{i=1}^\\infty f_i)$  \n",
    "\n",
    "And we recover the textbook definition of a defective (discrete) renewal process: where said process is defective iff the sum over all probabilities of having a first renewal is less than one.  \n",
    "\n",
    "- - - -  \n",
    "Note that in practice evaluating infinite product is not as easy as infinite sums. If given a chain like this, it is generally preferable to evaluate \n",
    "\n",
    "$\\sum_{k=1}^\\infty q_k$  \n",
    "\n",
    "and in particular to determine whether $\\sum_{k=1}^\\infty q_k \\lt \\infty$.  This in fact gives us the same information as the infinite product with respect to whether or not renewal process is defective, as discussed below.  \n",
    "\n",
    "\n",
    "Note that condioned on starting in the top left corner (state 0), the expected time until first renewal for a non-defective renewal process can be calculated as \n",
    "\n",
    "$\\mu = 1\\cdot f_1 + 2\\cdot f_2 + 3\\cdot f_3 + 4\\cdot f_4 + ... $  \n",
    "\n",
    "If we use out Renewal Matrix Chain, we see \n",
    "\n",
    "$\\mu = 1\\cdot q_1 + 2\\cdot q_2 p_1 + 3\\cdot q_3 p_1 p_2 + 4\\cdot q_4 p_1 p_2 p_3 + ... = 1\\cdot f_1 + 2\\cdot f_2 + 3\\cdot f_3 + 4\\cdot f_4 + ...$  \n",
    "\n",
    "as desired, \n",
    "\n",
    "because in general the $n$th term for natural number $n \\geq 2$ is given by \n",
    "\n",
    "$n \\cdot  \\big(q_n\\big)\\big( \\prod_{i=1}^{n-1} p_i\\big) =  n \\cdot \\big(\\frac{f_n}{1- (f_1  + f_2 + ... + f_{n-1})}\\big) \\big( 1- (\\sum_{i=1}^{n-1} f_k)\\big) = n\\cdot f_n$  \n",
    "\n",
    "- - - - \n",
    "where we used the 'telescoping' result from above which showed:    \n",
    "$Pr\\{\\text{no renewal at n trials}\\} = \\prod_{k=1}^n p_k = \\frac{1}{1 - (\\sum_{j=1}^{0} f_j)} \\cdot \\frac{1- (\\sum_{i=1}^n f_k)}{1 }= 1- (\\sum_{i=1}^n f_k)$  \n",
    "\n",
    "and by construction  \n",
    "$q_1 = f_1$  \n",
    "$q_k = Pr\\{ \\text{renewal at next iteration given at state k-1 (row k)}\\} = Pr\\{ f_k \\big \\vert f_j \\geq k\\} = \\frac{f_k}{\\sum_{j \\geq k} f_j} = \\frac{f_k}{1- (f_1  + f_2 + ... + f_{k-1})}$\n",
    "\n",
    "(though this writeup used $k$ not $n$ to index for this particular item earlier on)  \n",
    "- - - - \n",
    "**steady state **  \n",
    "\n",
    "For (concise) completeness in addressing steady state issues, we'll assume the renwal chain is aperiodic, and we'll verify that for a non-defective renewal process: \n",
    "\n",
    "(the emphasis here is on the positive recurrent case.  In the ull recurrenct case we have $\\mu = \\infty$ and $\\frac{1}{\\mu} = 0$ but some additional care is needed to verify the results)   \n",
    "\n",
    "$\\mathbf \\pi \\mathbf A = \\mathbf \\pi$  \n",
    "\n",
    "where   \n",
    "$\\mathbf {\\pi} = \\begin{bmatrix}\n",
    "\\pi_1\\\\ \n",
    "\\pi_2\\\\ \n",
    "\\pi_3\\\\ \n",
    "\\vdots \\\\ \n",
    "\\pi_i\\\\ \n",
    "\\vdots\n",
    "\\end{bmatrix}^T = \\frac{1}{\\mu} \\begin{bmatrix}\n",
    "1\\\\ \n",
    "p_1  \\\\ \n",
    "p_1p_2\\\\ \n",
    "\\vdots \\\\ \n",
    "\\prod_{j=1}^{r-1} p_j\\\\ \n",
    "\\vdots\n",
    "\\end{bmatrix}^T = \\frac{1}{\\mu} \\begin{bmatrix}\n",
    "Pr\\{\\text{1st Renewal time}\\gt 0\\}\\\\ \n",
    "Pr\\{\\text{1st Renewal time}\\gt 1\\} \\\\ \n",
    " Pr\\{\\text{1st Renewal time}\\gt 2\\}\\\\ \n",
    "\\vdots \\\\ \n",
    "\\Pr\\{\\text{1st Renewal time}\\gt r-1\\}\\\\ \n",
    "\\vdots\n",
    "\\end{bmatrix}^T $  \n",
    "\n",
    "that is, we shall confirm that our steady state vector is in fact a steady state.  \n",
    "\n",
    "for component 1, this is equivalent to confirming that \n",
    "\n",
    "$\\pi_1 = \\frac{1}{\\mu}=\\sum_k \\big(q_k\\big)\\big( \\pi_k\\big) = \\sum_k \\big(\\frac{f_k}{1- (f_1  + f_2 + ... + f_{k-1})} \\big)\\big(\\frac{1}{\\mu} \\prod_{j=1}^{k-1} p_j \\big)  =  \\frac{1}{\\mu} \\sum_k \\big(\\frac{f_k}{1- (f_1  + f_2 + ... + f_{k-1})} \\big)\\big(1- (\\sum_{j=1}^{k-1} f_j)\\big)  =  \\frac{1}{\\mu} \\sum_k f_k =  \\frac{1}{\\mu}$   \n",
    "\n",
    "for component $r \\geq 2$ we have \n",
    "\n",
    "$\\frac{1}{\\mu} \\prod_{j=1}^{r-1} p_j= \\pi_r =  p_{r-1} \\cdot \\pi_{r-1} =\\big(p_{r-1} \\big)\\big(\\frac{1}{\\mu} \\prod_{j=1}^{(r-1)-1} p_j \\big)= \\frac{1}{\\mu} \\big(p_{r-1} \\big)\\big( \\prod_{j=1}^{r-2} p_j \\big) = \\frac{1}{\\mu} \\cdot \\prod_{j=1}^{r-1} p_j $  \n",
    "\n",
    "both, as desired, hence \n",
    "\n",
    "$\\mathbf \\pi \\mathbf A = \\mathbf \\pi$  \n",
    "\n",
    "is in fact a steady state vector  \n",
    "\n",
    "- - - - \n",
    "\n",
    "\n",
    "# The below is a workthrough of (8.5), contained on page 400\n",
    "These seem to be standard results from analysis, though your author felt the need to go through them in some detail.  In particular it is very handy for interpretting when the above renewal age matrix is transient \n",
    "\n",
    "supposing we have probabilities of $0 \\leq \\epsilon_k \\lt 1$ (note in the case of $\\epsilon_k = 0$, we immediately see that the finite product is zero, and hence after passage to the limit it is still zero)\n",
    "\n",
    "of being absorbed on a countable state markov chain, at iteration $k$, then this means the probabilty of not being absorbed over trials $\\{j, j+1, ..., j+n -1\\}$ is given by \n",
    "\n",
    "$(1-\\epsilon_{j})(1-\\epsilon_{j+1})...(1-\\epsilon_{j+n-1}) = \\prod_{k = j}^{j+n-1} (1-\\epsilon_{k})$    \n",
    "\n",
    "Feller then states that \n",
    "\n",
    "$0 \\lt \\lim_{n \\to \\infty} \\prod_{k = j}^{j+n-1} (1-\\epsilon_{k}) $ \n",
    "\n",
    "**iff** $\\sum_k \\epsilon_{k} \\lt \\infty$  \n",
    "\n",
    "or equivalently\n",
    "\n",
    "$0 \\neq \\lim_{n \\to \\infty} \\prod_{k = j}^{j+n-1} (1-\\epsilon_{k}) $ \n",
    "- - -- \n",
    "\n",
    "This writeup flushes out why that is true.  \n",
    "\n",
    "In particular we'll consider the cases where that series is finite and where it is infinite.  For avoidance of doubt, that the finite case *could* be written as $\\sum_k -\\epsilon_{k} \\gt -\\infty$ or even $\\sum_k -2\\epsilon_{k} \\gt -\\infty$.  \n",
    "\n",
    "*key bounds:* \n",
    "\n",
    "make use of the fact that $\\big \\vert \\epsilon_k\\big \\vert \\lt 1$ and use the power serries for the natural logarithm so that we may work with sums instead of products.  \n",
    "\n",
    "In (natural) logspace our claim is \n",
    "\n",
    "the probability is not 0 **iff**\n",
    "\n",
    "$-\\infty \\lt \\lim_{n \\to \\infty} \\sum_{k=j}^{j+n-1} \\log(1-\\epsilon_k) $  \n",
    "\n",
    "equivalently, the probability is zero iff the above series has no lower bound.  \n",
    "\n",
    "We note that each term in the above series is bounded above by zero (i.e. everything is real non-positive) so there is no ambiguity in using $-\\infty$.  \n",
    "\n",
    "now consider the power series expansion for the natural log where $\\big \\vert x \\big \\vert \\lt 1$  \n",
    "\n",
    "$\\log\\big(1 + x\\big) = x -\\frac{x^2}{2} + \\frac{x^3}{3} - \\frac{x^4}{4}+...$ \n",
    "\n",
    "in our case, where each term in the series, we have \n",
    "\n",
    "$\\log\\big(1 - \\epsilon_k\\big) = \\log\\big(1 + (- \\epsilon_k)\\big) = (- \\epsilon_k) -\\frac{(- \\epsilon_k)^2}{2} + \\frac{(- \\epsilon_k)^3}{3} - \\frac{(- \\epsilon_k)^4}{4}+... = - \\epsilon_k -\\frac{\\epsilon_k^2}{2} - \\frac{ \\epsilon_k^3}{3} - \\frac{\\epsilon_k^4}{4}- = \\sum_{k=1}^\\infty -\\frac{\\epsilon_k^k}{k} = -\\sum_{k=1}^\\infty \\frac{\\epsilon_k^k}{k}$ \n",
    "\n",
    "recalling that $0 \\leq \\epsilon_k$ so each and every term in that series is 0 if $\\epsilon_k = 0$ (i.e. we recover the fact that $\\log\\big(1\\big) = 0$, otherwise each and every term in the series is negative.  \n",
    "**for the upper bound on our probability, we may simply recognize ** \n",
    "\n",
    "We thus have a term by term bound of  \n",
    "\n",
    "$\\sum_{k=1}^\\infty -\\frac{\\epsilon_k^k}{k} = -\\epsilon_k -\\sum_{k=2}^\\infty \\frac{\\epsilon_k^k}{k} \\leq  -\\epsilon_k$  \n",
    "\n",
    "(Alternatively we could recognize that the tangent line of $\\log(1+x)$ at $x =0$ has a slope of one and intercept of zero.  By the negative convexity of the natural logarithm, this means that $\\log(1+x) \\leq x$ for all $x$ in our domain.)  \n",
    "\n",
    "\n",
    "Hence, we have \n",
    "\n",
    "$-\\infty \\lt  \\sum_{k=j}^{j+n-1} \\log(1-\\epsilon_k) \\leq \\sum_{k=j}^{j+n-1} -\\epsilon_k$ \n",
    "\n",
    "now if the epsilon series diverges, we have \n",
    "$\\sum_k -\\epsilon_{k} = -\\infty$  \n",
    "\n",
    "passing limits to our inequality, we have \n",
    "\n",
    "$-\\infty \\leq \\lim_{n \\to \\infty} \\sum_{k=j}^{j+n-1} \\log(1-\\epsilon_k) \\leq \\lim_{n \\to \\infty} \\sum_{k=j}^{j+n-1} -\\epsilon_k = \\sum_k -\\epsilon_{k} = -\\infty$  \n",
    "\n",
    "which confirms that the probability(/ infinite product) goes to zero when the $\\sum_k -\\epsilon_{k} = -\\infty$  \n",
    "\n",
    "or equivalently that the probability(/ infinite product) goes to zero when the $\\sum_k \\epsilon_{k} = \\infty$  \n",
    "- - -- \n",
    "**for the lower bound: ** \n",
    "\n",
    "we use a geometric series\n",
    "\n",
    "$-\\log\\big(1 - \\epsilon_k\\big) = \\sum_{k=1}^\\infty \\frac{\\epsilon_k^k}{k} \\lt \\sum_{k=1}^\\infty \\epsilon_k^k = \\frac{\\epsilon}{1-\\epsilon_k}$   \n",
    "\n",
    "noting that for any non-zero $\\epsilon_k$ the inequality is strict because the term by term inequality is strict for all $k\\geq 2$  \n",
    "\n",
    "to extend this, we may say \n",
    "\n",
    "$-\\log\\big(1 - \\epsilon_k\\big) \\lt \\frac{\\epsilon}{1-\\epsilon_k} \\lt 2\\epsilon_k$   \n",
    "\n",
    "for sufficiently small $\\epsilon_k$  (specifically all $\\epsilon_k \\lt \\frac{1}{2}$ )  \n",
    "\n",
    "\n",
    "Now multiplying both sides by negative one we have \n",
    "\n",
    "$\\log\\big(1 - \\epsilon_k\\big) \\gt \\frac{-\\epsilon}{1-\\epsilon_k}$   \n",
    "\n",
    "or \n",
    "\n",
    "$\\frac{-\\epsilon}{1-\\epsilon_k} \\lt \\log\\big(1 - \\epsilon_k\\big) $   \n",
    "\n",
    "and again, with sufficiently small $\\epsilon_k$ we have \n",
    "\n",
    "$ -2\\epsilon_k \\lt \\frac{-\\epsilon}{1-\\epsilon_k} \\lt \\log\\big(1 - \\epsilon_k\\big) $  \n",
    "\n",
    "or more succinctly\n",
    "\n",
    "$ -2\\epsilon_k \\lt \\log\\big(1 - \\epsilon_k\\big) $  \n",
    "\n",
    "Now if our epsilon series converges i.e. $\\sum_k -\\epsilon_{k} \\gt -\\infty$ or in equivalent non-negative terms: $\\sum_k \\epsilon_{k} \\lt \\infty$, then there can only be finitely many $\\epsilon_k \\gt 0.5$ -- suppose for a contradiction that this is not true, then the real non-negative form of the infinite series may be lower bounded by $ \\infty = \\sum_k 0.5 \\leq \\sum_k \\epsilon_{k} \\leq \\infty$ which contradicts the fact that we've assumed the series converges.  Hence there must be at most $m$ terms (where $m$ is some finite natural number) that are not sufficiently small.  \n",
    "\n",
    "Our logspace infinite series consists entirely of real non-positive values (and recall there is no 'funny business' like $\\log(0)$ -- the logarithm is taken on values greater than zero and less than or equal to one), hence cannot diverge due to the first $m$ terms (or any sum of a finite number of terms, of course). That is, it diverges, **iff** the tail diverges (i.e. the sum of all but the first $m$ terms).  \n",
    "\n",
    "Hence we bound the tail with \n",
    "\n",
    "$ -\\infty \\lt -2c = -2 \\sum_{k} \\epsilon_k \\lt -2 \\sum_{k=j}^{\\infty} \\epsilon_k \\lt -2 \\sum_{k=j+m}^{\\infty} \\epsilon_k= \\lim_{n \\to \\infty} \\sum_{k=j+m}^{j+n-1+m} -2\\epsilon_k  \\lt \\lim_{n \\to \\infty} \\sum_{k=j+m}^{j+n-1+m}\\log\\big(1 - \\epsilon_k\\big) $  \n",
    "\n",
    "i.e.the tail of our logspace series is bounded below by $-2c$ for some finite, real non-negative $c$, which means that the logspace series is finite, and hence the infinite product that gives our probability is $\\gt 0$. \n",
    "\n",
    "This proves the claim in the book.  (Though the lower bound argument could probably be cleaned up a bit).  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renewal Matrix -- residual life\n",
    "\n",
    "Consider for example, this matrix\n",
    "\n",
    "$\\mathbf A_7 = \\left[\\begin{matrix}p_1 & p_2 & p_3 & p_4 & p_5 & p_6 & p_7 \\\\1 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 1 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 1 & 0\\end{matrix}\\right]$  \n",
    "\n",
    "and in general in finite dimensions, we have \n",
    "\n",
    "$\\mathbf A_n = \n",
    "\\left[\\begin{matrix}\n",
    "p_1 & p_2 & p_3 & p_4 & p_5 & \\dots & p_n \\\\\n",
    "1 & 0 & 0 & 0 & 0 & \\dots & 0\\\\\n",
    "0 & 1 & 0 & 0 & 0 & \\dots & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0 & \\dots & 0\\\\\n",
    "0 & 0 & 0 & 1 & 0 & \\dots & 0\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 1 & 0\\end{matrix}\\right]$  \n",
    "\n",
    "and in countably infinite dimensions we have \n",
    "\n",
    "$\\mathbf A_{\\infty} = \n",
    "\\left[\\begin{matrix}p_1 & p_2 & p_3 & p_4 & p_5 & p_6 & \\dots \\\\\n",
    "1 & 0 & 0 & 0 & 0 & 0 & \\dots\\\\ \n",
    "0 & 1 & 0 & 0 & 0 & 0 & \\dots\\\\\n",
    "0 & 0 & 1 & 0 & 0 & 0 & \\dots\\\\\n",
    "0 & 0 & 0 & 1 & 0 & 0 & \\dots\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\ddots & \\ddots\\\\\n",
    "\\end{matrix}\\right]$  \n",
    "\n",
    "\n",
    "What is interesting about all 3 of these matrices is that they are a particular kind of renewal matrix. Reference for example 2k on p. 381 of Feller volume 1 (3rd edition).  This is a matrix to model recurrent events and residual wait times. It also can be interpretted as the matrix for repeated averaging given by 10c on page 333 of Feller.  \n",
    "\n",
    "Understanding the underlying Markov Chain here can give us lots of insights into Renewal Theory, and of course Renewal Theory can give us lots of inights into how this chain works.  \n",
    "\n",
    "It is further interesting that in the finite dimensional case, $\\mathbf A$ is, up to a graph isomorphism, similar to the Companion matrix. We can effect the similarity tranform with the reflection matrix $\\mathbf J$, which is involutary and a permutation matrix.  (Note: if something is a graph isomorphism, one may ask why not just model the problem so that the top row is on the bottom row from the outset -- the we wouldn't need to do these similarity transforms.  The reality is that we could have, but then it would be quite difficult to talk about the countably infinite case -- we could of course just have labelled the finite graph two different ways and the countably infinite one in one way-- in effect that *is* what we're doing with the similarity transform below.)  \n",
    "\n",
    "For example, consider:  \n",
    "\n",
    "$\\mathbf J_7 = \\left[\\begin{matrix}0 & 0 & 0 & 0 & 0 & 0 & 1\\\\0 & 0 & 0 & 0 & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1& 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0 & 0 & 0\\\\1 & 0 & 0 & 0 & 0 & 0 & 0\\end{matrix}\\right]$  \n",
    "- - - - \n",
    "\n",
    "$\\mathbf J \\mathbf C \\mathbf J^{-1} = \\mathbf J \\mathbf C \\mathbf J =\\left[\\begin{matrix}0 & 1 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 1 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 1\\\\p_7 & p_6 & p_5 & p_4 & p_3 & p_2 & p_1\\end{matrix}\\right]$  \n",
    "\n",
    "which is the transpose of the Companion matrix.  \n",
    "- - - - \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Steady States of above matrix **  \n",
    "\n",
    "\n",
    "note that the above $\\mathbf A_n$ is row stochastic, so   \n",
    "$\\mathbf A_n \\mathbf 1 = \\mathbf 1$   \n",
    "\n",
    "(Note what is said below applies almost verbatim for the countably infinite case so we tackle both at the same time)  \n",
    "\n",
    "as for the dominant left eigvector / steady state probability distribution given by\n",
    "\n",
    "$\\mathbf \\pi \\mathbf A = \\mathbf \\pi $?  \n",
    "\n",
    "let's first assume that $p_1 \\gt 0$.  \n",
    "\n",
    "This immediately tells us that the chain is aperiodic.  (If however, for example $p_i=0$ for odd $i$, then the chain would have period 2. We could also just insist on some other sufficient condition like having at least one odd and at least one even indexed probability be non-zero and that would give aperiodicity.)  \n",
    "\n",
    "Next: there is, by construction one communicating class here.  Now to avoid special handling for the countable case let's assume that recurrence time from state 1 to state 1 is finite--  given by $\\bar{X} \\lt \\infty$ \n",
    "\n",
    "\n",
    "The steady state values are \n",
    "\n",
    "$1 = \\pi_1 + \\pi_2 + \\pi_3 + ... $  \n",
    "\n",
    "there is a natural relation / recurrence between these values, however.  In particular, consider that state $1$ has steady state probabilities 'flowing' in from all states, while state $2$ has probability flowing in from all states except transition probability $p_1$.  \n",
    "\n",
    "that is for state 1 we have the trivially true:  \n",
    "\n",
    "$\\pi_1 = \\pi_1\\big(p_1 + p_2 + p_3 + ...) = \\pi_1 \\big(1) $    \n",
    "\n",
    "but for state 2 we have the more interesting: \n",
    "\n",
    "$\\pi_2 = \\pi_1\\big(p_2 + p_3 +  ...) $ \n",
    "\n",
    "for state 3 we have \n",
    "\n",
    "$\\pi_3 = \\pi_1\\big(p_3 + p_4 +  ...)  $ \n",
    "\n",
    "and for state $k$ we have \n",
    "\n",
    "$\\pi_k = \\pi_1 \\cdot \\sum_{i\\geq k} p_i $ \n",
    "\n",
    "\n",
    "substituting back into our equation, we have \n",
    "\n",
    "$1 = \\pi_1 + \\pi_2 + \\pi_3 + ... = \\pi_1  \\cdot \\sum_k \\big(\\sum_{i\\geq k} p_i\\big) = \\pi_1  \\cdot \\sum_k Pr\\{X \\gt (k-1)\\} = \\pi_1  \\cdot \\bar{X} $  \n",
    "\n",
    "\n",
    "which immediately gives us \n",
    "\n",
    "$\\pi_1  = \\frac{1}{\\bar{X}} = \\frac{ Pr\\{X \\gt (0)\\}}{\\bar{X}} $ \n",
    "\n",
    "which we (hopefully!) already knew. However it also tells us \n",
    "\n",
    "$\\pi_2 = \\pi_1  - \\frac{p_1 }{\\bar{X}} =  \\frac{1 - p_1 }{\\bar{X}} =  \\frac{ Pr\\{X \\gt (1)\\}}{\\bar{X}} $  \n",
    "\n",
    "$\\pi_3 = \\pi_1  - \\frac{p_1  + p_2}{\\bar{X}} = \\frac{1 - (p_1  + p_2)}{\\bar{X}} =  \\frac{ Pr\\{X \\gt (2)\\}}{\\bar{X}}  $  \n",
    "\n",
    "and in general \n",
    "\n",
    "$\\pi_k  = \\pi_1  - \\frac{p_1  + p_2  + ... + p_{k-1}}{\\bar{X}} = \\frac{1 - (p_1  + p_2  + ... + p_{k-1})}{\\bar{X}} = \\frac{ Pr\\{X \\gt (k-1)\\}}{\\bar{X}}  $  \n",
    "\n",
    "where, for avoidance of doubt, for $k \\geq 2$, we have the recurrence:  \n",
    "\n",
    "$\\pi_{k}  = \\pi_{k-1} - \\frac{p_{k-2}}{\\bar{X}}  $  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Thus in the finite dimensional case we can read off the characteristic polynomial of the matrix.  We also know that we can use the Companion matrix to model many recurrence relations, which would be the question -- can we use renewal theory insights to easily solve for limiting behavior in more general recurrence relations.  For this generalization, which we attempts first, it is possible, to your authors knowledge for most cases where the recurrence relation has real non-negative coefficients.  \n",
    "\n",
    "For example consider the method suggested by Lalley as exponential tilting on page 6 of here, for use in the Fibonacci recursion.  \n",
    "\n",
    "http://galton.uchicago.edu/~lalley/Courses/Summer/Renewal2.pdf\n",
    "\n",
    "\n",
    "consider the typical Fibonacci matrix  \n",
    "\n",
    "$\\mathbf B= \\begin{bmatrix}\n",
    "1 & 1\\\\ \n",
    " 1& 0\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "and to generate fibonnaci numbers we proceed along the lines of \n",
    "\n",
    "$\\mathbf B^k \\mathbf x$  and so forth.  \n",
    "\n",
    "that is in general the recursion is \n",
    "\n",
    "$x_{k} = x_{k-1} + x_{k-2}$  \n",
    "\n",
    "We could interpret this as $\\mathbf A_2$ except while the top row is real non-negative, it sums to 2, not to 1, and hence can't be interpretted as probabilities.  We also can't quite do straightforward similarity transforms to make that issue with the row go away. The indication is to do a change of variables\n",
    "\n",
    "**tbc: I think the linked notes have it wrong they say: **\n",
    "\n",
    "$z_k := \\theta^{-k}x_k$   \n",
    "** but I think the correct way is** \n",
    "\n",
    "$z_k := \\theta^{k}x_k$   \n",
    "- - - - \n",
    "\n",
    "which makes the recurrence    \n",
    "\n",
    "$z_{k} = z_{k-1}\\theta^1 + z_{k-2}\\theta^2 $  \n",
    "\n",
    "(with additional handling needing for bases cases)  \n",
    "\n",
    "$\\mathbf C= \\begin{bmatrix}\n",
    "\\theta & \\theta^2 \\\\ \n",
    " 1 & 0\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "and is there a satisfactory that works with   \n",
    "\n",
    "$\\theta + \\theta^2 = 1$ \n",
    "\n",
    "or \n",
    "\n",
    "$\\theta^2 + \\theta - 1 =0$  \n",
    "\n",
    "There are two roots here, with one of them positive, which is given by \n",
    "\n",
    "$\\theta = \\frac{-1 + \\sqrt{5}}{2}$  \n",
    "\n",
    "and for avoidance of doubt, note: $0 \\lt \\theta \\lt 1$  which means \n",
    "\n",
    "$1 \\lt \\theta^{-1} $  \n",
    "\n",
    "\n",
    "in this setup we have an expected time until renewal of:  \n",
    "\n",
    "$\\mu = 1  \\theta^1 + 2 \\theta^2$ \n",
    "\n",
    "which tells us that the for large enough $k$ the equation tends to \n",
    "\n",
    "- - - - \n",
    "**what linked notes indicate** \n",
    "$z_k \\approx \\frac{1}{\\mu} = \\frac{1}{\\theta + 2 \\theta^2} = \\theta^{-k}x_k$  \n",
    "\n",
    "- - - - \n",
    "\n",
    "**what I think is correct: ** \n",
    "$z_k \\approx \\frac{1}{\\mu} = \\frac{1}{\\theta + 2 \\theta^2} = \\theta^{k}x_k$  \n",
    "\n",
    "which tells us that our $x_k$ grows exponentially (or technically geometrically in discrete time) with a rate of $\\theta^{-1}$ and a coefficient of $\\frac{1}{\\mu}$.  I.e. we have \n",
    "\n",
    "$x_k \\sim \\frac{\\theta^{-k}}{\\mu}  $  \n",
    "\n",
    "*there are some strong patterns / similarities with interpretting this and some ideas related to generating functions (and having a dummy variable like $\\theta$ where we can see the degree of the generating function by its exponent... for OGFs we typically extract the actual probabilities by setting it equal to one, though this seems like a variation on that theme.*)    \n",
    "\n",
    "For another worked example (note this comes up in some of the modelling on streaks problems consider: \n",
    "\n",
    "$\\mathbf B = \\left[\\begin{matrix}\\frac{3}{4} & \\frac{3}{4} & \\frac{3}{4} & \\frac{3}{4} &\\frac{3}{4}\\\\\\frac{1}{4} & 0 & 0 & 0 & 0\\\\0 & \\frac{1}{4} & 0 & 0 & 0\\\\0 & 0 & \\frac{1}{4} & 0 & 0\\\\0 & 0 & 0 & \\frac{1}{4} & 0\\end{matrix}\\right]$  \n",
    "\n",
    "This can be interpretted as the collection of the transient states in an absorbing state markov chain -- i.e. the stochastic matrix it is embedded in could be something like:  \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\mathbf B & \\mathbf 0\\\\ \n",
    " \\frac{1}{4}& 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "hence for $\\mathbf B$, the forward transition probability is $p = \\frac{1}{4}$, and we rescale, via dividing by $p$ to recover the companion matrix\n",
    "\n",
    "\n",
    "\n",
    "$\\mathbf C = \\left[\\begin{matrix}3 & 3 & 3 & 3 & 3\\\\1 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & 0\\end{matrix}\\right] = \\frac{1}{p}\\mathbf B$    \n",
    "\n",
    "\n",
    "**note** this is referred to as tilting probabilities, and while the above $x_k$ is not exact, it is an exponentially tight estimate... re-visiting the sections in Gallagher on large deviations and tilting probabilities... seems quite useful here.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now want to go from the recurrence (note this ignores nits surrounding the base case)  \n",
    "\n",
    "hence the first row of\n",
    "\n",
    "$\\mathbf {Cx}$  \n",
    "\n",
    "gives us  \n",
    "\n",
    "$x_k =  3 x_{k-1} + 3 x_{k-2} + 3 x_{k-3} + 3 x_{k-4} + 3 x_{k-5}$   \n",
    "\n",
    "$z_k := \\theta^{k}x_k$  \n",
    "\n",
    "hence \n",
    "\n",
    "$z_k = 3\\theta z_{k-1} + 3\\theta^2 x_{k-2} + 3\\theta^3 x_{k-3} + 3\\theta^4 x_{k-4} + 3\\theta^5 x_{k-5}$  \n",
    "\n",
    "\n",
    "hence we are looking for $\\theta \\in (0,1)$ where \n",
    "\n",
    "$3\\theta  + 3\\theta^2  + 3\\theta^3 + 3\\theta^4 + 3\\theta^5  = 1$   \n",
    "\n",
    "or \n",
    "\n",
    "\n",
    "$\\theta^5 + \\theta^4 + \\theta^3  + \\theta^2   + \\theta =  \\frac{1}{3} $  \n",
    "\n",
    "or   \n",
    "$g(\\theta ) = \\theta^5 + \\theta^4 + \\theta^3  + \\theta^2   + \\theta -  \\frac{1}{3} =0 $  \n",
    "\n",
    "note that we can **almost** guess the root here... \n",
    "\n",
    "note that the matrix that generated this had a forward transtion probabiltiy of $\\frac{1}{4}$.  \n",
    "\n",
    "if instead of having a finite series, it was an infinite series, it would be\n",
    "\n",
    "$\\frac{1}{3} = \\theta + \\theta^2  + \\theta^3  + \\theta^4  +\\theta^5 + \\theta^6 + ... = \\frac{\\theta}{1- \\theta} $  \n",
    "\n",
    "selecting $\\theta := \\frac{1}{4}$ gives \n",
    "\n",
    "$\\frac{1}{3} = \\frac{\\theta}{1- \\theta} = \\frac{\\frac{1}{4}}{1-\\frac{1}{4}} = \\frac{\\frac{1}{4}}{\\frac{4-1}{4}} = \\frac{\\frac{1}{4}}{\\frac{3}{4}} = \\frac{1}{3}$  \n",
    "\n",
    "Thus, taking advantage of positivity we see an $\\epsilon \\gt 0$ given below \n",
    "\n",
    "$ \\theta + \\theta^2  + \\theta^3  + \\theta^4  +\\theta^5 + \\epsilon = \\theta + \\theta^2  + \\theta^3  + \\theta^4  +\\theta^5 + \\big(\\theta^6 + \\theta^7 + ...\\big) = \\frac{\\theta}{1- \\theta} $  \n",
    "\n",
    "that is \n",
    "\n",
    "$\\theta + \\theta^2  + \\theta^3  + \\theta^4  +\\theta^5 + \\epsilon = \\frac{1}{3}$ \n",
    "\n",
    "$\\theta + \\theta^2  + \\theta^3  + \\theta^4  +\\theta^5  - \\frac{1}{3} = - \\epsilon$ \n",
    "\n",
    "and thus \n",
    "\n",
    "$g(\\frac{1}{4}) = -\\epsilon \\lt 0$  \n",
    "\n",
    "and we want to select some small $\\delta \\gt 0$ such that \n",
    "\n",
    "$g(\\frac{1}{4} + \\delta) \\gt 0$  \n",
    "\n",
    "which places the root   \n",
    "$\\lambda \\in \\big(\\frac{1}{4}, \\frac{1}{4} + \\delta\\big)$  \n",
    "\n",
    "\n",
    "now we know \n",
    "\n",
    "$\\epsilon = \\big(\\theta^6 + \\theta^7 + ...\\big) = \\frac{\\theta^6}{1-\\theta}$  \n",
    "\n",
    "where as always we are interested in $\\theta \\in (0,1)$ \n",
    "\n",
    "lazily taking advantage of positivity, and supposing $\\theta \\in \\big(0, \\frac{1}{2}\\big)$ to isolate the residual term:  \n",
    "\n",
    "$  \\frac{\\theta^6}{1-\\theta} \\leq \\delta \\lt  (\\delta) + (\\delta)^2 + ... + (\\delta)^5 \\lt \\big((\\theta +\\delta) + (\\theta +\\delta)^2 + ... + (\\theta +\\delta)^5\\big) - \\big((\\theta ) + (\\theta )^2 + ... + (\\theta )^5\\big) = g(\\frac{1}{3} + \\delta) - g(\\frac{1}{3})  $ \n",
    "\n",
    "that is, we can easily select \n",
    "\n",
    "$\\frac{\\theta^6}{1-\\theta} \\leq \\delta$  \n",
    "or \n",
    "\n",
    "$\\theta^6 \\leq \\delta (1 - \\theta)$  \n",
    "\n",
    "which, if $0 \\leq \\theta \\leq \\frac{1}{2}$, then we have \n",
    "\n",
    "$\\theta^6 \\leq \\delta \\theta \\leq \\delta (1 - \\theta)$  \n",
    "\n",
    "so selecting any $\\theta^5 \\leq \\delta  $ suffices \n",
    "\n",
    "in this case, we may in particular choose \n",
    "\n",
    "$\\delta:= \\big(\\frac{1}{2}\\big)^5 = \\frac{1}{32} = 0.03125 $   \n",
    "\n",
    "\n",
    "- - - - \n",
    "note that if we were to allow $0 \\lt \\theta \\leq \\frac{4}{5}$ then we know   \n",
    "\n",
    "$\\frac{5}{4}\\theta = \\frac{1}{4}\\theta + \\theta \\leq 1$ \n",
    "\n",
    "hence $\\frac{1}{4}\\theta \\leq 1 - \\theta$ which gives us \n",
    "\n",
    "$\\theta^6 \\leq \\delta \\frac{1}{4}\\theta \\leq \\delta (1 - \\theta)$  \n",
    "\n",
    "and hence selecting \n",
    "\n",
    "$4 \\theta^5 \\leq \\delta $ \n",
    "\n",
    "suffices.  The underlying mechanics here is general, however we want must **also** ensure that $\\theta + \\delta \\lt 1$ and in general that $\\delta$ is satisfactorily small.  So for the $\\frac{4}{5}$ case we can do a quick check and see\n",
    "\n",
    "$4 \\big(\\frac{4}{5}\\big)^5 = 1.31072$ \n",
    "\n",
    "**This is a failure**. Now if the exponent was much larger, of course we would have more flexibility. But this tells us the above method may not be appropriate for $\\theta $ near 1. (If we look at the underlying streaks graph we can see this corresponds to 'leaving' an absorbing state quite quickly and hence it is not of such a concern.)  \n",
    "- - - - \n",
    "\n",
    "Note we could allow $0 \\lt \\theta \\leq \\frac{3}{5}$  \n",
    "\n",
    "this would give us \n",
    "\n",
    "$\\frac{5}{3}\\theta = \\frac{2}{3}\\theta + \\theta \\leq 1 $  \n",
    "\n",
    "so \n",
    "\n",
    "$\\frac{2}{3}\\theta \\leq 1- \\theta $  \n",
    "\n",
    "hence we could select \n",
    "\n",
    "\n",
    "$\\frac{3}{2} \\theta ^5 \\leq \\delta$  \n",
    "\n",
    "(Here we could choose $\\delta$ in the neighborhood of $0.117$ would work if $\\theta = \\frac{3}{5}$ though this seems to put a rather larger gap around our $\\theta$ value... the results get much tighter for higher exponent values, of course).  \n",
    "\n",
    "\n",
    "\n",
    "- - - - \n",
    "note for example with $\\theta = \\frac{3}{5}$, we have $\\frac{3}{5} \\leq \\lambda \\leq \\frac{3}{5} + 0.117$\n",
    "\n",
    "\n",
    "As we'd expect, the result gets much better for larger $k$.  E.g. for $k=10$, we have analytical bounds of $0.6 \\leq \\lambda \\leq 0.60152$.  \n",
    "- - - - \n",
    "\n",
    "back to the matter at hand: \n",
    "\n",
    "$g(\\theta ) = \\theta^5 + \\theta^4 + \\theta^3  + \\theta^2   + \\theta -  \\frac{1}{3} $  \n",
    "\n",
    "we know that the result is very close to $\\frac{1}{4}$, in fact an easy bound tells us $\\frac{1}{4} \\lt \\lambda \\lt \\frac{1}{4} + \\frac{1}{64}$, or \n",
    "\n",
    "\n",
    " $\\frac{16}{64} \\lt \\lambda \\lt \\frac{17}{64}$  \n",
    "\n",
    "(numerically we can see that the actual root is $\\lambda \\approx 0.25018$)  \n",
    "\n",
    "setting $\\theta := \\lambda $ \n",
    "\n",
    "(i.e. the positive root of the polynomial between 0 and 1 *is* the value we want for $\\theta$.)  \n",
    "\n",
    "So to recap, we now have \n",
    "\n",
    "$\\mathbf R = \\left[\\begin{matrix}3\\theta & 3\\theta^2 & 3\\theta^3 & 3\\theta^4 & 3\\theta^5\\\\1 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & 0\\end{matrix}\\right]$\n",
    "\n",
    "which is a bonafide renewal matrix (and we've confirmed that all entries are real non-negative and row 1 sums to one)\n",
    "\n",
    "From here we compute \n",
    "\n",
    "$\\mu = 1(3\\theta) + 2(3\\theta^2) + 3 (3\\theta^3) + 4(3\\theta^4) + 5(3\\theta^5) + 6 (3\\theta^6) \\approx \\frac{4}{3}$   \n",
    "\n",
    "and as before \n",
    "\n",
    "$x_k \\sim \\frac{\\theta^{-k}}{\\mu} \\approx  \\frac{0.25018^{-k}}{\\frac{4}{3}} = (3)(4)^{-1}\\big(\\frac{1}{0.25018}\\big)^k \\lt (3)(4)^{-1}\\Big(\\frac{1}{\\frac{1}{4}}\\Big)^{k} = (3)(4)^{-1}(4)^k= (3)4^{k-1} = \\frac{3}{4}p^{-k}$  \n",
    "\n",
    "now recalling that \n",
    "\n",
    "$\\big(p\\big)^k \\mathbf C^k =  \\mathbf B^k$  \n",
    "\n",
    "we can recover the limitting tendancy for multiplication by $\\mathbf C$, which we have already solved for in terms of $\\mathbf B$, by multiplying by $p^k$  \n",
    "\n",
    "$x_k^{\\text{when multiplied by C}} \\sim p^k \\frac{\\theta^{-k}}{\\mu} \\approx  \\big(\\frac{1}{4}\\big)^k \\frac{0.25018^{-k}}{\\frac{4}{3}} = \\frac{3}{4}\\big(\\frac{\\frac{1}{4}}{0.25018}\\big)^k \\lt \\frac{3}{4}\\Big(1\\Big)^{k} = \\frac{3}{4}$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 open items:\n",
    "1.) explicitly connect the above with exponential tilting in the case of using OGFs -- can I do this? Basically mix some of the above ideas with what is said by Gallagher -- however, trying to toggle between OGFs and MGFs -- sticking with integer valued -- perhaps finitely so, would seem to be perhaps helpful and make the linkage -- not sure on this point. \n",
    "\n",
    "2.) Explicitly compare some of these results and bounds vs what is said below using expected absorbtion times plus perron frobenius to bound the dominant eigenvalue controlling the decay rate \n",
    "\n",
    "3.) working through the balance equations portion at other states in the markov chain -- should be relatively straight forward -- would be good to have on hand since these residual life markov chains seem to come up quite a bit \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** On decay rates for time until absorbtion in finite state markov chains**   \n",
    "\n",
    "a large portion of this motivated by \n",
    "\n",
    "\"Approximating the distributions of runs and patterns\" and open access paper by Johnson and Fu, and in particular page 9.  \n",
    "\n",
    "\n",
    "using notation mostly similar to that of \"markov_chains_absorbing_state_recut.ipynb\", where the final state, $m$ is the one we have particular interest in.  \n",
    "\n",
    "That is, consider the below row stochastic matrix given by:  \n",
    "\n",
    "\n",
    "$\\mathbf B = \\begin{bmatrix}\n",
    "\\mathbf A^T & *  \\\\ \n",
    "* & *\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "where $\\mathbf A \\in \\mathbb R^{\\text{m -1  x m - 1 }}$  \n",
    "\n",
    "so we know $\\mathbf B \\mathbf 1 = \\mathbf 1$ \n",
    "\n",
    "To avoid triviality, assume this is irreducible -- i.e. there is one communicating class in here.  \n",
    "\n",
    "In line with problem 5.45, page 285 of Gallagher's *Stochastic Processes*, we may be interested in finding expected time of getting to state $m$ from, say state $1$, and modify the graph to have all zeros in row zero, except a 1 (i.e. determinsitic path) from state $m \\to 1$.  Or if this is a streaks/runs problem or many other graphs that have a \"feed forward\" nature to them, it may in fact naturally be the case that state 1 is the 'beginning' and state $m$ is the end, and  after every renewal at $m$ it automatically progresses back to the 'begginning'.  (Of course in renewal theory terms, renewals to $m$ begin and start there, but starting at 0 and ending at $m$ is *almost* the same thing -- literally all sample paths are lessened in length by 1 which is an easy correction to deal with as needed.)  The above structure isn't needed per se -- the only essential is that there is one communicating class, but having a certain kind of monotonicity allows us to make additional claims.  \n",
    "\n",
    "Now consider what happens when we make state $m$ an absorbing state: \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\mathbf A^T & *  \\\\ \n",
    "\\mathbf 0^T & 1\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "Suppose that all transient states still communicate with each other. (This is not strictly necessary but it simplifies what follows in terms of positive matrices, in stead of mere non-negative ones, and lines up with the suggestion that it is particularly if state $m$ only communicated with state $1$ before becoming an absorbing state.)  \n",
    "\n",
    "the above matrix is still row stochastic.  However, the chain in $\\mathbf A^T$ is a transient chain.  Either by direct examination of the probabilities, or the fact that it is transient, we may confirm $\\mathbf A^T \\mathbf 1 \\neq \\mathbf 1$ i.e. that the dominant eigenvalue for $\\mathbf A$, given by $\\lambda_1$ is $\\lt 1$ (as the graph is transient -- alternatively at most $m$ iterations and application of Gerschgorin discs prove this claim as well.)  In general $\\mathbf 1$ is no longer an eigenvector of $\\mathbf A$.  Perron theory tells us that there is a single dominant eigenvalue $\\lambda_1$ with entirely positive entries in the associated left and right eigenvectors\n",
    "\n",
    "so we have \n",
    "\n",
    "$\\mathbf A^T \\mathbf v_1 = \\lambda_1 \\mathbf v_1$   \n",
    "\n",
    "we know that the geometric and algebraic multipilicities are 1 for this eigenpair.  This is as far as your author typically takes it.  However, there is much more information to be had here.  \n",
    "\n",
    "When we use the Collatz-Wielandt max-min formulation, we see that \n",
    "\n",
    "\n",
    "$\\lambda_1 = \\text{max}_{\\mathbf v \\in D} f(\\mathbf v)$  \n",
    "\n",
    "where \n",
    "\n",
    "$f(\\mathbf v) = \\text{min k} \\frac{\\mathbf e_k^T \\big(\\mathbf A^T v\\big)}{\\mathbf e_k^T \\mathbf v}$  \n",
    "\n",
    "where $D = \\{\\mathbf v \\big \\vert\\mathbf v .\\geq \\mathbf 0, \\text{ and } \\mathbf v \\neq 0\\}  $  \n",
    "\n",
    "and for avoidance of doubt $v .\\geq \\mathbf 0$ is a component wise comparison for each entry in $\\mathbf v$ being real non-negative, and $\\mathbf e_k$ is the $k$th standard basis vector.    \n",
    "\n",
    "$v .\\geq \\mathbf 0 $\n",
    "\n",
    "The above is the *max-min* formulation.  The below is the *min-max* characterization\n",
    "\n",
    "$\\lambda_1 = \\text{min}_{\\mathbf v \\in P} g(\\mathbf v)$  \n",
    "\n",
    "$g(\\mathbf v) = \\text{max j   } \\frac{\\mathbf e_j^T \\big(\\mathbf A^T v\\big)}{\\mathbf e_j^T \\mathbf v}$  \n",
    "\n",
    "where $P = \\{\\mathbf v \\big \\vert \\mathbf v .\\geq \\mathbf 0, \\text{ and } \\mathbf v \\neq 0\\}  $  \n",
    "\n",
    "(reference pages 667 - 669 of Meyer's *Matrix Analysis* for more information, though the earlier mentioned article references Karlin and Taylor volume 1, Corollary 2.2 on page 551.  )  \n",
    "\n",
    "Thus we have \n",
    "\n",
    "$f(\\mathbf v) \\leq \\lambda_1 \\leq g(\\mathbf v)$  \n",
    "\n",
    "supposing we select $\\mathbf v := \\mathbf 1$, we have \n",
    "\n",
    "\n",
    "$0 \\lt f(\\mathbf 1) \\leq \\lambda_1 \\leq g(\\mathbf 1)$ \n",
    "\n",
    "note that the denominator for $f$ and $g$ are $\\mathbf e_k^T \\mathbf 1 = 1$ and $\\mathbf e_j^T \\mathbf 1 = 1$ in each case-- i.e. a scalar value of one, hence the denominator may be ignored due to the homogeneity of the ones vector.  \n",
    "\n",
    "(note that we *know $\\mathbf A^T \\mathbf 1 \\gt 0$ as each row has positive entries on it, and we have assumed that the nodes still communicate -- otherwise it technically could be $0 \\leq f(\\mathbf 1\\big)$  )  \n",
    "\n",
    "\n",
    "re-running the above argument with \n",
    "\n",
    "$g_r(\\mathbf v) = \\text{max j   } \\frac{\\mathbf e_j^T \\big(\\mathbf A^T\\big)^r v}{\\mathbf e_j^T \\mathbf v}$ \n",
    "and \n",
    "\n",
    "$f_r(\\mathbf v) = \\text{min k} \\frac{\\mathbf e_k^T \\big(\\mathbf A^T\\big)^r v}{\\mathbf e_k^T \\mathbf v}$  \n",
    "\n",
    "we see \n",
    "\n",
    "$0 \\lt f_r(\\mathbf 1) \\leq \\lambda_1^r \\leq g_r(\\mathbf 1)^r$  \n",
    "\n",
    "for all natural numbers $r = \\{1, 2,...\\}$, and for notational convenience we take the convention that they all have value of 1 for $r =0$ case \n",
    "\n",
    "supposing for the moment that $g_r(\\mathbf 1)^r \\lt 1$  for large enough $r$ (this is true... why?)  \n",
    "\n",
    "then we may sum over the bound to get \n",
    "\n",
    "$0 \\lt \\sum_{r =0}^{\\infty} f_r(\\mathbf 1) \\leq \\sum_{r =0}^{\\infty} \\lambda_1^r = \\frac{1}{1 -\\lambda_1} \\leq \\sum_{r =0}^{\\infty} g_r(\\mathbf 1)$  \n",
    "\n",
    "And we note that \n",
    "\n",
    "$\\sum_{r =0}^{\\infty} f_r(\\mathbf 1) = \\sum_{r =0}^{\\infty} \\text{min row} \\mathbf A^r \\mathbf 1$  \n",
    "\n",
    "$\\sum_{r =0}^{\\infty} g_r(\\mathbf 1) = \\sum_{r =0}^{\\infty} \\text{max row} \\mathbf A^r \\mathbf 1$ \n",
    "\n",
    "This approach does not quite give us what we want, however, because it 'grabs' the min and max at each iteration.  So instead consider \n",
    "\n",
    "$\\mathbf N := \\mathbf I + \\mathbf A^T + \\big(\\mathbf A^T\\big)^2 + \\big(\\mathbf A^T\\big)^3 +...  = \\mathbf I + \\sum_{r=1}^\\infty \\big(\\mathbf A^T\\big)^r $    \n",
    "\n",
    "(which notationally differs / overloads that from the linked streaks paper, though this is nice in that it lines up exactly with standard fundamental matrix notation and that used in 'markov_chains_absorbing_state_recut.ipynb')  \n",
    "\n",
    "we end up with \n",
    "\n",
    "$\\mathbf N = \\big(\\mathbf I - \\mathbf A^T\\big)^{-1}$  \n",
    "\n",
    "now, re-running our argument we can say \n",
    "\n",
    "$\\mu_{k \\text{  shortest }} = f\\Big(\\mathbf {N1}\\Big) =  f\\Big(\\mathbf I + \\sum_{r=1}^\\infty \\big(\\mathbf A^T\\big)^r\\Big) \\leq \\lambda_{max}\\big(\\mathbf N\\big) =   \\frac{1}{1 -\\lambda_1}  \\leq   g\\Big(\\mathbf I + \\sum_{r=1}^\\infty \\big(\\mathbf A^T\\big)^r\\Big) = g\\Big(\\mathbf {N1}\\Big) = \\mu_{j \\text{  longest }}$ \n",
    "\n",
    "where \n",
    "\n",
    "$\\mu_{k \\text{  shortest }}$ denotes the shortest of all expected times from a node $k$ $\\lt m$, and going $k \\to m$   \n",
    "$\\mu_{j \\text{  longest }} $ denotest the longest of all expected times from a node $j$ $\\lt m$, and going $j \\to m$  \n",
    "\n",
    "we can easily see that $\\frac{1}{1 -\\lambda_1}$ is an eigenvalue of $\\mathbf N$.  It remains to verify that this is in fact the dominant eigenvalue.  This can be done by direct calculation and application of triangle inequality.  However, a more slick approach is to note that $\\mathbf N$ is the sum of real non-negative matrices (and because of underlying communicating states) we know that $\\mathbf N$ has entirely positive entries.  So its dominant eigenvalue is positive, and the associated left and right eigenvectors have strictly positive components -- and all other eigenvectors are orthogonal to these (left vs right and right vs left) -- i.e. no other eigenvectors can have strictly positive components. But we know that $\\big(\\mathbf I - \\mathbf A^T\\big)^{-1}$ has the same eigenvectors as $\\mathbf A^T$, and hence $\\mathbf v_1$ is the (right) eigenvector associated with the dominant eigenvalue, and thus $\\frac{1}{1-\\lambda_1}, \\mathbf v_1$ is the dominant (right) eigenpair.  \n",
    "\n",
    "- - - -\n",
    "This last fact is easily confirmable via a Jordan Form argument, or by observing \n",
    "\n",
    "$\\mathbf A^T \\mathbf v_1 = \\lambda_1 \\mathbf v_1 \\to \\big(\\mathbf I - \\mathbf A^T\\big) \\mathbf v_1 = (1-\\lambda_1) \\mathbf v_1 \\to \\big(\\mathbf I - \\mathbf A^T\\big)^{-1} \\mathbf v_1 = \\frac{1}{(1-\\lambda_1)} \\mathbf v_1$  \n",
    "\n",
    "because \n",
    "\n",
    "$\\mathbf v_1 = \\mathbf {Iv}_1 = \\big(\\mathbf I - \\mathbf A^T\\big)^{-1}\\big(\\mathbf I - \\mathbf A^T\\big)\\mathbf v_1 = \\big(\\mathbf I - \\mathbf A^T\\big)^{-1}\\Big(\\big(\\mathbf I - \\mathbf A^T\\big)\\mathbf v\\Big) = (1-\\lambda_1) \\big(\\mathbf I - \\mathbf A^T\\big)^{-1}\\mathbf v_1$ \n",
    "\n",
    "hence multiplying each side by $\\frac{1}{(1-\\lambda_1)}$ gives \n",
    "\n",
    "$\\frac{1}{(1-\\lambda_1)} \\mathbf v_1 =  \\big(\\mathbf I - \\mathbf A^T\\big)^{-1}\\mathbf v_1$ \n",
    "\n",
    "- - - -\n",
    "hence we have \n",
    "\n",
    "$\\mu_{k \\text{  shortest }} \\leq  \\frac{1}{1 -\\lambda_1}  \\leq   \\mu_{j \\text{  longest }}$ \n",
    "\n",
    "and taking advantage of positivity we may say \n",
    "\n",
    "$\\frac{1}{\\mu_{k \\text{  shortest }}} \\geq  1 -\\lambda_1  \\geq   \\frac{1}{\\mu_{j \\text{  longest }}}$ \n",
    "\n",
    "$-\\frac{1}{\\mu_{k \\text{  shortest }}} \\leq - 1 +\\lambda_1  \\leq   -\\frac{1}{\\mu_{j \\text{  longest }}}$ \n",
    "\n",
    "$1 -\\frac{1}{\\mu_{k \\text{  shortest }}} \\leq  \\lambda_1  \\leq   1-\\frac{1}{\\mu_{j \\text{  longest }}}$   \n",
    "\n",
    "and in *some* cases we may be able to see a natural affine relationship between the largest and smallest absorbtion times, for example:  \n",
    "\n",
    "suppose $\\mu_{k \\text{  shortest }} = s (\\mu_{j \\text{  longest }}) + c$  \n",
    "\n",
    "then, if this relationship exists and in a meaningful way, then we'd have \n",
    "\n",
    "$1 -\\frac{1}{s (\\mu_{j \\text{  longest }  })+c} \\leq  \\lambda_1  \\leq   1-\\frac{1}{\\mu_{j \\text{  longest }}}$   \n",
    "\n",
    "e.g. in the streaks problem mentioned below, we have \n",
    "\n",
    "$1 -\\frac{1}{(1-p) (\\mu_{j \\text{  longest } })+1} \\leq  \\lambda_1  \\leq   1-\\frac{1}{\\mu_{j \\text{  longest }}}$  \n",
    "\n",
    "and given how large $\\mu_{j \\text{  longest }}$ for moderately large problems, e.g. when $m\\geq 10$ and $p = \\frac{1}{2}$, this actually gives an extremely tight bound on $\\lambda_1$  \n",
    "\n",
    "\n",
    "**special cases of interest -- structured graphs with monotonocity**  \n",
    "\n",
    "for many graphs of interest,  including streaks/runs problems, there may be a node that has a clear longest expected time until absorbtion -- because, for instance, it is the 'starting node'. And it may be that via techniques from renewal theory, martingales, generating functions or otherwise, that we can easily calculate $\\mu_{j \\text{  longest }}$.  Hence we may choose to 'zoom in' on \n",
    "\n",
    "\n",
    "$ \\frac{1}{1 -\\lambda_1}  =\\mu_{j \\text{  longest }}$   \n",
    "\n",
    "taking advantage of positivity (and note we know $\\mu_{j \\text{  longest }}\\lt \\infty$ for finite state markov chains with communicating nodes) we can see this as   \n",
    "\n",
    "$  \\frac{1}{\\mu_{j \\text{  longest }}}\\leq 1 - \\lambda_1 $   \n",
    "\n",
    "$ -1 + \\frac{1}{\\mu_{j \\text{  longest }}}\\leq  - \\lambda_1 $   \n",
    "\n",
    "$e^{- \\frac{1}{\\mu_{j \\text{  longest }}}}\\gt 1 - \\frac{1}{\\mu_{j \\text{  longest }}}\\geq   \\lambda_1 $   \n",
    "\n",
    "where the fact that $1+ x \\lt e^x$ for all $x \\neq 0$ (by strict convexity of exponential map) was used, with  $x:=- \\frac{1}{\\mu_{j \\text{  longest }}}$  \n",
    "\n",
    "e.g. for a small streaks problem, we would have \n",
    "\n",
    "$\\mathbf B = (1-p)\\left[\\begin{matrix}1 & 1 & 1 & 1 & 1 & 1 & \\frac{1}{1-p}\\\\\\frac{p}{1-p} & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & \\frac{p}{1-p} & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & \\frac{p}{1-p} & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & \\frac{p}{1-p} & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & \\frac{p}{1-p} & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & \\frac{p}{1-p} & 0\\end{matrix}\\right]^T$  \n",
    "\n",
    "\n",
    "we can follow the graph to see that state $m$ feeds solely into state 1.  We can also see by monotonicity that the expected time from \n",
    "\n",
    "$1 \\to m$ is larger than from $i \\to m$ for $2 \\leq i \\leq m-1$  \n",
    "\n",
    "and because $m \\to 1$ with weighting 1, we can see \n",
    "\n",
    "$1 + \\mu_{j \\text{  longest }} = E\\big[\\text{time from 0 }  \\to \\text{ m} \\big] + 1 =  E\\big[\\text{time from m }  \\to \\text{ m} \\big] = \\bar{X} $ \n",
    "\n",
    "or equivalently \n",
    "\n",
    "$\\mu_{j \\text{  longest }} =\\bar{X} - 1 $  \n",
    "\n",
    "so we have a renewal rate of $\\frac{1}{\\bar{X}}$ \n",
    "\n",
    "- - - \n",
    "**maybe applying that problem muddies the waters too much and doesn't have enough offsetting benefits... not quite sure, a more direct approach is taken below** \n",
    "\n",
    "applying the ideas from ex 5.45 (page 285) of Gallagher's Stochastic processes,  \n",
    "\n",
    "we could look at the steady state presence on node $1$ and say that $q$ comes from state $m$ and $1-q$ comes from other states (including possibly self loops)  \n",
    "\n",
    "in particular \n",
    "\n",
    "$q = \\frac{\\frac{1}{\\bar{X}}}{\\text{total steady state on 1}}$  \n",
    "\n",
    "and \n",
    "\n",
    "$\\text{total steady state on 1} = \\frac{1}{\\bar{X}} + \\text{other in flow}$\n",
    "\n",
    "\n",
    "\n",
    "hence \n",
    "\n",
    "$1-q = 1 -\\frac{\\frac{1}{\\bar{X}}}{\\text{total steady state on 1}} $  \n",
    "- - -   \n",
    "\n",
    "\n",
    "and if we made it into an absorbing state markov chain, we'd see:  \n",
    "\n",
    "$\\mathbf B_{\\text{absorbing}} = (1-p)\\left[\\begin{matrix}1 & 1 & 1 & 1 & 1 & 1 & 0\\\\\\frac{p}{1-p} & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & \\frac{p}{1-p} & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & \\frac{p}{1-p} & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & \\frac{p}{1-p} & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & \\frac{p}{1-p} & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & \\frac{p}{1-p} & \\frac{1}{1-p}\\end{matrix}\\right]  =\\begin{bmatrix}\n",
    "\\mathbf A & \\mathbf 0  \\\\ \n",
    "* & 1\n",
    "\\end{bmatrix}^T$  \n",
    "\n",
    "(note that due to conflicting standards and legacy issues, this is shown 'pre-transpose' -- the adjustment should be immediate)  \n",
    "\n",
    "Thus with $m$ as an absorbing state, we'd estimate that 'outflow' is in the neighborhood of $\\frac{1}{\\bar{X}}$ flowing from the 'pool' of transient state weights into $1$.  Equivalently $\\approx \\big(1- \\frac{1}{\\bar{X}}\\big)$  is being retained by the transient states.  \n",
    "\n",
    "\n",
    "\n",
    "$ 1 - \\frac{1}{1 + \\mu_{j \\text{  longest }}} =  1- \\frac{1}{\\bar{X}}$  \n",
    "\n",
    "now using the fact that \n",
    "\n",
    "$\\frac{1}{1 + \\mu_{j \\text{  longest }}} \\lt \\frac{1}{ \\mu_{j \\text{  longest }}}$ (though recalling that $\\frac{1}{1 + \\mu_{j \\text{  longest }}} \\approx \\frac{1}{ \\mu_{j \\text{  longest }}}$ for large $\\mu_{j \\text{  longest }}$ ), we now negate then add one and see  \n",
    "\n",
    "$\\big(1- \\frac{1}{\\bar{X}}\\big) = 1 - \\frac{1}{1 + \\mu_{j \\text{  longest }}} \\gt 1 - \\frac{1}{ \\mu_{j \\text{  longest }}}\\geq   \\lambda_1$  \n",
    "\n",
    "hence \n",
    "\n",
    "$\\big(1- \\frac{1}{\\bar{X}}\\big)^r \\gt   \\lambda_1^r$  \n",
    "\n",
    "using the Bernouli inequality\n",
    "\n",
    "$\\big(1- \\frac{1}{\\bar{X}}\\big)^r \\geq  1- \\frac{r}{\\bar{X}} $ \n",
    "\n",
    "we may be inclined to use one or both of the above to estimate the amount of decay after a certain amount of iterations, though strictly speaking, the LHS is in fact an upper bound on $\\lambda_1^r$ (the dominant eigenvalue, hence which dominates the decay rate), while $1- \\frac{r}{\\bar{X}}$ has a relationship that may or may not be less than ($\\lambda_1^r$) -- for large enough $r$ it of course is because it becomes negative, and of course when $r=1$ it is an upper bound, but it is not clear in general at which point it 'crosses over' to become the a lower bound and hence in general is not of such great use.  \n",
    "\n",
    "**It may be prudent to compare the above results vs results from using the renewal matrix approach for a simple streaks problem**   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " this below is a re-cut and re-interpretation of a problem I like, here: \n",
    " https://www.physicsforums.com/threads/a-seemingly-simple-problem-about-probability.938359/\n",
    "\n",
    "\n",
    "if we discretize and bound U ~(0,1) with a uniform distribution with n intervals, we get the below workthrough.  The limmitting result can be derived without discretization via application of Blackwell's Theorem, though the discretization gives some additional insight to what is going on under the hood.  \n",
    "\n",
    "The problem is a rare one that actually has a closed form solution to the renewal equation, which gives a result *almost* identical to the Blackwell distribution result \n",
    "\n",
    "while the closed form solution is a bit tedious a pending open item is to show said solution, exactly  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02,  0.02,  0.02, ...,  0.02,  0.02,  0.02],\n",
       "       [ 1.  ,  0.  ,  0.  , ...,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  1.  ,  0.  , ...,  0.  ,  0.  ,  0.  ],\n",
       "       ..., \n",
       "       [ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  , ...,  1.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  , ...,  0.  ,  1.  ,  0.  ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "n = 50\n",
    "\n",
    "A = np.zeros((n,n))\n",
    "for j in range(n-1):\n",
    "    A[j+1,j] = 1\n",
    "A[0,:] += 1/n\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the above is for discretized uniform over 1 year\n",
    "# we want over 3 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ones_v = np.ones(n)\n",
    "# A @ ones_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.039215686274509796"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "renewal_rate = 1/(A[0] @ np.arange(n) + 1)\n",
    "renewal_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Q, R = np.linalg.qr(A - np.identity(n))\n",
    "\n",
    "q  = Q[:,-1]\n",
    "q *= 1/np.sum(q)\n",
    "# q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first consider this\n",
      "2.34666666667\n",
      "  \n",
      "[ 2.02  2.04  2.06  2.08  2.1   2.12  2.14  2.16  2.18  2.2   2.22  2.24\n",
      "  2.26  2.28  2.3   2.32  2.34  2.36  2.38  2.4   2.42  2.44  2.46  2.48\n",
      "  2.5   2.52  2.54  2.56  2.58  2.6   2.62  2.64  2.66  2.68  2.7   2.72\n",
      "  2.74  2.76  2.78  2.8   2.82  2.84  2.86  2.88  2.9   2.92  2.94  2.96\n",
      "  2.98  3.  ] begin \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3493052804535539"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "termination_threshold = 2\n",
    "\n",
    "k_iterations = n * termination_threshold\n",
    "x = np.zeros(n)\n",
    "\n",
    "# this is the backward take... it seems more in line with harmonic functions...\n",
    "\n",
    "for idx in range(n):\n",
    "    x[idx] += termination_threshold + idx/n + 1/n\n",
    "#     x[idx] += termination_threshold + idx/n \n",
    "    \n",
    "    # these actually should be useful for lower and upper bounding the amount I think... \n",
    "\n",
    "print(\"first consider this\")\n",
    "print(q @ x)\n",
    "print(\"  \")\n",
    "    \n",
    "print(x,\"begin \\n\")\n",
    "\n",
    "for _ in range(k_iterations):\n",
    "    b = A @ x\n",
    "    x = b \n",
    "\n",
    "# # this is the backward take\n",
    "\n",
    "# print(x,\"\\n\")\n",
    "x[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.5\n",
      "25.5\n"
     ]
    }
   ],
   "source": [
    "mu = 1/q[0]\n",
    "print(mu)\n",
    "print((n+1)/2)\n",
    "# the should agree\n",
    "# remember that mu refers to the expected time until renewal from state 1... it has nothing to do with vector x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3266666666666702"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_sum = 0\n",
    "for i in range(n):\n",
    "    running_sum += (n-i)/(n) * (termination_threshold + i/n )\n",
    "\n",
    "# the upper bound case is quite similar\n",
    "running_sum/mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3466666666666667"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "termination_threshold + 1/3 + 2/(3*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3266666666666667"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "termination_threshold + 1/3 - 1/(3*n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.6707742704716049"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-1/np.exp(1))*np.exp(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6707742704716053"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for t in 1 to 2\n",
    "t = 2\n",
    "\n",
    "m_of_t = (np.exp(t) - 1) - (t-1)*np.exp(t-1)\n",
    "m_of_t\n",
    "# http://www.randomservices.org/random/renewal/Equations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.589142896867322"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for t in 2 to 3\n",
    "t = 3\n",
    "\n",
    "m_of_t = (np.exp(t) - 1) - (t-1)*np.exp(t-1) -(t-2)*np.exp(t-2)\n",
    "m_of_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50999999999999934"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the analytical formula for the *lower bound case* is \n",
    "\n",
    "$\\frac{(n+1)(3nt + n -1)}{6n} \\frac{1}{mu} = \\frac{(n+1)(3nt + n -1)}{6n} \\frac{2}{n+1} = \\frac{(3nt + n -1)}{3n}$  \n",
    "\n",
    "$=t +\\frac{1}{3} - \\frac{1}{3n}$ \n",
    "\n",
    "\n",
    "the upper bound case comes from running \n",
    "sum from k = 0 to (n-1) of (n-k)/n*(t +( k+1)/n)\n",
    "through wolfram, then dividing resulting formula by mu and simplifying, which gives us \n",
    "\n",
    "$t + \\frac{1}{3}  +\\frac{2}{3n}$  \n",
    "\n",
    "which would suggest -- if the limit of large $n$ exists 'nicely' with this steady state, then in continuous time, we'd have a limitting /steady state value of \n",
    "\n",
    "$t + \\frac{1}{3}$\n",
    "\n",
    "now if we worked directly in continuous time, we'd have \n",
    "\n",
    "$\\frac{1}{\\mu}\\int_{0}^1 (1-x)(t+x)dx = 2 \\cdot \\int_{0}^1 (1-x)(t+x)dx = 2 \\big(\\frac{t}{2} +\\frac{1}{6}\\big) = t + \\frac{1}{3}$  \n",
    "\n",
    "http://www.wolframalpha.com/input/?i=2*integral+from+0+to+1+of+(1-x)(t%2Bx)dx\n",
    "\n",
    "i.e. this confirms that the approach from using Blackwell does match with what we conjectured using discrete approximations  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50*1.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.022719328905976933"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "65/2861"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.666666666666667"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now applying Wald's Equality, we have \n",
    "\n",
    "(t + 1/3) /0.5 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rescaled amount is below.  Result is the same, but cleaner to think about\n",
      "3.671283\n",
      "4.0 \n",
      "-------\n",
      "\n",
      "4.000352777094225\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "# done in python 3.x.  from __future__ imports may be needed in 2.x\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def run_sum(n_trials, threshold_time, time_per_X):\n",
    "    \"\"\"\n",
    "    where X arrives uniform in [0, time_per_X]\n",
    "    \"\"\"\n",
    "    counter = 0\n",
    "    # this is the main result we are actually interested in\n",
    "    alt_counter = 0\n",
    "    # this alternative counter gives partial credit for the delta's\n",
    "    for _ in range(n_trials):\n",
    "        local_counter = 0\n",
    "        local_alt_counter = 0\n",
    "        running_sum = 0\n",
    "        while True:\n",
    "            arrival = np.random.random()*time_per_X\n",
    "            running_sum += arrival\n",
    "            if running_sum >= threshold_time:\n",
    "                local_alt_counter += (arrival - (running_sum - threshold_time))/time_per_X\n",
    "                # i.e. incremented by that delta\n",
    "                # you can ignore the division of 'time_per_X' when it is equal to one,\n",
    "                # and that is probably the best way to think about this problem\n",
    "                break\n",
    "            local_counter += 1\n",
    "            local_alt_counter += 1\n",
    "       \n",
    "        counter += local_counter\n",
    "        alt_counter += local_alt_counter\n",
    "    mu = time_per_X / 2\n",
    "    naive_amount = threshold_time / mu\n",
    "    return counter / n_trials, naive_amount, alt_counter/n_trials\n",
    "\n",
    "\n",
    "N_TRIALS = 1000000\n",
    "\n",
    "print(\"rescaled amount is below.  Result is the same, but cleaner to think about\")\n",
    "actual_amount, naive_amount, other_metric = run_sum(N_TRIALS, threshold_time = 2, time_per_X = 1)\n",
    "print(actual_amount)\n",
    "print(naive_amount, \"\\n-------\\n\")\n",
    "print(other_metric)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3266666666666711"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/6*(10*n - 1/n + 9)/mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3266666666666667"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/6*(10*n - 1/n + 9)/((n+1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2666666666666671"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q@x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.33333333,  0.26666667,  0.2       ,  0.13333333,  0.06666667])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# the lower bound\n",
    "for k in range(1,n+1):\n",
    "    my_sum += (k-1)*(n-k +1)/n\n",
    "#     wannabe_q[k-1] += (k-1)*(n-k +1)/n\n",
    "my_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
