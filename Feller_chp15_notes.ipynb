{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This section really is a mixture of key topics from chapter 13 and chapter 15 of Feller volume 1, as well as some other closely related ideas like reversibility and exponential tilting of the residual life matrix (if the 'probability distribution' doesn't sum to one)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Renewal Matrix - Age\n",
    "This is example 2l) from chp 15, page 382 of Feller Vol. 1, 3rd edition. There are more moving parts to this matrix than the residual life matrix.  This is also page 184 of Ross & Pekoz, though they index at zero, not one there.  (In fact indexing at zero is a lot more convenient in this case.  However to keep with the presentation elsewhere in this writeup, and more importantly, to tie in to into the various extensions cited in Feller, this writeup will use indexing at 1.)  \n",
    "\n",
    "\n",
    "$\\mathbf A =\n",
    "\\left[\\begin{matrix}\n",
    "q_1 & p_1 & 0 & 0 & 0 & 0 & \\dots\\\\\n",
    "q_2 & 0 & p_2& 0 & 0 & 0 & \\dots\\\\\n",
    "q_3 & 0 & 0 & p_3 & 0 & 0 & \\dots\\\\\n",
    "q_4 & 0 & 0 & 0 & p_4 & 0 & \\dots\\\\\n",
    "q_5 & 0 & 0 & 0 & 0 & p_5 & \\dots\\\\\n",
    "q_6 & 0 & 0 & 0 & 0 & 0 & \\ddots\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots\\end{matrix}\\right]$  \n",
    "\n",
    "where for avoidance of doubt $q_k + p_k = 1$  and the top row refers to age zero \n",
    "\n",
    "hence row $i$ of the matrix refers to state $i - 1$ \n",
    "\n",
    "recall from page 308 of Feller that \n",
    "\n",
    "$f_k = Pr\\{\\text{Renewal occurs for the first time at the kth trial}\\}$  \n",
    "\n",
    "for $k = 1, 2, 3, ...$  \n",
    "- - - - \n",
    "\n",
    "Thus if we are on say row 3, this is state 2, and we want the conditional probability of a renewal given that we are in state 2.  So -- again with a pinch of awkwardness on 0 vs 1 indexing-- focus on the row number, this reads as \n",
    "\n",
    "$q_3 = Pr\\{ \\text{renewal at next iteration given at state 2 (row 3)}\\} = Pr\\{f_3 \\big \\vert f_j \\geq 3\\} = \\frac{f_3}{\\sum_{j \\geq 3} f_j}  = \\frac{f_3}{1- (f_1  + f_2)}$  \n",
    "\n",
    "and for some general state we have \n",
    "\n",
    "$q_k = Pr\\{ \\text{renewal at next iteration given at state k-1 (row k)}\\} = Pr\\{ f_k \\big \\vert f_j \\geq k\\} = \\frac{f_k}{\\sum_{j \\geq k} f_j} = \\frac{f_k}{1- (f_1  + f_2 + ... + f_{k-1})}$  \n",
    "\n",
    "and of course: \n",
    "\n",
    "$p_k = 1-q_k = 1 - \\frac{f_k}{1- (f_1  + f_2 + ... + f_{k-1})}  = \\frac{1- (f_1  + f_2 + ... + f_{k-1})}{1- (f_1  + f_2 + ... + f_{k-1})} - \\frac{f_k}{1- (f_1  + f_2 + ... + f_{k-1})} = \\frac{1- (f_1  + f_2 + ... + f_{k-1} + f_k)}{1- (f_1  + f_2 + ... + f_{k-1})} $\n",
    "\n",
    "which agrees with equation (2.5) (on page 382) of Feller.  \n",
    "\n",
    "\n",
    "**Technical Note: The above sums are fine for a non-defective renewal process.  However positing that they sum to one does not immediately follow for defective renewal processes. That is, the conditional probabilities for defective random variables is rather pecuiliar concept.  We address that the above modelling in fact correctly addresses defective renewal processes, via the below writeup.**   \n",
    "\n",
    "$p_k= \\frac{1- (f_1  + f_2 + ... + f_{k-1} + f_k)}{1- (f_1  + f_2 + ... + f_{k-1})}$ \n",
    "\n",
    "not that the probability of no renewal after $n$ trials is given by   \n",
    "$Pr\\{\\text{no renewal at n trials}\\} = \\prod_{k=1}^n p_k = \\prod_{k=1}^n \\frac{1- (\\sum_{i=1}^k f_k)}{1 - (\\sum_{j=1}^{k-1} f_j)} =  \\frac{1- (\\sum_{i=1}^1 f_k)}{1 - (\\sum_{j=1}^{0} f_j)}\\cdot\\frac{1- (\\sum_{i=1}^2 f_k)}{1 - (\\sum_{i=1}^{1} f_j)}\\cdot\\frac{1- (\\sum_{i=1}^3 f_k)}{1 - (\\sum_{i=1}^{2} f_j)}\\cdot\\frac{1- (\\sum_{i=1}^4 f_k)}{1 - (\\sum_{i=1}^{3} f_j)}...\\frac{1- (\\sum_{i=1}^n f_i)}{1 - (\\sum_{i=1}^{n-1} f_j)}$  \n",
    "\n",
    "we could take advantage of positivity and explicitly work in logspace here -- in such a case we'd identify the above relation as a telescoping sum.  The point is that the numerator of term $k-1$ cancels (is equivalent to) the denominator of term $k$.  Thus we can simplify the product as:  \n",
    "\n",
    "$Pr\\{\\text{no renewal at n trials}\\} = \\prod_{k=1}^n p_k = \\frac{1}{1 - (\\sum_{j=1}^{0} f_j)} \\cdot \\frac{1- (\\sum_{i=1}^n f_i)}{1 }= 1- (\\sum_{i=1}^n f_i)$  \n",
    "\n",
    "$Pr\\{\\text{no renewals}\\} = \\lim_{n \\to \\infty} Pr\\{\\text{no renewal at n trials}\\} =  \\lim_{n \\to \\infty} \\Big(1- (\\sum_{i=1}^n f_i)\\Big) = 1- (\\sum_{i=1}^\\infty f_i)$  \n",
    "\n",
    "And we recover the textbook definition of a defective (discrete) renewal process: where said process is defective iff the sum over all probabilities of having a first renewal is less than one.  \n",
    "\n",
    "- - - -  \n",
    "Note that in practice evaluating an infinite product is not as easy as evaluating an infinite sum. If given a chain like this, it is generally preferable to evaluate \n",
    "\n",
    "$\\sum_{k=1}^\\infty q_k$  \n",
    "\n",
    "and in particular to determine whether $\\sum_{k=1}^\\infty q_k \\lt \\infty$.  This in fact gives us the same information as the infinite product with respect to whether or not renewal process is defective, as discussed below.  \n",
    "\n",
    "\n",
    "Further note that condioned on starting in the top left corner (state 0), the expected time until first renewal for a non-defective renewal process can be calculated as \n",
    "\n",
    "$\\mu = 1\\cdot f_1 + 2\\cdot f_2 + 3\\cdot f_3 + 4\\cdot f_4 + ... $  \n",
    "\n",
    "If we use out Renewal Matrix Chain, we see \n",
    "\n",
    "$\\mu = 1\\cdot q_1 + 2\\cdot q_2 p_1 + 3\\cdot q_3 p_1 p_2 + 4\\cdot q_4 p_1 p_2 p_3 + ... = 1\\cdot f_1 + 2\\cdot f_2 + 3\\cdot f_3 + 4\\cdot f_4 + ...$  \n",
    "\n",
    "as desired, \n",
    "\n",
    "because in general the $n$th term for natural number $n \\geq 2$ is given by \n",
    "\n",
    "$n \\cdot  \\big(q_n\\big)\\big( \\prod_{i=1}^{n-1} p_i\\big) =  n \\cdot \\big(\\frac{f_n}{1- (f_1  + f_2 + ... + f_{n-1})}\\big) \\big( 1- (\\sum_{i=1}^{n-1} f_k)\\big) = n\\cdot f_n$  \n",
    "\n",
    "- - - - \n",
    "where we used the 'telescoping' result from above which showed:    \n",
    "$Pr\\{\\text{no renewal at n trials}\\} = \\prod_{k=1}^n p_k = \\frac{1}{1 - (\\sum_{j=1}^{0} f_j)} \\cdot \\frac{1- (\\sum_{i=1}^n f_k)}{1 }= 1- (\\sum_{i=1}^n f_k)$  \n",
    "\n",
    "and by construction  \n",
    "$q_1 = f_1$  \n",
    "$q_k = Pr\\{ \\text{renewal at next iteration given at state k-1 (row k)}\\} = Pr\\{ f_k \\big \\vert f_j \\geq k\\} = \\frac{f_k}{\\sum_{j \\geq k} f_j} = \\frac{f_k}{1- (f_1  + f_2 + ... + f_{k-1})}$\n",
    "\n",
    "(though this writeup used $k$ not $n$ to index for this particular item earlier on)  \n",
    "- - - - \n",
    "**steady state **  \n",
    "\n",
    "For (concise) completeness in addressing steady state issues, we'll assume the renewal chain is aperiodic, and we'll verify that for a non-defective renewal process: \n",
    "\n",
    "(the emphasis here is on the positive recurrent case.  In the null recurrenct case we have $\\mu = \\infty$ and $\\frac{1}{\\mu} = 0$ but some additional care is needed to verify the results... it is worth recalling Lemma 5.14 on page 152 of Ross & Pekoz which states that for an irreducible chain, a steady state vector implies that the chain is positive recurrent)   \n",
    "\n",
    "$\\mathbf \\pi \\mathbf A = \\mathbf \\pi$  \n",
    "\n",
    "where   \n",
    "$\\mathbf {\\pi} = \\begin{bmatrix}\n",
    "\\pi_1\\\\ \n",
    "\\pi_2\\\\ \n",
    "\\pi_3\\\\ \n",
    "\\vdots \\\\ \n",
    "\\pi_i\\\\ \n",
    "\\vdots\n",
    "\\end{bmatrix}^T = \\frac{1}{\\mu} \\begin{bmatrix}\n",
    "1\\\\ \n",
    "p_1  \\\\ \n",
    "p_1p_2\\\\ \n",
    "\\vdots \\\\ \n",
    "\\prod_{j=1}^{r-1} p_j\\\\ \n",
    "\\vdots\n",
    "\\end{bmatrix}^T = \\frac{1}{\\mu} \\begin{bmatrix}\n",
    "Pr\\{\\text{1st Renewal time}\\gt 0\\}\\\\ \n",
    "Pr\\{\\text{1st Renewal time}\\gt 1\\} \\\\ \n",
    " Pr\\{\\text{1st Renewal time}\\gt 2\\}\\\\ \n",
    "\\vdots \\\\ \n",
    "\\Pr\\{\\text{1st Renewal time}\\gt r-1\\}\\\\ \n",
    "\\vdots\n",
    "\\end{bmatrix}^T $  \n",
    "\n",
    "that is, we shall confirm that our steady state vector is in fact a steady state.  First, the expedient way: \n",
    "\n",
    "- - - -  \n",
    "notice that \n",
    "the complementary CDF for a renewal's interarrival time is given by \n",
    "$Pr\\{X\\gt k\\}= \\prod_{i=1}^k (1-q_i)  = \\prod_{i=1}^k p_i$  \n",
    "\n",
    "so \n",
    "$\\mu = 1 + p_1 + p_1p_2 + p_1p_2p_3 + ... = \\sum_{k=0}^\\infty Pr\\{X\\gt k\\} = \\mathbf 1^T \\begin{bmatrix}\n",
    "1\\\\ \n",
    "p_1  \\\\ \n",
    "p_1p_2\\\\ \n",
    "\\vdots \\\\ \n",
    "\\prod_{j=1}^{r-1} p_j\\\\ \n",
    "\\vdots\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "hence the steady state distribution vector, pre-normalization, sum to $\\mu$ (and in the positive recurrent case we know $\\mu \\lt \\infty$ so we divide by it to get a proper steady state vector).  \n",
    "\n",
    "The elementary renewal theorem would then state that the time averaged amount of time in the inital state (renewal) of the markov chain is given by $\\frac{1}{\\mu}$ which must be the limitting distribution value (after we've proven that a limit exists, etc.)    \n",
    "\n",
    "Given that we now have a 'toehold' in component 1, the other values follow by the simplicity of the graph-- in particular and solving the balance equations.  \n",
    "- - - -  \n",
    "Alternatively, for state 1, this is equivalent to confirming that   \n",
    "\n",
    "$\\pi_1 = \\frac{1}{\\mu}=\\sum_k \\big(q_k\\big)\\big( \\pi_k\\big) = \\sum_k \\big(\\frac{f_k}{1- (f_1  + f_2 + ... + f_{k-1})} \\big)\\big(\\frac{1}{\\mu} \\prod_{j=1}^{k-1} p_j \\big)  =  \\frac{1}{\\mu} \\sum_k \\big(\\frac{f_k}{1- (f_1  + f_2 + ... + f_{k-1})} \\big)\\big(1- (\\sum_{j=1}^{k-1} f_j)\\big)  =  \\frac{1}{\\mu} \\sum_k f_k =  \\frac{1}{\\mu}$   \n",
    "\n",
    "for component $r \\geq 2$ we have \n",
    "\n",
    "$\\frac{1}{\\mu} \\prod_{j=1}^{r-1} p_j= \\pi_r =  p_{r-1} \\cdot \\pi_{r-1} =\\big(p_{r-1} \\big)\\big(\\frac{1}{\\mu} \\prod_{j=1}^{(r-1)-1} p_j \\big)= \\frac{1}{\\mu} \\big(p_{r-1} \\big)\\big( \\prod_{j=1}^{r-2} p_j \\big) = \\frac{1}{\\mu} \\cdot \\prod_{j=1}^{r-1} p_j $  \n",
    "\n",
    "both, as desired, hence \n",
    "\n",
    "$\\mathbf \\pi \\mathbf A = \\mathbf \\pi$  \n",
    "\n",
    "is in fact a steady state vector  \n",
    "\n",
    "- - - - \n",
    "\n",
    "\n",
    "# The below is a workthrough of (8.5), contained on page 400\n",
    "These seem to be standard results from analysis, though your author felt the need to go through them in some detail.  In particular it is very handy for interpretting when the above renewal age matrix is transient \n",
    "\n",
    "supposing we have probabilities of $0 \\leq \\epsilon_k \\lt 1$ (note in the case of $\\epsilon_k = 0$, we immediately see that the finite product is zero, and hence after passage to the limit it is still zero)\n",
    "\n",
    "of being absorbed on a countable state markov chain, at iteration $k$, then this means the probabilty of not being absorbed over trials $\\{j, j+1, ..., j+n -1\\}$ is given by \n",
    "\n",
    "$(1-\\epsilon_{j})(1-\\epsilon_{j+1})...(1-\\epsilon_{j+n-1}) = \\prod_{k = j}^{j+n-1} (1-\\epsilon_{k})$    \n",
    "\n",
    "Feller then states that \n",
    "\n",
    "$0 \\lt \\lim_{n \\to \\infty} \\prod_{k = j}^{j+n-1} (1-\\epsilon_{k}) $ \n",
    "\n",
    "**iff** $\\sum_k \\epsilon_{k} \\lt \\infty$  \n",
    "\n",
    "or equivalently\n",
    "\n",
    "$0 \\neq \\lim_{n \\to \\infty} \\prod_{k = j}^{j+n-1} (1-\\epsilon_{k}) $ \n",
    "- - -- \n",
    "\n",
    "This writeup flushes out why that is true.  \n",
    "\n",
    "In particular we'll consider the cases where that series is finite and where it is infinite.  For avoidance of doubt, that the finite case *could* be written as $\\sum_k -\\epsilon_{k} \\gt -\\infty$ or even $\\sum_k -2\\epsilon_{k} \\gt -\\infty$.  \n",
    "\n",
    "*key bounds:* \n",
    "\n",
    "make use of the fact that $\\big \\vert \\epsilon_k\\big \\vert \\lt 1$ and use the power serries for the natural logarithm so that we may work with sums instead of products.  \n",
    "\n",
    "In (natural) logspace our claim is \n",
    "\n",
    "the probability is not 0 **iff**\n",
    "\n",
    "$-\\infty \\lt \\lim_{n \\to \\infty} \\sum_{k=j}^{j+n-1} \\log(1-\\epsilon_k) $  \n",
    "\n",
    "equivalently, the probability is zero iff the above series has no lower bound.  \n",
    "\n",
    "We note that each term in the above series is bounded above by zero (i.e. everything is real non-positive) so there is no ambiguity in using $-\\infty$.  \n",
    "\n",
    "now consider the power series expansion for the natural log where $\\big \\vert x \\big \\vert \\lt 1$  \n",
    "\n",
    "$\\log\\big(1 + x\\big) = x -\\frac{x^2}{2} + \\frac{x^3}{3} - \\frac{x^4}{4}+...$ \n",
    "\n",
    "in our case, where each term in the series, we have \n",
    "\n",
    "$\\log\\big(1 - \\epsilon_k\\big) = \\log\\big(1 + (- \\epsilon_k)\\big) = (- \\epsilon_k) -\\frac{(- \\epsilon_k)^2}{2} + \\frac{(- \\epsilon_k)^3}{3} - \\frac{(- \\epsilon_k)^4}{4}+... = - \\epsilon_k -\\frac{\\epsilon_k^2}{2} - \\frac{ \\epsilon_k^3}{3} - \\frac{\\epsilon_k^4}{4}- = \\sum_{k=1}^\\infty -\\frac{\\epsilon_k^k}{k} = -\\sum_{k=1}^\\infty \\frac{\\epsilon_k^k}{k}$ \n",
    "\n",
    "recalling that $0 \\leq \\epsilon_k$ so each and every term in that series is 0 if $\\epsilon_k = 0$ (i.e. we recover the fact that $\\log\\big(1\\big) = 0$, otherwise each and every term in the series is negative.  \n",
    "**for the upper bound on our probability, we may simply recognize ** \n",
    "\n",
    "We thus have a term by term bound of  \n",
    "\n",
    "$\\sum_{k=1}^\\infty -\\frac{\\epsilon_k^k}{k} = -\\epsilon_k -\\sum_{k=2}^\\infty \\frac{\\epsilon_k^k}{k} \\leq  -\\epsilon_k$  \n",
    "\n",
    "(Alternatively we could recognize that the tangent line of $\\log(1+x)$ at $x =0$ has a slope of one and intercept of zero.  By the negative convexity of the natural logarithm, this means that $\\log(1+x) \\leq x$ for all $x$ in our domain.  This is of course equivalent to the familiar bound of $1 + x \\leq e^x$ and then taking natural logs of both sides.)  \n",
    "\n",
    "\n",
    "Hence, we have \n",
    "\n",
    "$-\\infty \\lt  \\sum_{k=j}^{j+n-1} \\log(1-\\epsilon_k) \\leq \\sum_{k=j}^{j+n-1} -\\epsilon_k$ \n",
    "\n",
    "now if the epsilon series diverges, we have \n",
    "$\\sum_k -\\epsilon_{k} = -\\infty$  \n",
    "\n",
    "passing limits to our inequality, we have \n",
    "\n",
    "$-\\infty \\leq \\lim_{n \\to \\infty} \\sum_{k=j}^{j+n-1} \\log(1-\\epsilon_k) \\leq \\lim_{n \\to \\infty} \\sum_{k=j}^{j+n-1} -\\epsilon_k = \\sum_k -\\epsilon_{k} = -\\infty$  \n",
    "\n",
    "which confirms that the probability(/ infinite product) goes to zero when the $\\sum_k -\\epsilon_{k} = -\\infty$  \n",
    "\n",
    "or equivalently that the probability(/ infinite product) goes to zero when the $\\sum_k \\epsilon_{k} = \\infty$  \n",
    "- - -- \n",
    "**for the lower bound: ** \n",
    "\n",
    "we use a geometric series\n",
    "\n",
    "$-\\log\\big(1 - \\epsilon_k\\big) = \\sum_{k=1}^\\infty \\frac{\\epsilon_k^k}{k} \\lt \\sum_{k=1}^\\infty \\epsilon_k^k = \\frac{\\epsilon}{1-\\epsilon_k}$   \n",
    "\n",
    "noting that for any non-zero $\\epsilon_k$ the inequality is strict because the term by term inequality is strict for all $k\\geq 2$  \n",
    "\n",
    "to extend this, we may say \n",
    "\n",
    "$-\\log\\big(1 - \\epsilon_k\\big) \\lt \\frac{\\epsilon}{1-\\epsilon_k} \\lt 2\\epsilon_k$   \n",
    "\n",
    "for sufficiently small $\\epsilon_k$  (specifically all $\\epsilon_k \\lt \\frac{1}{2}$ )  \n",
    "\n",
    "\n",
    "Now multiplying both sides by negative one we have \n",
    "\n",
    "$\\log\\big(1 - \\epsilon_k\\big) \\gt \\frac{-\\epsilon}{1-\\epsilon_k}$   \n",
    "\n",
    "or \n",
    "\n",
    "$\\frac{-\\epsilon}{1-\\epsilon_k} \\lt \\log\\big(1 - \\epsilon_k\\big) $   \n",
    "\n",
    "and again, with sufficiently small $\\epsilon_k$ we have \n",
    "\n",
    "$ -2\\epsilon_k \\lt \\frac{-\\epsilon}{1-\\epsilon_k} \\lt \\log\\big(1 - \\epsilon_k\\big) $  \n",
    "\n",
    "or more succinctly\n",
    "\n",
    "$ -2\\epsilon_k \\lt \\log\\big(1 - \\epsilon_k\\big) $  \n",
    "\n",
    "Now if our epsilon series converges i.e. $\\sum_k -\\epsilon_{k} \\gt -\\infty$ or in equivalent non-negative terms: $\\sum_k \\epsilon_{k} \\lt \\infty$, then there can only be finitely many $\\epsilon_k \\gt 0.5$ -- suppose for a contradiction that this is not true, then the real non-negative form of the infinite series may be lower bounded by $ \\infty = \\sum_k 0.5 \\leq \\sum_k \\epsilon_{k} \\leq \\infty$ which contradicts the fact that we've assumed the series converges.  Hence there must be at most $m$ terms (where $m$ is some finite natural number) that are not sufficiently small.  \n",
    "\n",
    "Our logspace infinite series consists entirely of real non-positive values (and recall there is no 'funny business' like $\\log(0)$ -- the logarithm is taken on values greater than zero and less than or equal to one), hence cannot diverge due to the first $m$ terms (or any sum of a finite number of terms, of course). That is, it diverges, **iff** the tail diverges (i.e. the sum of all but the first $m$ terms).  \n",
    "\n",
    "Hence we bound the tail with \n",
    "\n",
    "$ -\\infty \\lt -2c = -2 \\sum_{k} \\epsilon_k \\lt -2 \\sum_{k=j}^{\\infty} \\epsilon_k \\lt -2 \\sum_{k=j+m}^{\\infty} \\epsilon_k= \\lim_{n \\to \\infty} \\sum_{k=j+m}^{j+n-1+m} -2\\epsilon_k  \\lt \\lim_{n \\to \\infty} \\sum_{k=j+m}^{j+n-1+m}\\log\\big(1 - \\epsilon_k\\big)$  \n",
    "\n",
    "i.e.the tail of our logspace series is bounded below by $-2c$ for some finite, real non-negative $c$, which means that the logspace series is finite, and hence the infinite product that gives our probability is $\\gt 0$. \n",
    "\n",
    "This proves the claim in the book.  (Though the lower bound argument could probably be cleaned up a bit).  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renewal Matrix -- residual life\n",
    "\n",
    "Consider for example, this matrix\n",
    "\n",
    "$\\mathbf A_7 = \\left[\\begin{matrix}p_1 & p_2 & p_3 & p_4 & p_5 & p_6 & p_7 \\\\1 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 1 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 1 & 0\\end{matrix}\\right]$  \n",
    "\n",
    "and in general in finite dimensions, we have \n",
    "\n",
    "$\\mathbf A_n = \n",
    "\\left[\\begin{matrix}\n",
    "p_1 & p_2 & p_3 & p_4 & p_5 & \\dots & p_n \\\\\n",
    "1 & 0 & 0 & 0 & 0 & \\dots & 0\\\\\n",
    "0 & 1 & 0 & 0 & 0 & \\dots & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0 & \\dots & 0\\\\\n",
    "0 & 0 & 0 & 1 & 0 & \\dots & 0\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n",
    "0 & 0 & 0 & 0 & 0 & 1 & 0\\end{matrix}\\right]$  \n",
    "\n",
    "and in countably infinite dimensions we have \n",
    "\n",
    "$\\mathbf A_{\\infty} = \n",
    "\\left[\\begin{matrix}p_1 & p_2 & p_3 & p_4 & p_5 & p_6 & \\dots \\\\\n",
    "1 & 0 & 0 & 0 & 0 & 0 & \\dots\\\\ \n",
    "0 & 1 & 0 & 0 & 0 & 0 & \\dots\\\\\n",
    "0 & 0 & 1 & 0 & 0 & 0 & \\dots\\\\\n",
    "0 & 0 & 0 & 1 & 0 & 0 & \\dots\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\ddots & \\ddots\\\\\n",
    "\\end{matrix}\\right]$  \n",
    "\n",
    "\n",
    "What is interesting about all 3 of these matrices is that they are a particular kind of renewal matrix. Reference for example 2k on p. 381 of Feller volume 1 (3rd edition).  This is a matrix to model recurrent events and residual wait times. It also can be interpretted as the matrix for repeated averaging given by 10c on page 333 of Feller.  \n",
    "\n",
    "Understanding the underlying Markov Chain here can give us lots of insights into Renewal Theory, and of course Renewal Theory can give us lots of inights into how this chain works.  \n",
    "\n",
    "It is further interesting that in the finite dimensional case, $\\mathbf A$ is, up to a graph isomorphism, similar to the Companion matrix. We can effect the similarity tranform with the reflection matrix $\\mathbf J$, which is involutary and a permutation matrix.  (Note: if something is a graph isomorphism, one may ask why not just model the problem so that the top row is on the bottom row from the outset -- the we wouldn't need to do these similarity transforms.  The reality is that we could have, but then it would be quite difficult to talk about the countably infinite case -- we could of course just have labelled the finite graph two different ways and the countably infinite one in one way-- in effect that *is* what we're doing with the similarity transform below.)  \n",
    "\n",
    "For example, consider:  \n",
    "\n",
    "$\\mathbf J_7 = \\left[\\begin{matrix}0 & 0 & 0 & 0 & 0 & 0 & 1\\\\0 & 0 & 0 & 0 & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1& 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0 & 0 & 0\\\\1 & 0 & 0 & 0 & 0 & 0 & 0\\end{matrix}\\right]$  \n",
    "- - - - \n",
    "\n",
    "$\\mathbf J \\mathbf C \\mathbf J^{-1} = \\mathbf J \\mathbf C \\mathbf J =\\left[\\begin{matrix}0 & 1 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 1 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 1\\\\p_7 & p_6 & p_5 & p_4 & p_3 & p_2 & p_1\\end{matrix}\\right]$  \n",
    "\n",
    "which is the transpose of the Companion matrix.  \n",
    "- - - - \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Steady States of above matrix **  \n",
    "\n",
    "\n",
    "note that the above $\\mathbf A_n$ is row stochastic, so   \n",
    "$\\mathbf A_n \\mathbf 1 = \\mathbf 1$   \n",
    "\n",
    "(Note what is said below applies almost verbatim for the countably infinite case so we tackle both at the same time)  \n",
    "\n",
    "as for the dominant left eigvector / steady state probability distribution given by\n",
    "\n",
    "$\\mathbf \\pi \\mathbf A = \\mathbf \\pi $?  \n",
    "\n",
    "let's first assume that $p_1 \\gt 0$.  \n",
    "\n",
    "This immediately tells us that the chain is aperiodic.  (If however, for example $p_i=0$ for odd $i$, then the chain would have period 2. We could also just insist on some other sufficient condition like having at least one odd and at least one even indexed probability be non-zero and that would give aperiodicity.)  \n",
    "\n",
    "Next: there is, by construction one communicating class here.  Now to avoid special handling for the countable case let's assume that recurrence time from state 1 to state 1 is finite--  given by $\\bar{X} \\lt \\infty$ \n",
    "\n",
    "\n",
    "The steady state values are \n",
    "\n",
    "$1 = \\pi_1 + \\pi_2 + \\pi_3 + ... $  \n",
    "\n",
    "there is a natural relation / recurrence between these values, however.  In particular, consider that state $1$ has steady state probabilities 'flowing' in from all states, while state $2$ has probability flowing in from all states except transition probability $p_1$.  \n",
    "\n",
    "that is for state 1 we have the trivially true:  \n",
    "\n",
    "$\\pi_1 = \\pi_1\\big(p_1 + p_2 + p_3 + ...\\big) = \\pi_1 \\big(1\\big) $    \n",
    "\n",
    "but for state 2 we have the more interesting: \n",
    "\n",
    "$\\pi_2 = \\pi_1\\big(p_2 + p_3 +  ...\\big) $ \n",
    "\n",
    "for state 3 we have \n",
    "\n",
    "$\\pi_3 = \\pi_1\\big(p_3 + p_4 +  ...\\big)  $ \n",
    "\n",
    "and for state $k$ we have \n",
    "\n",
    "$\\pi_k = \\pi_1 \\cdot \\sum_{i\\geq k} p_i $ \n",
    "\n",
    "\n",
    "substituting back into our equation, we have \n",
    "\n",
    "$1 = \\pi_1 + \\pi_2 + \\pi_3 + ... = \\pi_1  \\cdot \\sum_k \\big(\\sum_{i\\geq k} p_i\\big) = \\pi_1  \\cdot \\sum_k Pr\\{X \\gt (k-1)\\} = \\pi_1  \\cdot \\bar{X} $  \n",
    "\n",
    "\n",
    "which immediately gives us \n",
    "\n",
    "$\\pi_1  = \\frac{1}{\\bar{X}} = \\frac{ Pr\\{X \\gt (0)\\}}{\\bar{X}} $ \n",
    "\n",
    "which we (hopefully!) already knew. However it also tells us \n",
    "\n",
    "$\\pi_2 = \\pi_1  - \\frac{p_1 }{\\bar{X}} =  \\frac{1 - p_1 }{\\bar{X}} =  \\frac{ Pr\\{X \\gt (1)\\}}{\\bar{X}} $  \n",
    "\n",
    "$\\pi_3 = \\pi_1  - \\frac{p_1  + p_2}{\\bar{X}} = \\frac{1 - (p_1  + p_2)}{\\bar{X}} =  \\frac{ Pr\\{X \\gt (2)\\}}{\\bar{X}}  $  \n",
    "\n",
    "and in general \n",
    "\n",
    "$\\pi_k  = \\pi_1  - \\frac{p_1  + p_2  + ... + p_{k-1}}{\\bar{X}} = \\frac{1 - (p_1  + p_2  + ... + p_{k-1})}{\\bar{X}} = \\frac{ Pr\\{X \\gt (k-1)\\}}{\\bar{X}}  $  \n",
    "\n",
    "where, for avoidance of doubt, for $k \\geq 2$, we have the recurrence:  \n",
    "\n",
    "$\\pi_{k}  = \\pi_{k-1} - \\frac{p_{k-1}}{\\bar{X}}  $  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A closer look at the above two chains from the viewpoint of reversibility \n",
    "(see page 128-130 of Ross's Stochastic Processes, 1st edition)  \n",
    "\n",
    "note that by examining the graph, we can see that the detailed balance equations for say the Renewal Age matrix, can never be satisfied, so **the markov chain in question is *not* time reversible**.  However, we can still try to apply techniques associated with reversibility to it and create a new, reversed Markov chain that is the the reverse of this Aging process.  \n",
    "\n",
    "so, since we already know the steady-state probabilities for our positive recurrent chain, we can consider, where our original matrix/chain is given by $P$, the new reversed chain is given by $P^*$  \n",
    "\n",
    "and we want to create this new chain by applying the detailed balance condition \n",
    "\n",
    "$\\pi_i P_{i,j} = \\pi_j P^*_{j,i}$  \n",
    "\n",
    "the $\\pi_k$ are the stationary probabilities for the original chain (and as we'll see, they are the stationary probabilities for the new reversed chain as well)  \n",
    "\n",
    "- - - - -\n",
    "so the new chain has probabilities given by \n",
    "\n",
    "$P^*_{j,i} = \\frac{\\pi_i}{\\pi_j} P_{i,j} $    \n",
    "note this is in column stochastic form, because  \n",
    "\n",
    "$\\sum_{i}P^*_{j,i} = \\sum_{i}\\frac{\\pi_i}{\\pi_j} P_{i,j} = \\frac{1}{\\pi_j} \\sum_{i} \\pi_i P_{i,j} = \\frac{1}{\\pi_j} \\cdot \\pi_j = 1$    \n",
    "\n",
    "where we know $\\pi_j = \\sum_{i} \\pi_i P_{i,j}$ by the fact that these are stationary probabilities for our original chain.  But this, in essence, proves that the stationary probabilities for our new reversed chain are the same as those for our original chain.  In particular we have \n",
    "\n",
    "$\\pi_j = \\pi_j\\big(\\sum_{i} P^*_{j,i}\\big) = \\sum_{i}\\pi_j P^*_{j,i}= \\sum_{i}\\pi_i P_{i,j}$    \n",
    "\n",
    "and because the $\\pi_k$ were stationary probabilities for our original chain, we know $\\sum_k \\pi_k =1$  \n",
    "\n",
    "These two condtions thus verify that $\\pi_j$ are the stationary probabilities for our reversed chain (recall e.g. the \"Guessing Theorem\" for stationary probabilities, e.g. Remark 5.17 on page 156 of Ross and Pekoz).   \n",
    "\n",
    "- - - \n",
    "\n",
    "so in the case of our reversed chain (where row stochastic is the standard), and for convenience, we ignored the normalizing constant $\\frac{1}{\\mu}$ because it cancels for all $\\frac{\\pi_i}{\\pi_j}$, we have  \n",
    "\n",
    "note: if we place the steady state distributions along the diagonal of a diagonal matrix $\\mathbf D$ (i.e. so $d_{k,k} = \\pi_k$ ), what we have is \n",
    "\n",
    "$\\big(P^*\\big)^T = \\mathbf D^{-1} \\mathbf P \\mathbf D$  \n",
    "or  \n",
    "$\\big(P^*\\big) = \\mathbf D^T \\mathbf P^T \\mathbf D^{-T}= \\mathbf D \\mathbf P^T \\mathbf D^{-1}$  \n",
    "(so the transition matrix for the reversed chain is similar to the transpose of the transition matrix for the original chain)  \n",
    "\n",
    "\n",
    "$\\big(P^*\\big)^T = \n",
    "\\left[\\begin{matrix}\n",
    "q_1\\big(\\frac{\\pi_1}{\\pi_1}\\big) & p_1\\frac{\\pi_1}{\\pi_2} & 0 & 0 & 0 & 0 & \\dots\\\\\n",
    "q_2\\big(\\frac{\\pi_2}{\\pi_1}\\big) & 0 & p_2\\frac{\\pi_2}{\\pi_3}& 0 & 0 & 0 & \\dots\\\\\n",
    "q_3\\big(\\frac{\\pi_3}{\\pi_1}\\big) & 0 & 0 & p_3\\frac{\\pi_3}{\\pi_4} & 0 & 0 & \\dots\\\\\n",
    "q_4\\big(\\frac{\\pi_4}{\\pi_1}\\big) & 0 & 0 & 0 & p_4\\frac{\\pi_4}{\\pi_5} & 0 & \\dots\\\\\n",
    "q_5\\big(\\frac{\\pi_5}{\\pi_1}\\big) & 0 & 0 & 0 & 0 & p_5\\frac{\\pi_5}{\\pi_6} & \\dots\\\\\n",
    "q_6\\big(\\frac{\\pi_6}{\\pi_1}\\big) & 0 & 0 & 0 & 0 & 0 & \\ddots\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots\\end{matrix}\\right]=  \n",
    "\\left[\\begin{matrix}\n",
    "\\big(1\\big)q_1  & p_1\\frac{1}{p_1} & 0 & 0 & 0 & 0 & \\dots\\\\\n",
    "\\big(p_1\\big) q_2   & 0 & p_2\\frac{1}{p_2}& 0 & 0 & 0 & \\dots\\\\\n",
    "\\big(p_1p_2\\big) q_3  & 0 & 0 & p_3\\frac{1}{p_3} & 0 & 0 & \\dots\\\\\n",
    "\\big(p_1 p_2 p_3\\big)q_4 & 0 & 0 & 0 & p_4\\frac{1}{p_4} & 0 & \\dots\\\\\n",
    "\\big(\\prod_{k=1}^4 p_k\\big)q_5 & 0 & 0 & 0 & 0 & p_5\\frac{1}{p_5} & \\dots\\\\\n",
    "\\big(\\prod_{k=1}^5 p_k\\big)q_6 & 0 & 0 & 0 & 0 & 0 & \\ddots\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots\\end{matrix}\\right]$   \n",
    "\n",
    "$\\big(P^*\\big)^T = \\left[\\begin{matrix}\n",
    "Pr\\{\\text{trip length 1}\\} & 1 & 0 & 0 & 0 & 0 & \\dots\\\\\n",
    "Pr\\{\\text{trip length 2}\\}  & 0 & 1& 0 & 0 & 0 & \\dots\\\\\n",
    "Pr\\{\\text{trip length 3}\\} & 0 & 0 & 1 & 0 & 0 & \\dots\\\\\n",
    "Pr\\{\\text{trip length 4}\\} & 0 & 0 & 0 & 1 & 0 & \\dots\\\\\n",
    "Pr\\{\\text{trip length 5}\\} & 0 & 0 & 0 & 0 & 1 & \\dots\\\\\n",
    "Pr\\{\\text{trip length 6}\\} & 0 & 0 & 0 & 0 & 0 & \\ddots\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots\\end{matrix}\\right]$ \n",
    "\n",
    "$P^* = \n",
    "\\left[\\begin{matrix}Pr\\{\\text{trip length 1}\\} & Pr\\{\\text{trip length 2}\\} & Pr\\{\\text{trip length 3}\\} & Pr\\{\\text{trip length 4}\\} & Pr\\{\\text{trip length 5}\\} & Pr\\{\\text{trip length 6}\\} & \\dots \\\\\n",
    "1 & 0 & 0 & 0 & 0 & 0 & \\dots\\\\ \n",
    "0 & 1 & 0 & 0 & 0 & 0 & \\dots\\\\\n",
    "0 & 0 & 1 & 0 & 0 & 0 & \\dots\\\\\n",
    "0 & 0 & 0 & 1 & 0 & 0 & \\dots\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\ddots & \\ddots\\\\\n",
    "\\end{matrix}\\right]$  \n",
    "\n",
    "which, setting aside some notational overloading, is precisely our Residual Life matrix!  \n",
    "\n",
    "As noted in the closing remark by Ross, this helps us get a deeper understanding of why the residual life and age distributions for a renewal process tend to the same limitting distribution!  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the remainder of this post considers some interesting niche extensions, using the above chain for exponential tilting  \n",
    "\n",
    "The below starts with a simple and general example, however calculations get more and more detailed as the post progresses.  Once the readers feels saturated with overly detailed calculations, the rest of the post may be safely skipped.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Thus in the finite dimensional case we can read off the characteristic polynomial of the matrix.  We also know that we can use the Companion matrix to model many recurrence relations, which would be the question -- can we use renewal theory insights to easily solve for limiting behavior in more general recurrence relations.  For this generalization, which we attempts first, it is possible, to your authors knowledge for most cases where the recurrence relation has real non-negative coefficients.  \n",
    "\n",
    "For example consider the method suggested by Lalley as exponential tilting on page 9 of here, for use in the Fibonacci recursion.  \n",
    "\n",
    "http://galton.uchicago.edu/~lalley/Courses/383/Renewal.pdf\n",
    "\n",
    "\n",
    "consider the typical Fibonacci matrix  \n",
    "\n",
    "$\\mathbf B= \\begin{bmatrix}\n",
    "1 & 1\\\\ \n",
    " 1& 0\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "and to generate fibonnaci numbers we proceed along the lines of \n",
    "\n",
    "$\\mathbf B^k \\mathbf x$  and so forth.  \n",
    "\n",
    "that is in general the recursion is \n",
    "\n",
    "$x_{k} = x_{k-1} + x_{k-2}$  \n",
    "\n",
    "We could interpret this as $\\mathbf A_2$ except while the top row is real non-negative, it sums to 2, not to 1, and hence can't be interpretted as probabilities.  We also can't quite do straightforward similarity transforms to make that issue with the row go away. The indication is to do a change of variables\n",
    "\n",
    "**tbc: I think the linked notes have it wrong they say: **\n",
    "\n",
    "$z_k := \\theta^{-k}x_k$   \n",
    "** but I think the correct way is** \n",
    "\n",
    "$z_k := \\theta^{k}x_k$   \n",
    "- - - - \n",
    "\n",
    "which makes the recurrence    \n",
    "\n",
    "$z_{k} = z_{k-1}\\theta^1 + z_{k-2}\\theta^2 $  \n",
    "\n",
    "(with additional handling needing for bases cases)  \n",
    "\n",
    "$\\mathbf C= \\begin{bmatrix}\n",
    "\\theta & \\theta^2 \\\\ \n",
    " 1 & 0\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "and is there a $\\theta$ that satisfies\n",
    "\n",
    "$\\theta + \\theta^2 = 1$ \n",
    "\n",
    "or \n",
    "\n",
    "$\\theta^2 + \\theta - 1 =0$  \n",
    "- - - - \n",
    "**remark:** Exponential tilting comes in many varieties  \n",
    "\n",
    "in general we have some function \n",
    "\n",
    "$g(\\theta) \\sum_{k=1}^\\infty c_k \\theta^k := 1$  \n",
    "\n",
    "where $c_k \\geq 0$ with at least one $c_k \\gt 0$.  In particular, in general by construction $c_1 \\gt 0$ (though it is possible that this is not the case in some examples.)  \n",
    "\n",
    "There may be an infinite series or it may be a (finite degree) polynomial.  In all cases $g(0) = 0 \\lt 1$ \n",
    "And in all cases we know the lowest order term has degree $\\geq 1$  and since not all terms are identically zero, then there exists a lowest order term (i.e. pedantically we can start and check $c_1 \\gt 0$ if it's positive we are done.  If its zero, check $c_2$ and so on... such checking monotonically increases but we know there is an finite upper bound, hence a limit exists where we may stop at the optimal minimal degree must exist.)  \n",
    "\n",
    "Letting $c_{min}$ be the first nonzero degree coefficient, and $d$ be the degree, we have \n",
    "\n",
    "$c_{min}\\theta \\leq g(\\theta)$ \n",
    "\n",
    "But if we select \n",
    "\n",
    "$x:= \\big(\\frac{1}{c_{min}}\\big)^\\frac{1}{d}$  \n",
    "\n",
    "$1 = c_{min} \\leq g(x) $  \n",
    "\n",
    "and hence by Intermediate value theorem there is some $\\theta \\in [0, x]$ such that $g(\\theta) = 1$.  \n",
    "\n",
    "Note: there *are* some nits in the infinite case where by possibly $g(x) = \\infty$ i.e. it may not converge at this point.  We should be able to address this with a truncation argument or by selecting $x:= 1-\\delta$ for some small $\\delta \\gt 0$ -- the series should converge as it will be in the radius of convergence of the geometric series.  If it doesn't converge at $1$, then it may be made arbitrarily large as we shrink $\\delta$ and once again this implies a value $\\geq 1$ and we can call on Intermediate Value Theorem here.  \n",
    "\n",
    "- - - - \n",
    "\n",
    "\n",
    "There are two roots here, with one of them positive, which is given by \n",
    "\n",
    "$\\theta = \\frac{-1 + \\sqrt{5}}{2}$  \n",
    "\n",
    "and for avoidance of doubt, note: $0 \\lt \\theta \\lt 1$  which means \n",
    "\n",
    "$1 \\lt \\theta^{-1} $  \n",
    "\n",
    "\n",
    "in this setup we have an expected time until renewal of:  \n",
    "\n",
    "$\\mu = 1  \\theta^1 + 2 \\theta^2$ \n",
    "\n",
    "which tells us that the for large enough $k$ the equation tends to \n",
    "\n",
    "- - - - \n",
    "**what linked notes indicate** \n",
    "$z_k \\approx \\frac{1}{\\mu} = \\frac{1}{\\theta + 2 \\theta^2} = \\theta^{-k}x_k$  \n",
    "\n",
    "- - - - \n",
    "\n",
    "**what I think is correct: ** \n",
    "$z_k \\approx \\frac{1}{\\mu} = \\frac{1}{\\theta + 2 \\theta^2} = \\theta^{k}x_k$  \n",
    "\n",
    "which tells us that our $x_k$ grows exponentially (or technically geometrically in discrete time) with a rate of $\\theta^{-1}$ and a coefficient of $\\frac{1}{\\mu}$.  I.e. we have \n",
    "\n",
    "$x_k \\sim \\frac{\\theta^{-k}}{\\mu}  $  \n",
    "\n",
    "*there are some strong patterns / similarities with interpretting this and some ideas related to generating functions (and having a dummy variable like $\\theta$ where we can see the degree of the generating function by its exponent... for OGFs we typically extract the actual probabilities by setting it equal to one, though this seems like a variation on that theme.*)    \n",
    "\n",
    "note: that due to some cancelations the above works, but it is misleading.  A more technically careful and insightful approach, would use the setup of (c) on page 333 of Feller, which states:  \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "v_{r+k}\\\\ \n",
    "v_{r+k-1}\\\\ \n",
    "\\vdots\\\\ \n",
    "v_{k+2}\\\\ \n",
    "v_{k +1}\n",
    "\\end{bmatrix} = \\mathbf A^k \\mathbf v = \\begin{bmatrix}\n",
    "f_1 & f_2 & f_3 & \\dots  & f_r\\\\ \n",
    "1 & 0 & 0 & \\dots & 0\\\\ \n",
    " 0&  1& 0 & \\dots & 0\\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\ \n",
    "0 & 0 & 0 & 1 & 0\n",
    "\\end{bmatrix}^k\\begin{bmatrix}\n",
    "v_r\\\\ \n",
    "v_{r-1}\\\\ \n",
    "\\vdots\\\\ \n",
    "v_{2}\\\\ \n",
    "v_{1}\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "\n",
    "hence we may use the $1$st standard basis vector $\\mathbf e_1^T$ to \n",
    "$v_{r+k} = \\mathbf e_1^T \\mathbf A^k \\mathbf v = \\mathbf e_1^T \\big(\\mathbf A^k \\mathbf v\\big)$  \n",
    "\n",
    "and we have a limmitting value (via renewal theory) of \n",
    "\n",
    "$\\lim_{k \\to \\infty} v_{r+k} = \\lim_{k \\to \\infty} \\mathbf e_1^T \\mathbf A^k \\mathbf v = \\frac{1}{\\mu}\\big(\\pi_1 v_r + \\pi_2 v_{r-1} + ... + \\pi_r v_1\\big)$   \n",
    "\n",
    "which in the case of this problem reads: \n",
    "$\\lim_{k \\to \\infty} v_{r+k} = \\frac{1}{\\mu}\\big(\\pi_1 v_2 + \\pi_2 v_{1}\\big) = \\frac{1}{\\theta + 2 \\theta^2}\\big(1 v_1 + (1-p_1) v_2 \\big) = \\frac{1}{\\theta + 2 \\theta^2}\\big(1 v_1 + (1-\\theta)\\cdot (v_2) \\big) = \\frac{1}{\\theta + 2 \\theta^2}\\big(1 v_1 + (1-\\theta)\\cdot (\\theta^{-1}v_1)  \\big)$  \n",
    "$= \\frac{1}{\\theta + 2 \\theta^2}\\big(\\theta^{-1}v_1   \\big) =  \\frac{1}{\\theta + 2 \\theta^2}$   \n",
    "\n",
    "where we make use of the fact that $v_1 = \\theta ^{-1} v_2$, given starting / boundary conditons for the problem (1's vector pre-tilting) and appropriate exponential tilting.  We may have defined $v_1 = 1$ or $v_1 = \\theta$ it's just a matter or shifting the indexing in the exponent of the the decay rate to recover the stated solution.  \n",
    "\n",
    "\n",
    "- - - - \n",
    "\n",
    "For another worked example (note this comes up in some of the modelling on streaks problems consider: \n",
    "\n",
    "$\\mathbf B = \\left[\\begin{matrix}\\frac{3}{4} & \\frac{3}{4} & \\frac{3}{4} & \\frac{3}{4} &\\frac{3}{4}\\\\\\frac{1}{4} & 0 & 0 & 0 & 0\\\\0 & \\frac{1}{4} & 0 & 0 & 0\\\\0 & 0 & \\frac{1}{4} & 0 & 0\\\\0 & 0 & 0 & \\frac{1}{4} & 0\\end{matrix}\\right]$  \n",
    "\n",
    "This can be interpretted as the collection of the transient states in an absorbing state markov chain -- i.e. the stochastic matrix it is embedded in could be something like:  \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\mathbf B & \\mathbf 0\\\\ \n",
    " \\frac{1}{4}& 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "hence for $\\mathbf B$, the forward transition probability is $p = \\frac{1}{4}$, and we rescale, via dividing by $p$ to recover the companion matrix  \n",
    "\n",
    "since this is a column stochast markov chain, we may model transitions amongst non-absorbing states as \n",
    "\n",
    "$\\mathbf B^k \\mathbf x$   \n",
    "\n",
    "For a problem of runs, we assume we have proper cold start, and hence we start with no heads, which is the top state and hence $\\mathbf x := \\mathbf e_1$  \n",
    "\n",
    "and our interest is in summing over all of these non-absorbing states at time $k$, hence we are interested in the equation of \n",
    "\n",
    "$\\mathbf 1^T \\big(\\mathbf B^k \\mathbf e_1\\big)$  \n",
    "\n",
    "\n",
    "$\\mathbf C = \\left[\\begin{matrix}3 & 3 & 3 & 3 & 3\\\\1 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & 0\\end{matrix}\\right] = \\frac{1}{p}\\mathbf B$    \n",
    "\n",
    "\n",
    "**note** this is referred to as tilting probabilities, and while the above $x_k$ is not exact, it is an exponentially tight (and in fact assymptocially corect) estimate... re-visiting the sections in Gallagher on large deviations and tilting probabilities seems appropriate here.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now want to go from the recurrence (note this ignores nits surrounding the base case)  \n",
    "\n",
    "hence the first row of\n",
    "\n",
    "$\\mathbf {Cx}$  \n",
    "\n",
    "gives us  \n",
    "\n",
    "$x_k =  3 x_{k-1} + 3 x_{k-2} + 3 x_{k-3} + 3 x_{k-4} + 3 x_{k-5}$   \n",
    "\n",
    "$z_k := \\theta^{k}x_k$  \n",
    "\n",
    "hence \n",
    "\n",
    "$z_k = 3\\theta z_{k-1} + 3\\theta^2 x_{k-2} + 3\\theta^3 x_{k-3} + 3\\theta^4 x_{k-4} + 3\\theta^5 x_{k-5}$  \n",
    "\n",
    "\n",
    "hence we are looking for $\\theta \\in (0,1)$ where \n",
    "\n",
    "$3\\theta  + 3\\theta^2  + 3\\theta^3 + 3\\theta^4 + 3\\theta^5  = 1$   \n",
    "\n",
    "or \n",
    "\n",
    "\n",
    "$\\theta^5 + \\theta^4 + \\theta^3  + \\theta^2   + \\theta =  \\frac{1}{3} $  \n",
    "\n",
    "or   \n",
    "$g(\\theta ) = \\theta^5 + \\theta^4 + \\theta^3  + \\theta^2   + \\theta -  \\frac{1}{3} =0 $  \n",
    "\n",
    "note that we can **almost** guess the root here... \n",
    "\n",
    "note that the matrix that generated this had a forward transtion probabiltiy of $\\frac{1}{4}$.  \n",
    "\n",
    "if instead of having a finite series, it was an infinite series, it would be\n",
    "\n",
    "$\\frac{1}{3} = \\theta + \\theta^2  + \\theta^3  + \\theta^4  +\\theta^5 + \\theta^6 + ... = \\frac{\\theta}{1- \\theta} $  \n",
    "\n",
    "selecting $\\theta := \\frac{1}{4}$ gives \n",
    "\n",
    "$\\frac{1}{3} = \\frac{\\theta}{1- \\theta} = \\frac{\\frac{1}{4}}{1-\\frac{1}{4}} = \\frac{\\frac{1}{4}}{\\frac{4-1}{4}} = \\frac{\\frac{1}{4}}{\\frac{3}{4}} = \\frac{1}{3}$  \n",
    "\n",
    "(i.e. the idea is $ \\frac{w}{1-w} = \\frac{\\frac{1}{s+1}}{1-\\frac{1}{s+1}} = \\frac{\\frac{1}{s+1}}{\\frac{s+1}{s+1}-\\frac{1}{s+1}}= \\frac{\\frac{1}{s+1}}{\\frac{s}{s+1}} = \\frac{1}{s}$  so for any $s$, selecting $w = \\frac{1}{s+1}$ is quite natural)  \n",
    "\n",
    "Thus, taking advantage of positivity we see an $\\epsilon \\gt 0$ given below \n",
    "\n",
    "$ \\theta + \\theta^2  + \\theta^3  + \\theta^4  +\\theta^5 + \\epsilon = \\theta + \\theta^2  + \\theta^3  + \\theta^4  +\\theta^5 + \\big(\\theta^6 + \\theta^7 + ...\\big) = \\frac{\\theta}{1- \\theta} $  \n",
    "\n",
    "that is \n",
    "\n",
    "$\\theta + \\theta^2  + \\theta^3  + \\theta^4  +\\theta^5 + \\epsilon = \\frac{1}{3}$ \n",
    "\n",
    "$\\theta + \\theta^2  + \\theta^3  + \\theta^4  +\\theta^5  - \\frac{1}{3} = - \\epsilon$ \n",
    "\n",
    "and thus \n",
    "\n",
    "$g(\\frac{1}{4}) = -\\epsilon \\lt 0$  \n",
    "\n",
    "and we want to select some small $\\delta \\gt 0$ such that \n",
    "\n",
    "$g(\\frac{1}{4} + \\delta) \\gt 0$  \n",
    "\n",
    "which places the root   \n",
    "$\\lambda \\in \\big(\\frac{1}{4}, \\frac{1}{4} + \\delta\\big)$  \n",
    "\n",
    "\n",
    "now we know \n",
    "\n",
    "$\\epsilon = \\big(\\theta^6 + \\theta^7 + ...\\big) = \\frac{\\theta^6}{1-\\theta}$  \n",
    "\n",
    "where as always we are interested in $\\theta \\in (0,1)$ \n",
    "\n",
    "lazily taking advantage of positivity, and supposing $\\theta \\in \\big(0, \\frac{1}{2}\\big)$ to isolate the residual term:  \n",
    "\n",
    "$  \\frac{\\theta^6}{1-\\theta} \\leq \\delta \\lt  (\\delta) + (\\delta)^2 + ... + (\\delta)^5 \\lt \\big((\\theta +\\delta) + (\\theta +\\delta)^2 + ... + (\\theta +\\delta)^5\\big) - \\big((\\theta ) + (\\theta )^2 + ... + (\\theta )^5\\big) = g(\\frac{1}{3} + \\delta) - g(\\frac{1}{3})  $ \n",
    "\n",
    "that is, we can easily select \n",
    "\n",
    "$\\frac{\\theta^6}{1-\\theta} \\leq \\delta$  \n",
    "or \n",
    "\n",
    "$\\theta^6 \\leq \\delta (1 - \\theta)$  \n",
    "\n",
    "which, if $0 \\leq \\theta \\leq \\frac{1}{2}$, then we have \n",
    "\n",
    "$\\theta^6 \\leq \\delta \\theta \\leq \\delta (1 - \\theta)$  \n",
    "\n",
    "so selecting any $\\theta^5 \\leq \\delta  $ suffices \n",
    "\n",
    "in this case, we may in particular choose \n",
    "\n",
    "$\\delta:= \\big(\\frac{1}{2}\\big)^5 = \\frac{1}{32} = 0.03125 $   \n",
    "\n",
    "\n",
    "- - - - \n",
    "note that if we were to allow $0 \\lt \\theta \\leq \\frac{4}{5}$ then we know   \n",
    "\n",
    "$\\frac{5}{4}\\theta = \\frac{1}{4}\\theta + \\theta \\leq 1$ \n",
    "\n",
    "hence $\\frac{1}{4}\\theta \\leq 1 - \\theta$ which gives us \n",
    "\n",
    "$\\theta^6 \\leq \\delta \\frac{1}{4}\\theta \\leq \\delta (1 - \\theta)$  \n",
    "\n",
    "and hence selecting \n",
    "\n",
    "$4 \\theta^5 \\leq \\delta $ \n",
    "\n",
    "suffices.  The underlying mechanics here is general, however we want must **also** ensure that $\\theta + \\delta \\lt 1$ and in general that $\\delta$ is satisfactorily small.  So for the $\\frac{4}{5}$ case we can do a quick check and see\n",
    "\n",
    "$4 \\big(\\frac{4}{5}\\big)^5 = 1.31072$ \n",
    "\n",
    "**This is a failure**. Now if the exponent was much larger, of course we would have more flexibility. But this tells us the above method may not be appropriate for $\\theta $ near 1. (If we look at the underlying streaks graph we can see this corresponds to 'leaving' an absorbing state quite quickly and hence it is not of such a concern.)  \n",
    "- - - - \n",
    "\n",
    "Note we could allow $0 \\lt \\theta \\leq \\frac{3}{5}$  \n",
    "\n",
    "this would give us \n",
    "\n",
    "$\\frac{5}{3}\\theta = \\frac{2}{3}\\theta + \\theta \\leq 1 $  \n",
    "\n",
    "so \n",
    "\n",
    "$\\frac{2}{3}\\theta \\leq 1- \\theta $  \n",
    "\n",
    "hence we could select \n",
    "\n",
    "\n",
    "$\\frac{3}{2} \\theta ^5 \\leq \\delta$  \n",
    "\n",
    "(Here we could choose $\\delta$ in the neighborhood of $0.117$ would work if $\\theta = \\frac{3}{5}$ though this seems to put a rather larger gap around our $\\theta$ value... the results get much tighter for higher exponent values, of course).  \n",
    "\n",
    "\n",
    "\n",
    "- - - - \n",
    "note for example with $\\theta = \\frac{3}{5}$, we have $\\frac{3}{5} \\leq \\lambda \\leq \\frac{3}{5} + 0.117$\n",
    "\n",
    "\n",
    "As we'd expect, the result gets much better for larger $k$.  E.g. for $k=10$, we have analytical bounds of $0.6 \\leq \\lambda \\leq 0.60152$.  \n",
    "- - - - \n",
    "\n",
    "back to the matter at hand: \n",
    "\n",
    "$g(\\theta ) = \\theta^5 + \\theta^4 + \\theta^3  + \\theta^2   + \\theta -  \\frac{1}{3} $  \n",
    "\n",
    "we know that the result is very close to $\\frac{1}{4}$, in fact an easy bound tells us $\\frac{1}{4} \\lt \\lambda \\lt \\frac{1}{4} + \\frac{1}{64}$, or \n",
    "\n",
    "\n",
    " $\\frac{16}{64} \\lt \\lambda \\lt \\frac{17}{64}$  \n",
    "\n",
    "(numerically we can see that the actual root is $\\lambda \\approx 0.25018$)  \n",
    "\n",
    "setting $\\theta := \\lambda $ \n",
    "\n",
    "(i.e. the positive root of the polynomial between 0 and 1 *is* the value we want for $\\theta$.)  \n",
    "\n",
    "So to recap, we now have \n",
    "\n",
    "$\\mathbf R = \\left[\\begin{matrix}3\\theta & 3\\theta^2 & 3\\theta^3 & 3\\theta^4 & 3\\theta^5\\\\1 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & 0\\end{matrix}\\right]$\n",
    "\n",
    "which is a bonafide renewal matrix (and we've confirmed that all entries are real non-negative and row 1 sums to one)\n",
    "\n",
    "From here we compute \n",
    "\n",
    "$\\mu = 1(3\\theta) + 2(3\\theta^2) + 3 (3\\theta^3) + 4(3\\theta^4) + 5(3\\theta^5) + 6 (3\\theta^6) \\approx \\frac{4}{3}$   \n",
    "\n",
    "and as before \n",
    "\n",
    "$x_k \\sim \\frac{\\theta^{-k}}{\\mu}\\alpha$  \n",
    "\n",
    "now recalling that \n",
    "\n",
    "$\\big(p\\big)^k \\mathbf C^k =  \\mathbf B^k$  \n",
    "\n",
    "we can recover the limitting tendancy for multiplication by $\\mathbf C$, which we have already solved for in terms of $\\mathbf B$, by multiplying by $p^k$  \n",
    "\n",
    "$x_k^{\\text{when multiplied by C}} \\sim p^k \\frac{\\theta^{-k}}{\\mu}\\alpha$  \n",
    "\n",
    "**There are a few issues here** \n",
    "\n",
    "1.) It turns out that $\\alpha$ ends up being rather difficult / tedious to calculate because we are actually looking for \n",
    "\n",
    "$\\mathbf 1^T \\big(\\mathbf B_{\\text{tilted}}^k \\mathbf e_1\\big)$  *not*   \n",
    "$\\mathbf e_1^T \\big(\\mathbf B_{\\text{tilted}}^k \\mathbf 1\\big)$  (the original / sample problem setup) \n",
    "\n",
    "and having the ones vector on the left hand side ends up requiring adding values with different degrees of exponential tilting \n",
    "\n",
    "2.) We have to juggle the original matrix -> companion system -> titled sytem, and then backward making extra adjustments as we go and take care that the rescaling holds correctly.  \n",
    "\n",
    "3.) This problem setup + tilting works in the case of a run of $HHH$ as in the above, but if we looked at $HTH$ and $p \\neq 0.5$ then it would not work (i.e. we wouldn't be able to homogenize the chain into a companion system).\n",
    "\n",
    "**There must be a better way**  \n",
    "\n",
    "**enter (4.12) in chapter 11, page 278 of Feller**  \n",
    "This problem is in the Generating Functions Chapter, however some additional insights may be had under the lense of renewal theory and Markov chains -- that's what this writeup seeks to do.    \n",
    "\n",
    "First we consider, as Feller does, the problem **(a)** of not having a run of $HHH$ after $n$ tosses of a coin.  Then we consider an extension: **(b)** the problem of not having a run of $HTH$ after $n$ tosses of a coin.  \n",
    "\n",
    "*key idea:*  \n",
    "This problem **(a)** is a defective renewal process.  Define a renewal as any time a tails comes up, given no run of $HHH$ has occurred.  Let $p\\in(0,1)$ be the probability of heads and $q = 1-p$ be the probability of tails.  \n",
    "\n",
    "We can verify this process is defective by checking that with probability $p^3$ a run of $HHH$ occurs on the first 3 tosses -- recall: for a renewal process to be non-defective, it must renew with probability one.  Equivalently this problem may be modelled/ interpretted as a (finite)  discrete state markov chain that is transient (or more technically, a single absorbing state and a class of $r$  transient states).\n",
    "\n",
    "Rather than jumping directly into markov chains, let us start with a probabilistic / renewal argument. \n",
    "\n",
    "Let $w_i: \\text{probability of not having a run HHH on ith trial}$.  Since this is a defective renewal process we know that $\\sum_i w_i = c \\lt \\infty$ \n",
    "\n",
    "but in general $c \\neq 1$, so this is not a probability distribution.  \n",
    "\n",
    "Remark: It is immediate that $w_0 = w_1 = w_2 = 1$  \n",
    "\n",
    "Now we consider all of the mutually exclusive ways to get a renewal. \n",
    "\n",
    "$\\text{Starting Sequence: } T  \\text{:  with probability : } q$  \n",
    "$\\text{Starting Sequence: } HT \\text{:  with probability : } pq $  \n",
    "$\\text{Starting Sequence: } HHT\\text{:  with probability : } p^2 q $    \n",
    "\n",
    "\n",
    "and that is it  \n",
    "\n",
    "\n",
    "The big ideas as always: \n",
    "\n",
    "1.)  Work through different ways to get a renewal  \n",
    "2.)  Select these ways such that they are mutually exclusive (pay attention to overlaps!)  \n",
    "3.) Via a forward look (first step analysis) or backward look (last step analysis), we can add the above probabilities since the underlying events are mutually exclusive -- figure out the appropriate linear combination of them would be, and what the other side of the equation is.  \n",
    "\n",
    "In this case we may reason that the above events comprise at most 3 tosses, and we already know $w_0, w_1, w_2$.  \n",
    "\n",
    "Now we have \n",
    "\n",
    "$w_3 = q \\cdot w_2 + pq \\cdot w_1 + p^2 q \\cdot w_0$  \n",
    "\n",
    "another key idea common in renewal (and convolution related) equations: \n",
    "notice: the homogeneity on the RHS with respect to indices \n",
    "\n",
    "the terms are  \n",
    "$q^1 w_2$ with total 'term value' $1 + 2 = 3$, and  \n",
    "$pq w_1$ with total 'term value' $1 + 1 +1 = 3$, and  \n",
    "$p^2q w_0$ with total 'term value' $2 + 1 +0 = 3$  \n",
    "\n",
    "the result up until here is fairly mechanical.  We may now recognize a self-generalizing component.  I.e. consider the recurrence:  \n",
    "\n",
    "$w_i = q \\cdot w_{i-1} + pq \\cdot w_{i-2} + p^2 q \\cdot w_{i-3}$   \n",
    "\n",
    "we may verify that this holds via probabilisitic argument.  The right hand side has some probability that occurs when a $T$ occurs, which triggers a renewal, and we chain on the probability of $i-1$ turns not having a run of $HHH$, union: $HT$, which triggers a renewal, and we chain on the probability of $i-2$ turns not having a run of $HHH$, and finally $HHT$ occurs and we chain on the probability of $i-3$ turns not having a run of $HHH$.  Inspection confirms that each of these events are mutually exclusive (note: no overlaps), and that these are the only ways we can have had no run for $i$ tosses.  Hence we now have a useful recurrence.  And since it is a straightforward recurrence, we may model it with a (modified) companion system, which, after tilting, becomes a Renewal Matrix for Residual Life.  Thus our system may be described as \n",
    "\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "w_{k+2}\\\\ \n",
    "w_{k+1}\\\\ \n",
    "w_k\n",
    "\\end{bmatrix}=\\mathbf {C}^k \\mathbf w =  \\begin{bmatrix}\n",
    "p & pq & p^2 q\\\\ \n",
    "1 & 0 & 0\\\\ \n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}^k\\begin{bmatrix}\n",
    "w_2\\\\ \n",
    "w_1\\\\ \n",
    "w_0\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "Note: as is common with these renewal arguments, all values in the recurrence are real non-negative, so we may now make use of exponential titling, and we can re-write this as   \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "z_{k+2}\\\\ \n",
    "z_{k+1}\\\\ \n",
    "z_k\n",
    "\\end{bmatrix}=  \\begin{bmatrix}\n",
    "p\\cdot\\theta & p(1-p)\\cdot\\theta^2 & p^2(1-p)\\cdot\\theta^3\\\\ \n",
    "1 & 0 & 0\\\\ \n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}^k\\begin{bmatrix}\n",
    "w_2\\\\ \n",
    "\\theta^{-1}w_1\\\\ \n",
    "\\theta^{-2} w_0\n",
    "\\end{bmatrix} =  \\begin{bmatrix}\n",
    "p\\cdot\\theta & p(1-p)\\cdot\\theta^2 & p^2(1-p)\\cdot\\theta^3\\\\ \n",
    "1 & 0 & 0\\\\ \n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}^k\\begin{bmatrix}\n",
    "1\\\\ \n",
    "\\theta^{-1}\\\\ \n",
    "\\theta^{-2}\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "to solve for theta, we look for a probability distribution, where \n",
    "\n",
    "$p\\cdot\\theta + p(1-p)\\cdot\\theta^2 + p^2(1-p)\\cdot\\theta^3 = 1$ \n",
    "\n",
    "or equivalently, find the positive root $\\theta$ such that\n",
    "\n",
    "$p\\cdot\\theta + p(1-p)\\cdot\\theta^2 + p^2(1-p)\\cdot\\theta^3 - 1 = 0$  \n",
    "\n",
    "As a cubic this is in fact solvable exactly (though it is a mess-- there is one real root and two imaginery ones).  This gives us:  \n",
    "\n",
    "$\\mu = 1 \\big(p\\cdot\\theta\\big ) + 2\\big(p(1-p)\\cdot\\theta^2\\big) + 3\\big(p^2(1-p)\\cdot\\theta^3\\big)$   \n",
    "\n",
    "which tells us that \n",
    "\n",
    "$\\lim_{k\\to \\infty} z_{k+2} = \\frac{1}{\\mu}\\big(\\pi_1 \\cdot 1 + \\pi_2 \\cdot \\theta^{-1} + \\pi_3 \\cdot \\theta^{-2}  \\big) = \\frac{1}{\\mu}\\big( 1 + (1- (p\\cdot\\theta)) \\cdot \\theta^{-1} + (1-(p\\cdot\\theta) -  (p(1-p)\\cdot\\theta^2 ))\\cdot \\theta^{-2}  \\big)$   \n",
    "\n",
    "in general, in particular for larger problems: first fix $p$, numerically solve for $\\theta$ and then get limmitting value of $z$.  \n",
    "\n",
    "*residual item: confirming that this matches, after form adjustments, in case of* $p=\\frac{1}{2}$  \n",
    "\n",
    "**(b)**  the problem of not having a run of $HTH$ after $n$ tosses of a coin  \n",
    "\n",
    "The argument is not quite as clean this way, but in short we have another defective renewal process.  $w_0 = w_1 = w_2 = 1$, and for $i\\geq 3$ a renewal is said to occur when no run of $HTH$ has happened, *and* a \"breaking sequence\" has just occurrred. That is, supposing a run has not already happened, if a 'breaking sequence' has just occurred, we probabilistically start over (renew).  \n",
    "\n",
    "In this case we may notice that the run ends on heads, hence $T$ is the key item for our \"breaking sequence\".  So this gives us \n",
    "\n",
    "$\\text{Starting Sequence: } T  \\text{:  with probability : } q$  \n",
    "$\\text{Starting Sequence: } HTT\\text{:  with probability : } p q^2 $    \n",
    "\n",
    "notice that we want mutually exclusive events, so we cannot have another sequence start with $T$.  Furthermore, we cannot have $HH$ as a breaking sequence because $TH$ could follow and 'end the game' (i.e. an actual renewal would only have the game end afterward if $HTH$ occurred, but in this case $HH$ immediately after could end the game), and we cannot have $HT$ for a sequence of $2$ becasuse if a single $H$ followed, the game would end.  Notice that while the reasoning is a bit different, we are once again quite interested in how 'overlaps' play in here.  And notice, once again we have some kind of dependencies between the simple events (individual coin tosses), however the renewal structure allows us to isolate them -- at any given point in time $i \\geq 3$ we only concern ourselves with coin tosses $\\{i, i-1, i-2\\}$ -- as these are the only current ones that relate to the run of $HTH$.  Overall the technique is wildly different from, say, Stein-Chen but it is part of a general set of techniques where we try to isolate / contain dependencies between simple events (or relatively straight forward random variables), and once isolated we can abstract.  \n",
    "\n",
    "\n",
    "We now take this, as before and get \n",
    "\n",
    "$w_i = q \\cdot w_{i-1} + 0 \\cdot w_{i-2} + p q^2 \\cdot w_{i-3}$  \n",
    "\n",
    "which has the matrix form of \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "w_{k+2}\\\\ \n",
    "w_{k+1}\\\\ \n",
    "w_k\n",
    "\\end{bmatrix}=\\mathbf {C}^k \\mathbf w =  \\begin{bmatrix}\n",
    "p & 0 & p q^2\\\\ \n",
    "1 & 0 & 0\\\\ \n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}^k\\begin{bmatrix}\n",
    "w_2\\\\ \n",
    "w_1\\\\ \n",
    "w_0\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "and tilted form of \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "z_{k+2}\\\\ \n",
    "z_{k+1}\\\\ \n",
    "z_k\n",
    "\\end{bmatrix}=  \\begin{bmatrix}\n",
    "p\\cdot\\theta & 0 & p(1-p)^2\\cdot\\theta^3\\\\ \n",
    "1 & 0 & 0\\\\ \n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}^k\\begin{bmatrix}\n",
    "1\\\\ \n",
    "\\theta^{-1}\\\\ \n",
    "\\theta^{-2}\n",
    "\\end{bmatrix} $  \n",
    "\n",
    "and as before, we technically can solve the cubic, but for larger problems, we'd select $p$, find positive $\\theta$ such that \n",
    "\n",
    "$p\\cdot\\theta + + p(1-p)^2\\cdot\\theta^3 - 1 =0$  \n",
    "\n",
    "*remark*:  \n",
    "while all of these results are the same up to a (relatively) straightforward re-scaling, it occurs to your author that it may be easiest to interpret in terms of \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "z_{k+2}\\\\ \n",
    "z_{k+1}\\\\ \n",
    "z_k\n",
    "\\end{bmatrix}=  \\begin{bmatrix}\n",
    "p\\cdot\\theta & 0 & p(1-p)^2\\cdot\\theta^3\\\\ \n",
    "1 & 0 & 0\\\\ \n",
    "0 & 1 & 0\n",
    "\\end{bmatrix}^k\\begin{bmatrix}\n",
    "\\theta^{2}\\\\ \n",
    "\\theta^{1}\\\\ \n",
    "1\n",
    "\\end{bmatrix} $  \n",
    "\n",
    "hence, for say $k=1$ we have $x_3 = \\theta^{-3} z_3$ and in general $x_k = \\theta^{-k} z_k$  \n",
    "\n",
    "- - - - \n",
    "*remark* \n",
    "from \"Feller_chp13_notes.ipynb\" as well as worked through elsewhere, we know that if this was a recurrent chain, where the process renewed each time $HTH$ occurred, it would fall under the theory of runs and have an expected time until renewal of \n",
    "\n",
    "$\\bar{X} = \\frac{1}{p} + \\frac{1}{p(1-p^2)} = \\frac{p(1-p^2) + p}{p^2(1-p^2)}$  \n",
    "\n",
    "or equivalently \n",
    "\n",
    "$\\frac{1}{\\bar{X}}= \\frac{p^2(1-p^2)}{p(1-p^2) + p}= \\frac{p(1-p^2)}{(1-p^2) + 1}$   \n",
    "\n",
    "which the Perron Frobenius analysis at the very end of this notebook tells us:  \n",
    "\n",
    "$\\theta = \\lambda_1 \\leq 1 - \\frac{1}{\\bar{X}} = 1 - \\frac{p(1-p^2)}{(1-p^2) + 1}=  \\frac{(1-p^2) + 1 - p(1-p^2)}{(1-p^2) + 1} \\lt \\exp\\Big(- \\frac{p(1-p^2)}{(1-p^2) + 1}\\Big)$   \n",
    "\n",
    "- - - - - \n",
    "then, as before,  notice \n",
    "\n",
    "$\\mu = 1 \\big(p\\cdot\\theta\\big ) + 2\\big(p(1-p)\\cdot\\theta^2\\big) + 3\\big(p^2(1-p)\\cdot\\theta^3\\big)$  \n",
    "\n",
    "and finally observe \n",
    "\n",
    "$\\lim_{k\\to \\infty} z_{k+2} = \\frac{1}{\\mu}\\big(\\pi_1 \\cdot 1 + \\pi_2 \\cdot \\theta^{-1} + \\pi_3 \\cdot \\theta^{-2}  \\big) = \\frac{1}{\\mu}\\big( 1 + p(1-p)^2\\cdot\\theta^3 \\cdot \\theta^{-1} + p(1-p)^2\\cdot\\theta^3\\cdot \\theta^{-2}  \\big)$   \n",
    "\n",
    "$\\lim_{k\\to \\infty} z_{k+2} =  \\frac{1}{\\mu}\\big( 1 + p(1-p)^2\\cdot ( \\theta + \\theta^2)  \\big)$  \n",
    "\n",
    "and $w_i = \\theta^{i-2}z_i$  \n",
    "\n",
    "- - - - \n",
    "**remark:**   \n",
    "\n",
    "Feller on page 278 only considers *(a)* but there is a useful general technique underlying it that this section sought to belabor.  It is worth considering the above technique, and it's similarities with that in 'another look at the theory of runs' in Feller_chp13_notes.ipynb.   \n",
    "\n",
    "As a point of fact for *(a)*, we have a plain vanilla linear recurrence.  Feller goes on to derive the exact ordinary generating function associated with that (with $p=\\frac{1}{2}$), and then point out that there is a dominant root which gives an assymptotic relation and a very good approximation over the short-run.  As your author prefers matrices to generating functions, this post did something similar, where after the recurrence is derived, it is shown in matrix form.  Note that we can in fact solve the linear recurrence exactly (or for larger recurrences: 'exactly' up to numeric agorithm estimates of the roots).  However, these expressions can look a bit unwieldly.  What was done instead in this posting was to make use of exponential tilting, to get the exact same assymptotic relation and high quality short-run approximation.  This exponential tilting approach has the added benef of tieing in with the renewal matrix associated with recurrent events and residual wating times, from Feller p. 381.  Overall this is a very nice linkage of many different concepts.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**simplifications** \n",
    "\n",
    "while not always the case, a *very* a common structural feature in the renewal chain matrix for a defective renewal process is \n",
    "\n",
    "$z_k = \n",
    "\\mathbf e_1^T \\mathbf R^k \\mathbf z = \\mathbf e_1^T\\left[\\begin{matrix}\\alpha_1 \\theta & \\alpha_2 \\theta^2 & \\alpha_3\\theta^3 & \\alpha_4\\theta^4 & \\alpha_5\\theta^5\\\\1 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & 0\\end{matrix}\\right]^k\\begin{bmatrix}\n",
    "\\theta^4\\\\ \n",
    "\\theta^3\\\\ \n",
    "\\theta^2\\\\ \n",
    "\\theta^1\\\\ \n",
    "1\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "though frequently the structure is even nicer and we have something homogenized like \n",
    "\n",
    "$z_k = \n",
    "\\mathbf e_1^T \\mathbf R^k \\mathbf z = \\mathbf e_1^T\\left[\\begin{matrix}\\alpha\\theta & \\alpha \\theta^2 & \\alpha\\theta^3 & \\alpha\\theta^4 & \\alpha\\theta^5\\\\1 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & 0\\end{matrix}\\right]^k\\begin{bmatrix}\n",
    "\\theta^4\\\\ \n",
    "\\theta^3\\\\ \n",
    "\\theta^2\\\\ \n",
    "\\theta^1\\\\ \n",
    "1\n",
    "\\end{bmatrix}$  \n",
    "- - - - \n",
    "*remark*:  \n",
    "\n",
    "There are basically 2, perhaps 3, ways to interpret this kind of exponential tilting. \n",
    "\n",
    "The probabilistic one is that it is tilting a super/sub renewal function that has a proper probability distribution.  In fact, this approach rapidly generalizes the the countably infinitie case, and we can think of our $\\theta$ as belonging to the OGF for this 'distribution.'  For the non-negative continuous case, we'd naturally want to use $E\\big[e^{tX}\\big]$ with the choice of $t$ that makes this equal to one.  Of course for the positive integers case setting $e^t = \\theta$ recover the OGF approach / approach to what we've done here.  This is the setup mentioned in brief in Karlin & Taylor volume 1.  \n",
    "\n",
    "For the finite dimensional case, we can interpret this in terms of $A$ being similar to the transpose of the Companion matrix. Referencing e.g. \"Vandermonde_Matrices_Permutations_and_Discrete_Fourier_Transform.ipynb\" in the Linear Algebra folder, we can see that $\\mathbf C^T$ has right eigenvectors that are the moment curve -- i.e. slices of the Vandermonde matrix for a given root.  Now the similarity transform being effected here for $\\mathbf A$ is with the reflection matrix $\\mathbf J$ which merely 'flips' the ordering in the eigenvectors. So when we see  \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\theta^4\\\\ \n",
    "\\theta^3\\\\ \n",
    "\\theta^2\\\\ \n",
    "\\theta^1\\\\ \n",
    "1\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\lambda_1^{-4}\\\\ \n",
    "\\lambda_1^{-3}\\\\ \n",
    "\\lambda_1^{-2}\\\\ \n",
    "\\lambda_1^{-1}\\\\ \n",
    "1\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "where $\\lambda_1 \\gt 0$ is the dominant eigenvalue for the matrix $\\mathbf A$.  In our examples, $\\mathbf A$ is substochastic so $\\lambda \\lt 1$ and this eigenvector has an increasing sequence which is exactly the 'flipped' nature of what we'd expect from the transpose of the Companion matrix.  Indeed we can in general see that  \n",
    "\n",
    "$p_1\\theta + p_2\\theta^2 + p_3\\theta^3 + p_4\\theta^4 + p_5\\theta^5 +... + p_n \\theta^n = 1$  \n",
    "\n",
    "is equivalent to \n",
    "\n",
    "$0 = 1 - p_1\\theta - p_2\\theta^2 - p_3\\theta^3 - p_4\\theta^4 - p_5\\theta^5 -... - p_n \\theta^n$    \n",
    "\n",
    "and rescaling each side by $ \\theta^{-n}$ and making the substitution $\\lambda_1 = \\theta^{-1}$, we have the familiar \n",
    "\n",
    "$0 = \\lambda_1 a^n - p_1\\lambda_1^{n-1} - p_2\\lambda_1^{n-2} - p_3\\lambda_1^{n-3} - p_4\\lambda_1^{n-4} - p_5\\lambda_1^{n-5} -... - p_n \\lambda_1^0$    \n",
    "\n",
    "\n",
    "which is to say, of course that $\\lambda_1$ is a root to the characteristic polynomial -- and recall that the transpose of the Companion Matrix has the negative of each coefficient (except monic / highest) term just sitting there waiting to be read off, so with a little bit of work we can see that that the coefficients of the characteristic polynomial are precisely $-p_1$ for the the second highest coefficient, which is of course given by the negative of the trace of $\\mathbf A$ and so on.  This approach does not extend easily to infinite dimensional cases.  Further while it gives insights into the tilted result -- i.e. the right eigenvector -- in general it isn't easy to figure out the associated dominant left eigenvector would look like  (/what the first row of $\\mathbf A$).  Conveniently in the exponentially tilted case, we can use the renewal theorem to figure this out.    \n",
    "\n",
    "- - - - \n",
    "\n",
    "\n",
    "in general, for $n$ x $n$ matrices (or $n$ element streaks problems) we find that for even moderately large $n$, so long as none of the probabilities are 'too extreme', then while it must be the case that \n",
    "\n",
    "$1 \\lt \\theta $ (since the chain is transient afterall),\n",
    "\n",
    "we tend to find $\\theta \\approx 1$ -- i.e. the amount of geometric tilting is quite small.  \n",
    "\n",
    "while the renewal chain insights allow us to get very close estimates that are assymptotically exact, there are a couple of downsides.  \n",
    "\n",
    "1.) despite having a closed form for the renewal steady state in the renewal matrix, it is somewhat tedious to do the calculations, and we get a rather cluttered result.  \n",
    "\n",
    "2.) While these results *are* asymptotically correct they do not give bounds on the error in the approximation being made.   \n",
    "\n",
    "- - - -  \n",
    "**an even better approach:** Take advantage of row stochasticiity of tilted matrix.  \n",
    "\n",
    "We can observe that a properly tilted $\\mathbf R$ is a row stochastic matrix, hence $\\mathbf R^k \\mathbf 1 = \\mathbf 1$ and all of its components are real non-negative.  \n",
    "\n",
    "Taking advantage of non-negativity and the fact that we know $1 \\lt \\theta $, we may observe that for any $\\mathbf v$ with strictly real non-negative entries and for $i \\in \\{1, 2, ...,n\\}$   \n",
    "$1 \\lt \\theta \\leq \\theta^i \\leq \\theta^n$  \n",
    "\n",
    "hence   \n",
    "$v_{n-i + 1} \\leq v_{n-i + 1} \\theta^i \\leq v_{n-i + 1}\\theta^n$ \n",
    "\n",
    "and summing over the bound gives:  \n",
    "\n",
    "$\\mathbf v^T \\mathbf 1 = \\sum_{i=1}^n v_i \\cdot 1  = \\sum_{i=1}^n v_{n-i + 1} \\leq \\sum_{i=1}^n v_{n-i + 1}\\theta^i \\leq \\sum_{i=1}^n v_{n-i + 1} \\theta^n$  \n",
    "\n",
    "for any choice of natural number $k$ we can assign\n",
    "\n",
    "$\\mathbf v^T := \\mathbf e_1^T \\mathbf R^k $    \n",
    "\n",
    "giving us  \n",
    "\n",
    "$1 = \\mathbf e_1^T \\mathbf 1 = \\mathbf e_1^T \\mathbf R^k \\mathbf 1 \\leq z_k  = \\mathbf e_1^T \\mathbf R^k \\mathbf z \\leq \\mathbf e_1^T \\mathbf R^k (\\theta^n) \\mathbf 1 = \\theta^n \\cdot \\mathbf e_1^T \\mathbf 1 = \\theta^n$    \n",
    "\n",
    "and since $x_k = \\theta^{-k}z_k$, we can rescale the above inequality by the (positive number) $\\theta^{-k}$ to see \n",
    "\n",
    "$\\theta^{-k} \\leq \\theta^{-k} z_k = x_k \\leq  \\theta^{n-k} $   \n",
    "\n",
    "or more simply  \n",
    "\n",
    "$\\theta^{-k} \\leq x_k  = Pr\\{\\text{first 'arrival' takes more than k iterations }\\} \\leq  \\theta^{n-k} $   \n",
    "\n",
    "for avoidance of doubt: since in general we only have *very* close numeric estimates of $\\theta$, the actual relation we work with is:  \n",
    "\n",
    "$\\theta_{\\text{upper bound}}^{-k} \\lt \\theta^{-k} \\leq x_k  = Pr\\{\\text{first 'arrival' takes more than k iterations }\\} \\leq  \\theta^{n-k} \\lt \\theta_{\\text{lower bound}}^{n-k}$   \n",
    "- - - -\n",
    "and for further avoidance of doubt, for any natural number $r$  \n",
    "$1 \\lt \\theta_{\\text{lower bound}}^r \\lt \\theta^r \\longrightarrow_{\\text{inverting  }} \\theta^{-r} \\lt \\theta_{\\text{lower bound}}^{-r} \\lt  1$    \n",
    "and  \n",
    "$1 \\lt \\theta^r \\lt \\theta_{\\text{upper bound}}^{r}  \\longrightarrow_{\\text{inverting  }} 1 \\lt   \\theta_{\\text{upper bound}}^{-r} \\lt \\theta^{-r} $  \n",
    "- - -- \n",
    "so we have the relation  \n",
    "$\\theta_{\\text{upper bound}}^{-k} \\lt \\theta^{-k} \\leq x_k  = Pr\\{\\text{first 'arrival' takes more than k iterations }\\} \\leq  \\theta^{n-k} \\lt \\theta_{\\text{lower bound}}^{n-k}$   \n",
    "\n",
    "\n",
    "which is a complementary CDF of the distribution of time until first arrival. Note that the complementary CDF of the geometric distribution (indexing at one) is $(1-p)^k$ which is strongly suggestive that these two distributions may be linked.  **follow-up:** explicitly compare this with the stein-chen method on page 65, 66 of Ross and Pekoz, that addresses probability of a run, as well as the Stein Method for Geometric distribution on pages 67, 68 of the same book.  (These are extremely good comparables, though note indexing issues -- e.g. the geometric seriess approach is number of trials up until first run begins, not ends.)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.683840560623607e-09\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import comb\n",
    "\n",
    "million = 1000000\n",
    "theta_test = 1.0000004755\n",
    "running_sum = -1\n",
    "for i in range(1, 20+1):\n",
    "    running_sum += (theta_test/2)**i\n",
    "print(running_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.75,  1.  ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1/2, 1/4],[1,0]])\n",
    "x = np.ones(2)\n",
    "A@x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000004755"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20 streak, million coin filps\n",
    "1.000000479  # theta upper bound \n",
    "\n",
    "1.0000004755 # theta lower bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6194025556836156"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.000000479**(-million)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.621580173100842"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0000004755**(20-million)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.682479612503 we want the complement of course\n",
      "0.693404516439 is a tight upper bound\n"
     ]
    }
   ],
   "source": [
    "k = 7\n",
    "n = 100\n",
    "F = np.zeros((k,k))\n",
    "p = 0.5\n",
    "for i in range(1, k):\n",
    "    F[i, i - 1] = 1\n",
    "for i in range(k):\n",
    "    F[0,i] = (1-p)*(p)**(i)\n",
    "\n",
    "u = np.ones(k)\n",
    "for _ in range(n-k+1):\n",
    "    b = F @ u\n",
    "    u = b\n",
    "print(u[0], \"we want the complement of course\") \n",
    "\n",
    "mu = (1 - p**k)/((1-p)*p**k)\n",
    "print(np.exp((k-n)/mu), \"is a tight upper bound\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0004946133014671342"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_sum = p**k # the immediate case \n",
    "for i in range(n - k - 1):\n",
    "    running_sum += (1-p)*p**k\n",
    "running_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "247.29143197863448"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-p)*p**k*comb(n-2*k,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.306595483561 <-- analytic lower bound\n",
      "Bonferroni bound is below \n",
      "\n",
      "0.37109375 for m = 1\n",
      "0.312698364258 for m = 2\n",
      "0.317779123783 for m = 3\n",
      "0.317511817906 for m = 4\n",
      "0.317520561759 for m = 5\n",
      "0.317520385404 for m = 6\n",
      "0.31752038751 for m = 7\n",
      "0.317520387497 for m = 8\n",
      "0.317520387497 for m = 9\n",
      "0.317520387497 for m = 10\n",
      "0.317520387497 for m = 11\n",
      "0.317520387497 for m = 12\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import comb\n",
    "# in Python 3.x \n",
    "\n",
    "k = 7 # run of tails \n",
    "n = 100 # total tosses \n",
    "p = 1/2 # probability of tails \n",
    "\n",
    "upper_threshold = 50\n",
    "# some positive integer\n",
    "# some caution is needed when playing around with markedly different scenarios as this approach\n",
    "# has some significant numeric stability issues\n",
    "X_bar = (1 - p**k)/((1-p)*p**k)\n",
    "print(1 - np.exp(-(n-k)/X_bar), \"<-- analytic lower bound\")  \n",
    "\n",
    "print(\"Bonferroni bound is below \\n\")\n",
    "\n",
    "running_sum = 0\n",
    "for m in range(1, upper_threshold + 1):\n",
    "    if m - 1 >= n - m * k:\n",
    "        break\n",
    "    running_sum += (-1)**(m+1)*((1-p)**(m)*p**(m*k)*comb(n-m*k,m) + p**(m*k +m-1)*comb(n-m*k,m-1))\n",
    "    # if we ignore A_0 or bound its impact via other means, we can comment out the above line \n",
    "    # and uncomment the below line, which is simpler and just has the first of the two terms \n",
    "#     running_sum+= (-1)**(m+1)*(1-p)**(m)*p**(m*k)*comb(n-m*k,m)\n",
    "    \n",
    "    print(running_sum, \"for m =\", m)\n",
    "    # this shows the alternating and tightening bound on probability estimates \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1/X_bar)**((n-k)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.476837158203125"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**(-21)*1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999980655895403"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the azuma hoeffding approach would be\n",
    "n = 1000000\n",
    "prob = p**k\n",
    "expected_allowing_overlaps = (n-k+1) * prob\n",
    "a = expected_allowing_overlaps - 1\n",
    "np.exp(-2*(a**2/(n-k+1)))\n",
    "# note that this is a very weak bound... this issue is that  are squaring the prob then multiplying by 2... \n",
    "# but this radically shrinks the amount inside the exponential function \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a direct comparion on page 66 of Ross and Pekoz, (with slightly different notation)\n",
    "\n",
    "consider  \n",
    "\n",
    "** cleanup: consider that I may have swapped tails and heads... i.e. p vs (1-p) may be backward here**    \n",
    "\n",
    "$n_{\\text{heads per run}} = 1024$  \n",
    "$k_{\\text{iterations}} = 10$  \n",
    "$p = \\frac{1}{2}$   \n",
    "\n",
    "$ 0.6 = 1-0.4 \\leq  Pr\\{\\text{no run of 10 heads after 1024 tosses}\\} \\leq 1-0.388 = 0.612$  \n",
    "\n",
    "**MORE CLEANUP: the Ross Pekoz setup jumps around between n+k tosses, and n tosses, which means I may be off by 10 for total number of tosses... of course I know I am using n and k in some sense in the 'opposite' manner as Ross and Pekoz**  \n",
    "\n",
    "using our approach above we see the titled renewal matrix is \n",
    "\n",
    "$\\left[\\begin{matrix}  \n",
    "p \\theta & p(1-p) \\theta^2 & p(1-p)^2\\theta^3 & \\cdots & p(1-p)^9\\theta^{10}\n",
    "\\\\1 & 0 & 0 & \\cdots & 0 \n",
    "\\\\0 & 1 & 0 & \\cdots & 0 \n",
    "\\\\\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \n",
    "\\\\0 & 0 & 0 & 1 & 0  \n",
    "\\end{matrix}\\right]$  \n",
    "\n",
    "so we need to solve $\\theta$ such that\n",
    "\n",
    "$p \\theta + p(1-p) \\theta^2 + p(1-p)^2\\theta^3 + \\cdots + p(1-p)^9\\theta^{10} = 1$    \n",
    "\n",
    "where as always, we have $p \\in(0,1)$ or equivalently, we can homogenize this to be    \n",
    "\n",
    "$(1-p)\\theta + \\big((1-p) \\theta\\big)^2 + \\big((1-p)\\theta\\big)^3 + \\cdots +\\big((1-p)\\theta\\big)^{10} = \\frac{(1-p)}{p}$    \n",
    "\n",
    "assigning  \n",
    "$w := (1-p)\\theta$  \n",
    "we have \n",
    "\n",
    "$\\frac{w(1-w^{11})}{1-w}= w + w^2 + ... + w^{10} = \\frac{(1-p)}{p} \\lt   w + w^2 + ... + w^{10} + w^{11}+...=\\frac{w}{1-w}$   \n",
    "\n",
    "which suggests $w \\in \\big((1-p), (1-p)(1+ \\delta)\\big)$  \n",
    "\n",
    "which, with $p=\\frac{1}{2}$ we can numerically solve for \n",
    "\n",
    "$1.000485 \\lt \\theta \\approx 1.00049 \\lt 1.000495$  \n",
    "\n",
    "which gives us \n",
    "\n",
    "- - - -  \n",
    "$1.000495^{-k}\\lt \\theta^{-k} \\leq x_k  = Pr\\{\\text{first 'arrival' takes more than k iterations }\\} \\leq  \\theta^{n-k} \\lt 1.000485^{n-k}$  \n",
    "\n",
    "or \n",
    "\n",
    "$0.602 \\approx \\big(\\frac{1}{1.000495}\\big)^{1024}\\lt Pr\\{\\text{first 'arrival' takes more than k iterations }\\} \\lt \\big(\\frac{1}{1.000485}\\big)^{1014} \\approx 0.612$  \n",
    "\n",
    "*comment:*  \n",
    "it is worth pointing out, using the fact that the $\\bar{X} = \\frac{1-p^k}{(1-p)p^k}$ as expected time until a run of $k$ heads, and the overly detailed linkage to Perron Frobenius theory below, we may see that \n",
    "\n",
    "$\\lambda = \\theta^{-1}$  and in our case \n",
    "\n",
    "$\\bar{X} = 2046$ \n",
    "\n",
    "hence we have \n",
    "\n",
    "$Pr\\{\\text{first 'arrival' takes more than k iterations }\\} \\lt \\approx 0.6092$  \n",
    "\n",
    "*this is actually a tightening of the upper bound estimate because we know* \n",
    "\n",
    "$\\lambda^{k-n} = \\theta^{-(k-n)} \\leq \\exp\\big(-(k-n)\\frac{1}{\\bar{X}}\\big)$  \n",
    "\n",
    "- - - - \n",
    "**remark:** \n",
    "this upper bound on probability of renewa;l in terms of the exponential function applied to the negative of the renewal rate (inverse of expected time until renewal) should remind us of the probability of zero arrivals in that favorite idealized renewal process the Poisson.  \n",
    "\n",
    "This has an explicit and direct comparison in terms of our process being so called \"New Better than Used in Expectations\" (NBUE) meaning that the expected time until absorbtion / renewal is always at least as long for a brand new process than one that has been running for some amount.  (Your author has referred to this as a kind of structured monotonocity in the underlying graph, earlier in this or the Chp 13 writeup... however it seems that this is part of a more general area in stochastics.)  The linkage of the Poisson approximation holds as an upper bound for probability of zero arrivals, irrespective even if we are modelling a much messier process that doesn't have a graphical representation and isn't amenable to matrix modelling.  See e.g. 8.6 \"Applications of Variability Orderings\" on page 273 of Ross's *Stochastic Processes* first edition (which is likely section 9.6 in the second edition of the book).  \n",
    "\n",
    "- - - -  \n",
    "\n",
    "**remark:** in essence this *is* this Stein Chen estimate, though we went through a somewhat more tedious route with the renewal chain and exponential tilting.  Given what else has been written on this page, we may fairly easily adjust our approach to consider other kinds of streaks.  (Stein Chen can be adjusted as well though it requires some additional work for the boundg of the error.)  The gating item with this approach -- simpler as it deals with a polynomial, but fundamentally not unlike finding tilted random variables via Legendre transforms of MGFs-- is we need a root here, which in general requires a specific selection of $p$ and then some care to numerically find (though there are extremely good numeric root finding algorithms). \n",
    "\n",
    "There of course is an exact formula, using incluson-exclusion (example 1.10 on Ross Pekoz) but the formula is messy and unpleasant -- and more importantly it is hard to look at that formula and see what should be immediate -- that there is exponential decay in the probability of not having a run, over time.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6024475998613804\n",
      "0.6116036851798372\n"
     ]
    }
   ],
   "source": [
    "n = 10  # recall that changing n requires recalulating our estimate for theta \n",
    "k = 1024\n",
    "assert(k >= n)\n",
    "# k is number of iterations, which clashes with Ross unfortunately\n",
    "\n",
    "print((1.000495)**(-k))\n",
    "print((1.000485)**(-(k-n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6054765416907315\n",
      "0.6085738811597805\n"
     ]
    }
   ],
   "source": [
    "# depending on how comfortable we are with the numeric accuracy of theta, \n",
    "# and of floating point precision in the resulting exponentiation, we can tighten the bound a bit  \n",
    "\n",
    "print((1.0004901)**(-k))\n",
    "print((1.0004899)**(-(k-n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision = 3, linewidth=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "k = 1024\n",
    "A = np.zeros((n,n))\n",
    "\n",
    "p = 1/2\n",
    "\n",
    "for j in range(n):\n",
    "    A[0,j] = p*(1-p)**j\n",
    "\n",
    "for i in range(1,n):\n",
    "    A[i,i-1]=1\n",
    "    \n",
    "# print(A)\n",
    "x = np.ones(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.605  0.605  0.605  0.606  0.606  0.606  0.606  0.607  0.607  0.607] \n",
      "\n",
      "0.609204559843\n"
     ]
    }
   ],
   "source": [
    "mu = (1 - 0.5**10)/(0.5*0.5**10)\n",
    "print(np.linalg.matrix_power(A, k)@x,\"\\n\")\n",
    "print(np.exp(-(k-n)*1/mu)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is a re-hash of my original challenge problem at the carnival... which is quite nice \n",
    "\n",
    "import sympy as sp\n",
    "\n",
    "x = sp.Symbol('x')\n",
    "k = sp.Symbol('k')\n",
    "\n",
    "A = sp.Matrix([[2/5,0,2/5],[1,0,0],[0,1,0]])\n",
    "v = sp.Matrix([x, x + 1, x+ 2])\n",
    "e_1 = sp.Matrix([1,0,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matrix([\n",
       "[3.36939008],\n",
       "[ 3.7527552],\n",
       "[  4.185088]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A*A*A*A*A*A*A*A*v.subs(x,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matrix([\n",
       "[3.36939008],\n",
       "[ 3.7527552],\n",
       "[  4.185088]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A**8*v.subs(x,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89706444743783009"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_inv = np.roots([-1, 2/5, 0, 2/5])[0]\n",
    "theta_inv = abs(theta_inv)\n",
    "theta_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matrix([\n",
       "[ 0.42643664638525, 0.254191793911501, 0.233534132273736],\n",
       "[0.583835330684341, 0.192902514111514, 0.254191793911501],\n",
       "[0.635479484778753,  0.32964353677284, 0.192902514111514]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A/theta_inv)**8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matrix([[3.36939008]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = sp.Matrix([1,0,0])\n",
    "\n",
    "result = sp.transpose(y) * A**8\n",
    "result*sp.Matrix([8,9,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4211625\n",
      "0.33693\n"
     ]
    }
   ],
   "source": [
    "p_upper_estimate = 3.3693/8\n",
    "p_lower_estimate = 3.3693/10\n",
    "\n",
    "print(p_upper_estimate)\n",
    "print(p_lower_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.871902175526074"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5*(1-p_upper_estimate)/p_upper_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.839877719407593"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5*(1-p_lower_estimate)/p_lower_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.08317440000000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expecter = 0\n",
    "B = sp.eye(3)\n",
    "\n",
    "stop_at = 8\n",
    "for i in range(1, stop_at+ 1):    \n",
    "    expecter += B[0,0] \n",
    "    B *= A\n",
    "expecter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6546635761596429"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_inv**9/(1-theta_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PurePoly(1.0*_lambda**3 - 0.4*_lambda**2 - 0.4, _lambda, domain='RR')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.charpoly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2/15 + (-1/2 - sqrt(3)*I/2)*(sqrt(2073)/225 + 683/3375)**(1/3) + 4/(225*(-1/2 - sqrt(3)*I/2)*(sqrt(2073)/225 + 683/3375)**(1/3)): 1,\n",
       " 2/15 + 4/(225*(-1/2 + sqrt(3)*I/2)*(sqrt(2073)/225 + 683/3375)**(1/3)) + (-1/2 + sqrt(3)*I/2)*(sqrt(2073)/225 + 683/3375)**(1/3): 1,\n",
       " 4/(225*(sqrt(2073)/225 + 683/3375)**(1/3)) + 2/15 + (sqrt(2073)/225 + 683/3375)**(1/3): 1}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.eigenvals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x*(0.804724622856939*0.89706444743783**k*(1.24266111859457 + 1.11474710970452*(-0.400587110512434 + 0.382829019410516*I)/(0.110820231223639 - 0.963204988591056*I) - (-1.24266111859457 + 1.11474710970452*(1.40058711051243 - 0.382829019410516*I)/(0.110820231223639 - 0.963204988591056*I))*(-0.400587110512434 - 0.382829019410516*I - (-0.400587110512434 + 0.382829019410516*I)*(0.110820231223639 + 0.963204988591056*I)/(0.110820231223639 - 0.963204988591056*I))/(2.67763768857153 - 0.308071938263819*I)) + (-0.322362311428469 + 0.308071938263819*I)*(-0.248532223718915 - 0.619782685830395*I)**k*(-1.11474710970452/(0.110820231223639 - 0.963204988591056*I) - (-1.24266111859457 + 1.11474710970452*(1.40058711051243 - 0.382829019410516*I)/(0.110820231223639 - 0.963204988591056*I))*(0.110820231223639 + 0.963204988591056*I)/((0.110820231223639 - 0.963204988591056*I)*(2.67763768857153 - 0.308071938263819*I))) + (-1.24266111859457 + 1.11474710970452*(1.40058711051243 - 0.382829019410516*I)/(0.110820231223639 - 0.963204988591056*I))*(-0.322362311428469 - 0.308071938263819*I)*(-0.248532223718915 + 0.619782685830395*I)**k/(2.67763768857153 - 0.308071938263819*I)) + (x + 1)*(0.804724622856939*0.89706444743783**k*((1.40058711051243 - 0.382829019410516*I)*(-0.400587110512434 - 0.382829019410516*I - (-0.400587110512434 + 0.382829019410516*I)*(0.110820231223639 + 0.963204988591056*I)/(0.110820231223639 - 0.963204988591056*I))/((0.110820231223639 - 0.963204988591056*I)*(2.67763768857153 - 0.308071938263819*I)) - (-0.400587110512434 + 0.382829019410516*I)/(0.110820231223639 - 0.963204988591056*I)) + (-0.322362311428469 + 0.308071938263819*I)*(-0.248532223718915 - 0.619782685830395*I)**k*((0.110820231223639 + 0.963204988591056*I)*(1.40058711051243 - 0.382829019410516*I)/((0.110820231223639 - 0.963204988591056*I)**2*(2.67763768857153 - 0.308071938263819*I)) + 1/(0.110820231223639 - 0.963204988591056*I)) - (-0.322362311428469 - 0.308071938263819*I)*(-0.248532223718915 + 0.619782685830395*I)**k*(1.40058711051243 - 0.382829019410516*I)/((0.110820231223639 - 0.963204988591056*I)*(2.67763768857153 - 0.308071938263819*I))) + (x + 2)*(-0.804724622856939*0.89706444743783**k*(-0.400587110512434 - 0.382829019410516*I - (-0.400587110512434 + 0.382829019410516*I)*(0.110820231223639 + 0.963204988591056*I)/(0.110820231223639 - 0.963204988591056*I))/(2.67763768857153 - 0.308071938263819*I) - (-0.322362311428469 + 0.308071938263819*I)*(-0.248532223718915 - 0.619782685830395*I)**k*(0.110820231223639 + 0.963204988591056*I)/((0.110820231223639 - 0.963204988591056*I)*(2.67763768857153 - 0.308071938263819*I)) + (-0.322362311428469 - 0.308071938263819*I)*(-0.248532223718915 + 0.619782685830395*I)**k/(2.67763768857153 - 0.308071938263819*I))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sp.transpose(e_1)*A**k *v)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.032723987340647"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1/theta_inv)**1 * 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.3548966584535207"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_inv**8* 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Follow - up: explicitly compare this with the geometric approximation methods in Ross & Pekoz on pages 67-68... There would seem to be some nice ideas lurking underneath  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7761963196364963"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_sum = 0 \n",
    "raw_value = (3*(1*0.25018 + 2*0.25018**2 + 3*0.25018**3 + 4*0.25018**4 + 5*0.25018**5 + 6*0.25018**6))\n",
    "\n",
    "for k in range(5):\n",
    "    running_sum += raw_value*(0.25018)**k\n",
    "running_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.75,  0.75,  0.75,  0.75,  0.75],\n",
       "       [ 0.25,  0.  ,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.25,  0.  ,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.25,  0.  ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.25,  0.  ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision = 3, linewidth=180)\n",
    "\n",
    "TILTED = False\n",
    "\n",
    "B = np.zeros((5,5))\n",
    "\n",
    "if TILTED:\n",
    "    for j in range(B.shape[0]):\n",
    "        B[0,j] = 3* 0.25018**(j+1)\n",
    "else:\n",
    "    B[0] += 3/4\n",
    "    \n",
    "for i in range(1,B.shape[0]):\n",
    "    if TILTED:\n",
    "        B[i,i-1] = 1\n",
    "    else:\n",
    "        B[i,i-1] = 1/4\n",
    "    \n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.699,  0.175,  0.044,  0.011,  0.003])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_1 = np.zeros(B.shape[0])\n",
    "e_1[0] = 1\n",
    "\n",
    "value = np.linalg.matrix_power(B,100) @ e_1\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.999  0.238  0.238  0.228  0.228]\n"
     ]
    }
   ],
   "source": [
    "thelam = np.linalg.eig(B)[0]\n",
    "thelam = abs(thelam)\n",
    "print(thelam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.230368277254\n"
     ]
    }
   ],
   "source": [
    "x = np.zeros(B.shape[0])\n",
    "x[0] = 1\n",
    "\n",
    "z = 2000\n",
    "for _ in range(z):\n",
    "    c = B @ x\n",
    "    x = c\n",
    "#     print(np.sum(x) / np.sum(c))\n",
    "print(np.sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.230592420905\n"
     ]
    }
   ],
   "source": [
    "if TILTED:\n",
    "    print(((1/4)/0.25018)**(z-5))\n",
    "else:\n",
    "    print(thelam[0]**(z-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21482358588759556"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thelam[0]**(z-5) * sum(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 open items:\n",
    "1.) explicitly connect the above with exponential tilting in the case of using OGFs -- can I do this? Basically mix some of the above ideas with what is said by Gallagher -- however, trying to toggle between OGFs and MGFs -- sticking with integer valued -- perhaps finitely so, would seem to be perhaps helpful and make the linkage -- not sure on this point. \n",
    "\n",
    "2.) Explicitly compare some of these results and bounds vs what is said below using expected absorbtion times plus perron frobenius to bound the dominant eigenvalue controlling the decay rate \n",
    "\n",
    "3.) working through the balance equations portion at other states in the markov chain -- should be relatively straight forward -- would be good to have on hand since these residual life markov chains seem to come up quite a bit \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** On decay rates for time until absorbtion in finite state markov chains**   \n",
    "\n",
    "a large portion of this motivated by \n",
    "\n",
    "\"Approximating the distributions of runs and patterns\" and open access paper by Johnson and Fu, and in particular page 9.  \n",
    "\n",
    "\n",
    "using notation mostly similar to that of \"markov_chains_absorbing_state_recut.ipynb\", where the final state, $m$ is the one we have particular interest in.  \n",
    "\n",
    "That is, consider the below row stochastic matrix given by:  \n",
    "\n",
    "\n",
    "$\\mathbf B = \\begin{bmatrix}\n",
    "\\mathbf A^T & *  \\\\ \n",
    "* & *\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "where $\\mathbf A \\in \\mathbb R^{\\text{m -1  x m - 1 }}$  \n",
    "\n",
    "so we know $\\mathbf B \\mathbf 1 = \\mathbf 1$ \n",
    "\n",
    "To avoid triviality, assume this is irreducible -- i.e. there is one communicating class in here.  \n",
    "\n",
    "In line with problem 5.45, page 285 of Gallagher's *Stochastic Processes*, we may be interested in finding expected time of getting to state $m$ from, say state $1$, and modify the graph to have all zeros in row zero, except a 1 (i.e. determinsitic path) from state $m \\to 1$.  Or if this is a streaks/runs problem or many other graphs that have a \"feed forward\" nature to them, it may in fact naturally be the case that state 1 is the 'beginning' and state $m$ is the end, and  after every renewal at $m$ it automatically progresses back to the 'begginning'.  (Of course in renewal theory terms, renewals to $m$ begin and start there, but starting at 0 and ending at $m$ is *almost* the same thing -- literally all sample paths are lessened in length by 1 which is an easy correction to deal with as needed.)  The above structure isn't needed per se -- the only essential is that there is one communicating class, but having a certain kind of monotonicity allows us to make additional claims.  \n",
    "\n",
    "Now consider what happens when we make state $m$ an absorbing state: \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\mathbf A^T & *  \\\\ \n",
    "\\mathbf 0^T & 1\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "Suppose that all transient states still communicate with each other. (This is not strictly necessary but it simplifies what follows in terms of positive matrices, in stead of mere non-negative ones, and lines up with the suggestion that it is particularly if state $m$ only communicated with state $1$ before becoming an absorbing state.)  \n",
    "\n",
    "the above matrix is still row stochastic.  However, the chain in $\\mathbf A^T$ is a transient chain.  Either by direct examination of the probabilities, or the fact that it is transient, we may confirm $\\mathbf A^T \\mathbf 1 \\neq \\mathbf 1$ i.e. that the dominant eigenvalue for $\\mathbf A$, given by $\\lambda_1$ is $\\lt 1$ (as the graph is transient -- alternatively at most $m$ iterations and application of Gerschgorin discs prove this claim as well.)  In general $\\mathbf 1$ is no longer an eigenvector of $\\mathbf A$.  Perron theory tells us that there is a single dominant eigenvalue $\\lambda_1$ with entirely positive entries in the associated left and right eigenvectors\n",
    "\n",
    "so we have \n",
    "\n",
    "$\\mathbf A^T \\mathbf v_1 = \\lambda_1 \\mathbf v_1$   \n",
    "\n",
    "we know that the geometric and algebraic multipilicities are 1 for this eigenpair.  This is as far as your author typically takes it.  However, there is much more information to be had here.  \n",
    "\n",
    "When we use the Collatz-Wielandt max-min formulation, we see that \n",
    "\n",
    "\n",
    "$\\lambda_1 = \\text{max}_{\\mathbf v \\in D} f(\\mathbf v)$  \n",
    "\n",
    "where we have  \n",
    "\n",
    "$f(\\mathbf v) = \\text{min k} \\frac{\\mathbf e_k^T \\big(\\mathbf A^T \\mathbf v\\big)}{\\mathbf e_k^T \\mathbf v}$  \n",
    "\n",
    "where $D = \\{\\mathbf v \\big \\vert\\mathbf v .\\geq \\mathbf 0, \\text{ and } \\mathbf v \\neq 0\\}  $  \n",
    "\n",
    "and for avoidance of doubt $v .\\geq \\mathbf 0$ is a component wise comparison for each entry in $\\mathbf v$ being real non-negative, and $\\mathbf e_k$ is the $k$th standard basis vector.    \n",
    "\n",
    "$v .\\geq \\mathbf 0 $\n",
    "\n",
    "The above is the *max-min* formulation.  The below is the *min-max* characterization\n",
    "\n",
    "$\\lambda_1 = \\text{min}_{\\mathbf v \\in P} g(\\mathbf v)$  \n",
    "\n",
    "$g(\\mathbf v) = \\text{max j   } \\frac{\\mathbf e_j^T \\big(\\mathbf A^T \\mathbf v\\big)}{\\mathbf e_j^T \\mathbf v}$  \n",
    "\n",
    "where $P = \\{\\mathbf v \\big \\vert \\mathbf v .\\geq \\mathbf 0, \\text{ and } \\mathbf v \\neq 0\\}  $  (and $\\mathbf e_j^T \\mathbf v \\gt 0$)   \n",
    "\n",
    "(reference pages 667 - 669 of Meyer's *Matrix Analysis* for more information, though the earlier mentioned article references Karlin and Taylor volume 1, Corollary 2.2 on page 551.  )  \n",
    "\n",
    "Thus we have \n",
    "\n",
    "$f(\\mathbf v) = \\lambda_1 = g(\\mathbf v)$  \n",
    "\n",
    "if we are choosing optimally.  However the outer optimization for $f$ requires choosing the maximizing $\\mathbf v$, so if we select some other $\\mathbf v^*$ (and keep the coordinate wise inner optimization) then $f(\\mathbf v^*) \\leq \\lambda_1$ and the 'flipped' relationship holds for $f$.  In particular, supposing we set $\\mathbf v^* := \\mathbf 1$, we have  \n",
    "\n",
    "$0 \\lt f(\\mathbf 1) \\leq \\lambda_1 \\leq g(\\mathbf 1)$ \n",
    "\n",
    "note that the denominator for $f$ and $g$ are $\\mathbf e_k^T \\mathbf 1 = 1$ and $\\mathbf e_j^T \\mathbf 1 = 1$ in each case-- i.e. a scalar value of one, hence the denominator may be ignored due to the homogeneity of the ones vector.  \n",
    "\n",
    "(note that we *know* $\\mathbf A^T \\mathbf 1 \\gt 0$ as each row has positive entries on it, and we have assumed that the nodes still communicate -- otherwise it technically could be $0 \\leq f(\\mathbf 1\\big)$  )  \n",
    "\n",
    "\n",
    "re-running the above argument with where the $r$th exponent is considered\n",
    "\n",
    "$g_r(\\mathbf v) := \\text{max j   } \\Big\\{ \\frac{\\mathbf e_j^T \\big(\\mathbf A^T\\big)^r \\mathbf v}{\\mathbf e_j^T \\mathbf v}\\Big\\}$   \n",
    "and  \n",
    "$f_r(\\mathbf v) := \\text{min k} \\Big\\{  \\frac{\\mathbf e_k^T \\big(\\mathbf A^T\\big)^r \\mathbf v}{\\mathbf e_k^T \\mathbf v}\\Big\\}$  \n",
    "\n",
    "we see \n",
    "\n",
    "$0 \\lt f_r(\\mathbf 1) \\leq \\lambda_1^r \\leq g_r(\\mathbf 1)$  \n",
    "\n",
    "for all natural numbers $r = \\{1, 2,...\\}$, and for notational convenience we take the convention that they all have value of 1 for $r =0$ case \n",
    "\n",
    "supposing for the moment that $g_r(\\mathbf 1)^r \\lt $  for large enough $r$ (this is true... why?)  \n",
    "\n",
    "then we may sum over the bound to get \n",
    "\n",
    "$0 \\lt \\sum_{r =0}^{\\infty} f_r(\\mathbf 1) \\leq \\sum_{r =0}^{\\infty} \\lambda_1^r = \\frac{1}{1 -\\lambda_1} \\leq \\sum_{r =0}^{\\infty} g_r(\\mathbf 1)$  \n",
    "\n",
    "And we note that \n",
    "\n",
    "$\\sum_{r =0}^{\\infty} f_r(\\mathbf 1) = \\sum_{r =0}^{\\infty} \\text{min row  } \\mathbf A^r \\mathbf 1$  \n",
    "\n",
    "$\\sum_{r =0}^{\\infty} g_r(\\mathbf 1) = \\sum_{r =0}^{\\infty} \\text{max row  } \\mathbf A^r \\mathbf 1$ \n",
    "\n",
    "This approach does not quite give us what we want, however, because it 'grabs' the min and max at each iteration.  So instead consider \n",
    "\n",
    "$\\mathbf N := \\mathbf I + \\mathbf A^T + \\big(\\mathbf A^T\\big)^2 + \\big(\\mathbf A^T\\big)^3 +...  = \\mathbf I + \\sum_{r=1}^\\infty \\big(\\mathbf A^T\\big)^r $    \n",
    "\n",
    "(which notationally differs / overloads from the linked streaks paper, though this is nice in that it lines up exactly with standard fundamental matrix notation and that used in 'markov_chains_absorbing_state_recut.ipynb')  \n",
    "\n",
    "we end up with \n",
    "\n",
    "$\\mathbf N = \\big(\\mathbf I - \\mathbf A^T\\big)^{-1}$  \n",
    "\n",
    "now, re-running our argument we can say \n",
    "\n",
    "$\\mu_{k \\text{  shortest }} = f\\Big(\\mathbf {N1}\\Big) =  f\\Big(\\mathbf I + \\sum_{r=1}^\\infty \\big(\\mathbf A^T\\big)^r\\Big) \\leq \\lambda_{max}\\big(\\mathbf N\\big) =   \\frac{1}{1 -\\lambda_1}  \\leq   g\\Big(\\mathbf I + \\sum_{r=1}^\\infty \\big(\\mathbf A^T\\big)^r\\Big) = g\\Big(\\mathbf {N1}\\Big) = \\mu_{j \\text{  longest }}$ \n",
    "\n",
    "where \n",
    "\n",
    "$\\mu_{k \\text{  shortest }}$ denotes the shortest of all expected times from a node $k$ $\\lt m$, and going $k \\to m$   \n",
    "$\\mu_{j \\text{  longest }} $ denotest the longest of all expected times from a node $j$ $\\lt m$, and going $j \\to m$  \n",
    "\n",
    "we can easily see that $\\frac{1}{1 -\\lambda_1}$ is an eigenvalue of $\\mathbf N$.  It remains to verify that this is in fact the dominant eigenvalue.  This can be done by direct calculation and application of triangle inequality.  However, a more slick approach is to note that $\\mathbf N$ is the sum of real non-negative matrices (and because of underlying communicating states) we know that $\\mathbf N$ has entirely positive entries.  So its dominant eigenvalue is positive, and the associated left and right eigenvectors have strictly positive components -- and all other eigenvectors are orthogonal to these (left vs right and right vs left) -- i.e. no other eigenvectors can have strictly positive components. But we know that $\\big(\\mathbf I - \\mathbf A^T\\big)^{-1}$ has the same eigenvectors as $\\mathbf A^T$, and hence $\\mathbf v_1$ is the (right) eigenvector associated with the dominant eigenvalue, and thus $\\frac{1}{1-\\lambda_1}, \\mathbf v_1$ is the dominant (right) eigenpair.  \n",
    "\n",
    "- - - -\n",
    "This last fact is easily confirmable via a Jordan Form argument, or by observing \n",
    "\n",
    "$\\mathbf A^T \\mathbf v_1 = \\lambda_1 \\mathbf v_1 \\to \\big(\\mathbf I - \\mathbf A^T\\big) \\mathbf v_1 = (1-\\lambda_1) \\mathbf v_1 \\to \\big(\\mathbf I - \\mathbf A^T\\big)^{-1} \\mathbf v_1 = \\frac{1}{(1-\\lambda_1)} \\mathbf v_1$  \n",
    "\n",
    "because \n",
    "\n",
    "$\\mathbf v_1 = \\mathbf {Iv}_1 = \\big(\\mathbf I - \\mathbf A^T\\big)^{-1}\\big(\\mathbf I - \\mathbf A^T\\big)\\mathbf v_1 = \\big(\\mathbf I - \\mathbf A^T\\big)^{-1}\\Big(\\big(\\mathbf I - \\mathbf A^T\\big)\\mathbf v\\Big) = (1-\\lambda_1) \\big(\\mathbf I - \\mathbf A^T\\big)^{-1}\\mathbf v_1$ \n",
    "\n",
    "hence multiplying each side by $\\frac{1}{(1-\\lambda_1)}$ gives \n",
    "\n",
    "$\\frac{1}{(1-\\lambda_1)} \\mathbf v_1 =  \\big(\\mathbf I - \\mathbf A^T\\big)^{-1}\\mathbf v_1$ \n",
    "\n",
    "- - - -\n",
    "hence we have \n",
    "\n",
    "$\\mu_{k \\text{  shortest }} \\leq  \\frac{1}{1 -\\lambda_1}  \\leq   \\mu_{j \\text{  longest }}$ \n",
    "\n",
    "and taking advantage of positivity we may say \n",
    "\n",
    "$\\frac{1}{\\mu_{k \\text{  shortest }}} \\geq  1 -\\lambda_1  \\geq   \\frac{1}{\\mu_{j \\text{  longest }}}$ \n",
    "\n",
    "$-\\frac{1}{\\mu_{k \\text{  shortest }}} \\leq - 1 +\\lambda_1  \\leq   -\\frac{1}{\\mu_{j \\text{  longest }}}$ \n",
    "\n",
    "$1 -\\frac{1}{\\mu_{k \\text{  shortest }}} \\leq  \\lambda_1  \\leq   1-\\frac{1}{\\mu_{j \\text{  longest }}} $   \n",
    "\n",
    "\n",
    "**special cases of interest -- structured graphs with monotonocity**  \n",
    "\n",
    "for many graphs of interest,  including streaks/runs problems, there may be a node that has a clear longest expected time until absorbtion -- because, for instance, it is the 'starting node'. And it may be that via techniques from renewal theory, martingales, generating functions or otherwise, that we can easily calculate $\\mu_{j \\text{  longest }}$.  Hence we may choose to 'zoom in' on \n",
    "\n",
    "$ \\frac{1}{1 -\\lambda_1}  \\leq \\mu_{j \\text{  longest }}$   \n",
    "\n",
    "taking advantage of positivity (and note we know $\\mu_{j \\text{  longest }}\\lt \\infty$ for finite state markov chains with communicating nodes) we can see this as   \n",
    "\n",
    "$  \\frac{1}{\\mu_{j \\text{  longest }}}\\leq 1 - \\lambda_1 $   \n",
    "\n",
    "$ -1 + \\frac{1}{\\mu_{j \\text{  longest }}}\\leq  - \\lambda_1 $   \n",
    "\n",
    "$e^{- \\frac{1}{\\mu_{j \\text{  longest }}}}\\gt 1 - \\frac{1}{\\mu_{j \\text{  longest }}}\\geq   \\lambda_1 $   \n",
    "\n",
    "where the fact that $1+ x \\lt e^x$ for all $x \\neq 0$ (by strict convexity of exponential map) was used, with  $x:=- \\frac{1}{\\mu_{j \\text{  longest }}}$  \n",
    "\n",
    "e.g. for a small streaks problem, we would have \n",
    "\n",
    "$\\mathbf B = (1-p)\\left[\\begin{matrix}1 & 1 & 1 & 1 & 1 & 1 & \\frac{1}{1-p}\\\\\\frac{p}{1-p} & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & \\frac{p}{1-p} & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & \\frac{p}{1-p} & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & \\frac{p}{1-p} & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & \\frac{p}{1-p} & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & \\frac{p}{1-p} & 0\\end{matrix}\\right]^T$  \n",
    "\n",
    "\n",
    "we can follow the graph to see that state $m$ feeds solely into state 1.  We can also see by monotonicity that the expected time from \n",
    "\n",
    "$1 \\to m$ is larger than from $i \\to m$ for $2 \\leq i \\leq m-1$  \n",
    "\n",
    "and because $m \\to 1$ with weighting 1, we can see \n",
    "\n",
    "$1 + \\mu_{j \\text{  longest }} = E\\big[\\text{time from 0 }  \\to \\text{ m} \\big] + 1 =  E\\big[\\text{time from m }  \\to \\text{ m} \\big] = \\bar{X} $ \n",
    "\n",
    "or equivalently \n",
    "\n",
    "$\\mu_{j \\text{  longest }} =\\bar{X} - 1 $  \n",
    "\n",
    "so we have a renewal rate of $\\frac{1}{\\bar{X}}$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extension:  Semi-Markov Processes (work in progress)  \n",
    "\n",
    "this section primarily comes from Ross's Stochastic Processes.  \n",
    "A semi markov process is a markov process where at each state the time to transition out of a given state is itself a random variable with expected time until exit given by $\\mu_i$ for state $i$.  We assume each $\\mu_i\\lt \\infty$ and the underlying markov chain aka the 'jump chain' is irreducible, positive recurrent (and in general aperiodic to streamline arguments).  Beyond this, unless otherwise noted, the chains are assumed to be regular (to avoid bizarre behavior with infinite transitions in a finite amount of time).  \n",
    "\n",
    "A key idea is that while the jump chain has $\\pi_i = \\frac{1}{\\bar{X_i}}$, this in effect 'samples' the chain based on states (in effect positing uniformity of time spent in a state), however if we sample the chain based on time spent in a given state, then the steady steady /lr average time spent in a given state tends to $P_i = \\pi_i \\mu_i$. (Note: to focus on recurrent setups, we require that $\\sum_i \\pi_i \\mu_i \\lt \\infty$).   The discrepancy between the two in some sense is familiar in the sense of the inspection paradox.  \n",
    "\n",
    "The book tackles this directly in terms of steady state probabilities.  This writeup will consider the time averaged value from the viewpoint of a Renewal Rewards Process, and also from the viewpoint of Bayesian Inference.  \n",
    "\n",
    "\n",
    "** Renewal Rewards Process**  \n",
    "consider a renewal reward process where we start in state $i$, and upon 'exiting' (note self transition count as an exit and re-entry here) the state and being in the state for time $\\tau$ a reward of $\\tau$ is given *if* that state was state $i$, and the reward is $0$ otherwise.  The renewal cycle is defined by least $n$ transitions occurring, with the renewal cycle terminating immediately at the first revisit to state $i$ once at least $n$ transitions have occurred.  \n",
    "\n",
    "*outline:*  \n",
    "The use of having at least $n$ transitions is useful in teasing out the underlying math.  However in a fundamental sense, the Renewal Rewards theorem tells us that \n",
    "\n",
    "$P_i = \\lim_{t \\to \\infty} \\frac{E[r(t)]}{t} = \\frac{E[R_1^{(i)}]}{E[X_1^{(i)}]}$  \n",
    "we will ultimately show, in effect that  \n",
    "$-2\\epsilon^* \\lt E\\big[R_1^{(i)}\\big] - \\pi_i \\mu_i^* \\lt 2\\epsilon$  \n",
    "\n",
    "for any $\\epsilon^* \\gt 0$  \n",
    "\n",
    "i.e. \n",
    "$E\\big[R_1^{(i)}\\big] = \\pi_i \\mu_i $  \n",
    "\n",
    "and hence  \n",
    "$P_i = \\frac{\\pi_i \\mu_i}{\\sum_k \\pi_k \\mu_k} $   \n",
    "but a virtually identical argument tells us that \n",
    "$E[X_1^{(i)}] = \\sum_k \\pi_k \\mu_k$  \n",
    "because the expected time of the cycle is given by another rewards process that is *upon 'exiting' (note self transition count as an exit and re-entry here) the state and being in the state for time $\\tau$ a reward of $\\tau$ is given no matter what state was visited.  \n",
    "\n",
    "*main argument*  \n",
    "\n",
    "since the underlying jump chain has is ergodic, we know that for any $\\epsilon \\gt 0$ \n",
    "\n",
    "$\\big \\vert \\pi_j \\text{ - Probability of being in state j at transition n}\\big \\vert \\lt \\epsilon$ \n",
    "for all $n \\geq N$  (i.e. by selecting large enough $n$)  \n",
    "\n",
    "\n",
    "suppose we select $n = k\\cdot N$ where $k$ is some natural number.  \n",
    "\n",
    "then we have, where $X^{(i)}_j$ is the random variable for time spent in state $i$ on the $jth$ transition, and $\\mathbb I_{A^j_i}$  is the indicactor random variable for the jth state visited being state $i$, total expectation gives us the following bound:    \n",
    "\n",
    "$0 + \\Big(\\sum_{j=N}^{kN} (\\pi_i -\\epsilon) \\cdot E\\big[X^{(i)}_j\\big \\vert I_{A^j_i} = 1\\big]\\Big) = \\Big(\\sum_{j=1}^{N-1} 0 \\cdot E\\big[X^{(i)}_j\\big \\vert I_{A^j_i} = 1\\big]\\Big) + \\Big(\\sum_{j=N}^{kN} (\\pi_i -\\epsilon) \\cdot E\\big[X^{(i)}_j\\big \\vert I_{A^j_i} = 1\\big]\\Big)$  \n",
    "$\\leq E\\big[R_1^{(i)}\\big] = \\sum_{i=1}^{kn} E\\Big[E\\big[X^{(i)}_j\\big \\vert \\mathbb I_{A^j_i} = 1\\big]\\Big] = \\sum_{i=1}^{kn} P\\big(A^j_i\\big)\\cdot E\\big[X^{(i)}_j\\big \\vert \\mathbb I_{A^j_i} = 1\\big] $  \n",
    "$\\leq \\Big(\\sum_{j=1}^{N-1} 1 \\cdot E\\big[X^{(i)}_j\\big \\vert \\mathbb I_{A^j_i} = 1\\big]\\Big) + \\Big(\\sum_{j=N}^{kN} (\\pi_i +\\epsilon) \\cdot E\\big[X^{(i)}_j\\big \\vert I_{A^j_i} = 1\\big]\\Big)$  \n",
    "\n",
    "but given the underlying renewal /fresh start that occurs each time state $i$ is entered, we know   \n",
    "$E\\big[X^{(i)}_j\\big \\vert I_{A^j_i} = 1\\big] = \\mu_i$  \n",
    "so we can re-write this as   \n",
    "\n",
    "$\\sum_{j=N}^{kN} (\\pi_i -\\epsilon)\\mu_i \\leq E\\big[R_1^{(i)}\\big] \\leq \\Big(\\sum_{j=1}^{N-1} \\mu_i \\Big) + \\Big(\\sum_{j=N}^{kN} (\\pi_i +\\epsilon)\\mu_i\\Big)$  \n",
    "\n",
    "now, selecting small enough $\\epsilon$ (which fixes $N$) we select $n = kN$ so we see that \n",
    "\n",
    "$\\mu_i(\\pi_i -2\\epsilon) \\leq \\frac{-1}{k}\\mu_i\\cdot(\\pi_i -\\epsilon) + \\mu_i(\\pi_i -\\epsilon) \\leq \\frac{1}{n}E\\big[R_1^{(i)}\\big] \\leq \\frac{N-1}{kN} u_i  + \\frac{1}{N}\\frac{N(k-1)}{k}\\Big(\\mu_i(\\pi_i +\\epsilon)\\Big)\\leq \\mu_i(\\pi_i + 2\\epsilon)$  \n",
    "\n",
    "where the right hand side and left hand side both hold, for any $\\epsilon \\gt 0$, and then choosing large enough $k$, so we have \n",
    "\n",
    "$\\mu_i\\pi_i -2\\mu_i\\epsilon    =\\mu_i(\\pi_i -2\\epsilon)  \\leq \\frac{E[R_1^{(i)}]}{n}  \\leq \\mu_i(\\pi_i +2\\epsilon)=\\mu_i\\pi_i + 2\\mu_i\\epsilon$  \n",
    "\n",
    "however, $2\\mu_i\\epsilon$ may be made arbitrarily small,\n",
    "\n",
    "and via a virtually idendtical argument (that would unfortunately require more indexing to spell out in detail), we have \n",
    "\n",
    "$\\sum_i \\mu_i(\\pi_i -2\\epsilon)  \\leq \\frac{E[X_1^{(i)}]}{n}  \\leq \\sum_i \\mu_i(\\pi_i +2\\epsilon)$  \n",
    "in fact the argument for the lower bound is the same in this case.  **Except:** unfortunately as in manner similar to the elementary renewal theorem, the upper bound is much more difficult to deal with and may not be directly provable -- and in this case a truncation argument does not immediately come to mind. \n",
    "\n",
    "So what we have to actually work with is \n",
    "\n",
    "$\\sum_i \\mu_i(\\pi_i -2\\epsilon)  \\leq \\frac{E[X_1^{(i)}]}{n} $  \n",
    "\n",
    "However, this (arbitrarily) sharp lower bound and a summation /convex combination of these results over all states is enough to get us to the finish line.  \n",
    "\n",
    "**the close**   \n",
    "Re-run the argument above, over all $i$ then sum to see \n",
    "\n",
    "for $i \\in \\{1,2,3,...\\}$  \n",
    "\n",
    "$P_i = \\frac{E[R_1^{(i)}]}{E[X_1^{(i)}]} = \\frac{\\frac{E[R_1^{(i)}]}{n}}{\\frac{E[X_1^{(i)}]}{n}}$  \n",
    "\n",
    "so \n",
    "\n",
    "$\\sum_i P_i \\frac{E[X_1^{(i)}]}{n} = \\sum_i \\frac{E[R_1^{(i)}]}{n}$  \n",
    "\n",
    "\n",
    "if $\\frac{E[X_1^{(i)}]}{n}=\\gamma$ (i.e. they are homogenous), then we have \n",
    "$\\gamma = \\sum_i P_i\\cdot \\gamma = \\sum_i \\mu_i \\pi_i$  and we are done.  \n",
    "\n",
    "Suppose for a contradiction that they are inhomogenous, then, because we are taking a convex combination on the left hand side (albeit with possibly countably infinite terms), and each $\\mu_i \\pi_i \\gt 0$ then we see that if some term $\\frac{E[X_1^{(i)}]}{n} \\gt \\sum_i \\mu_i \\pi_i = \\gamma$, \n",
    "\n",
    "then there must be (at least) one $E[X_1^{(\\eta)}]$ and some $\\delta \\gt 0$ such that \n",
    "\n",
    "$\\frac{E[X_1^{(\\eta)}]}{n} - \\big(\\sum_i \\mu_i \\pi_i\\big)= -\\delta \\lt 0$  \n",
    "equivalently:  \n",
    "\n",
    "$\\frac{E[X_1^{(\\eta)}]}{n} = \\big(\\sum_i \\mu_i \\pi_i\\big) -\\delta $   \n",
    "must exist.  Then we may again take advantage of choosing large enough $n$ for this particular term, in particular we see \n",
    "\n",
    "$\\sum_i \\mu_i(\\pi_i -2\\epsilon)  \\leq \\frac{E[X_1^{(i)}]}{n} =  \\gamma -\\delta $\n",
    "\n",
    "but we also have an absolutely convergent series and continuity, i.e.   \n",
    "\n",
    "$0 \\lt \\gamma = \\sum_i \\mu_i\\pi_i \\lt \\infty$ \n",
    "\n",
    "but   \n",
    "$\\lim_{2\\epsilon \\to 0^+} \\sum_i \\mu_i(\\pi_i -2\\epsilon) = \\gamma \\leq \\frac{E[X_1^{(\\eta)}]}{n} = \\gamma - \\delta $  \n",
    "\n",
    "which implies that $0 \\leq -\\delta$ but we also know that $\\delta \\gt 0$, which is a contradiction.  \n",
    "\n",
    "the result then follows because $\\lim_{t\\to \\infty}\\frac{r(t)}{t}$ is the same whether we specify the renewal process to include at least $n$ transitions and terminate upon return to state $i$  or if we we specify the renewal process to include $1$ transition and terminate upon return to state $i$.  \n",
    "\n",
    "**What is the flaw in the above?  It assumes $E[X_1^{(k)}] \\lt \\infty$.  In particular for a countable state markov chain, this is quite an assumption, and needs a lot more consideration.**  \n",
    "- - - -  \n",
    "Another way to interpret lazy chains:  \n",
    "Consider a markov (countable state time homogenous, irreducible, positive recurrent) chain with a transition matrix and suppose $\\mathbf P$ that perhaps has no self loops. Now it may be the case that $\\mathbf P$ is periodic, but \n",
    "\n",
    "now consider a convex combination where \n",
    "\n",
    "$\\mathbf A = w_1 \\mathbf P + w_2 \\mathbf I$  \n",
    "\n",
    "now the steady state vector for $\\mathbf A$ is the same as that of $\\mathbf P$ -- we just have an additional guaranty that $\\mathbf A$ is aperiodic now and hence ergodic.  However, if we liked, we could interpret $\\mathbf A$ as a semi-markov process, where the self-loops refer to the times until exit of a given state -- each of which is a discrete time memoryless process -- i.e. each is geometrically distributed. And since each state has an exist probability that is geometrically distributed where parameter $w_2$, we get  \n",
    "\n",
    "$P_i = \\frac{\\pi_i \\mu_i}{\\sum_k \\pi_k \\mu_k} = \\frac{\\pi_i \\frac{1}{w_2}}{\\sum_k \\pi_k \\frac{1}{w_2}}= \\frac{\\pi_i }{\\sum_k \\pi_k } $   \n",
    "\n",
    "and hence the steady state in the time averaged chain is the same as in the jump chain.  This approach is more work and doesn't, in some sense, get us anything more than our existing understanding of markov chains.  However if we switch this memoryless state exit process from discrete time to continuous time, we very naturally get exponentially distributed exit times, and in fact recover the standard continuous time markov process.  (There are more idea to be mined here.)  \n",
    "\n",
    "\n",
    "- - - -  \n",
    "**THIS BELOW ENDING IS SUBTLY BROKEN AND I AM CALCULATING SOMETHING A LITTLE BIT DIFFERENT THAN WHAT I THINK**  \n",
    "*an alternative renewal type closer would use bunching, selecting the $\\frac{E[X_1^{(i)}]}{n}\\gt \\gamma$ or $\\frac{E[X_1^{(i)}]}{n}\\lt \\gamma$ case to be in one bunch and all other states in the other bunch, and then combine this with the alternating renewal theorem.   Note: if there are only 2 states, the below argument has a singularity to consider (though in such a case we can directly address the problem and e.g. get the desirred upper bound exactly via a geometric series argument).  In all other cases, we select bunch one to be $\\pi_A \\neq \\frac{1}{2}$  and bunch two to $\\pi_B = 1-\\frac{1}{2} \\neq \\frac{1}{2}$.  (In short: any excess over gamma implies at least one other value with less than gamma -- and these both can have probability of $\\frac{1}{2}$ iff there are only 2 states... if there are more than 2 states, at least one of these has probability not equal to one half, and we select that to be in bunch one.*  \n",
    "\n",
    "The alternative renewal setup stated at the top of the page in Ross has, under our bunched setup, \n",
    "\n",
    "$P_{i=A} = \\frac{\\bar{A}}{\\bar{A} + \\bar{B}}$  and \n",
    "$P_{i=B} = \\frac{\\bar{B}}{\\bar{A} + \\bar{B}}$  \n",
    "\n",
    "but the work we've already done tells us that $\\bar{A} = \\mu_{i=A}$ and $\\bar{B} = \\mu_{i=B}$, so we know \n",
    "\n",
    "$\\frac{\\mu_{i=A}}{\\pi_{i=a}^{-1}E[X^{(i=A)}]} = \\frac{\\bar{A}}{\\bar{A} + \\bar{B}} \\longrightarrow E[X^{(i=A)}] = \\pi_{i=a}(\\bar{A} + \\bar{B})$  \n",
    "\n",
    "and  \n",
    "\n",
    "$\\frac{\\mu_{i=B}}{\\pi_{i=B}^{-1}E[X^{(i=B)}]} = \\frac{\\bar{B}}{\\bar{A} + \\bar{B}}$  \n",
    "\n",
    "$\\mathbf {Mv}= \\left[\\begin{matrix}\\pi_{i=A}^{-1} & \\pi_{i=B}^{-1} \\\\ P_{i=A} & P_{i=B}\\end{matrix}\\right]\\left[\\begin{matrix}E[X^{(i=A)}]\\\\E[X^{(i=B)}]\\end{matrix}\\right]\n",
    " =\\left[\\begin{matrix}2(\\bar{A} + \\bar{B})\\\\2\\end{matrix}\\right]$  \n",
    "\n",
    "\n",
    "\n",
    "** Bayes take **  \n",
    "*needs more work*  \n",
    "\n",
    "again supposing we have an irreducible aperiodic (jump) chain:  \n",
    "\n",
    "in the typical integer time based countable state markov chain we have \n",
    "\n",
    "$\\text{number of transitions}(t) \\to \\infty$ as $t \\to \\infty$  \n",
    "\n",
    "and if we sample according to a long-run equilibrium or use a time averaged result, then we get a mixture that tends the limitting distribution.  \n",
    "\n",
    "However, if we allow a semi-markov process and we sample not according to number of transitions but instead in as a function of time, then what we get is as $t \\to \\infty$ the jump chain still tends to the steady state.  This in effect gives us a prior probability of what state our sample will be in.  \n",
    "\n",
    "(Recall e.g. at end of renewal theory from Feller that the equilibrium renewal distribution tends to   \n",
    "$\\frac{1}{\\bar{X}}\\int_t^{t + \\delta} \\Pr{X\\gt x}dx$  for some $\\delta \\gt 0$)  \n",
    "\n",
    "**needs more cleanup around here... probably delete the above parenthesis**  \n",
    "\n",
    "hence once in equilibirum, we have a prior probability of selecting a given state, and a likelihood function proportional to the probability density (or mass) function multiplied by time.  Hence for any given state $i$ we have \n",
    "\n",
    "$\\text{prior} \\cdot \\text{likelihood function}$ and integrating over time, we get $\\pi_i \\cdot \\mu_i$\n",
    "(needs cleaned up)  \n",
    "\n",
    "with a normalizing constant $c$ given by  \n",
    "\n",
    "$c^{-1} = \\sum_i \\pi_i \\cdot \\mu_i$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
