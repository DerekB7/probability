{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run 'mitx_plotting.ipynb'\n",
    "# %run '_normal_distribution_class.ipynb'\n",
    "\n",
    "from fractions import Fraction\n",
    "# this imports the do_plots() function into the workbook, albeit in a Jupyter / Ipython way\n",
    "from sympy.abc import a, b, c, d, e, f, g, h, q\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from sympy import Integral\n",
    "from scipy.sparse import dok_matrix\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse\n",
    "import scipy\n",
    "import sympy as sp\n",
    "import math\n",
    "# from sympy.mpmath import fac\n",
    "# from sympy import gamma\n",
    "# import itertools\n",
    "# import pyperclip\n",
    "\n",
    "\n",
    "# huge find:%%!\n",
    "# press 'l' in command mode (esc) to toggle line numbers in a given cell, which is quite helpful for auditing\n",
    "import numba\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "![example markov chain](illustrations/markov_chain_example__.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preamble**\n",
    "\n",
    "In the form $\\mathbf{A}^n\\mathbf x = \\mathbf b$, we can represent the above graph as\n",
    "\n",
    "$$ \\mathbf{A} = \\left[\\begin{matrix}0.3 & 0.0 & 0.1 & 0.6 & 0.0 & 0.0\\\\0.7 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\\\0.0 & 0.75 & 0.0 & 0.0 & 0.0 & 0.0\\\\0.0 & 0.0 & 0.3 & 0.0 & 0.0 & 0.0\\\\0.0 & 0.25 & 0.4 & 0.4 & 1.0 & 0.0\\\\0.0 & 0.0 & 0.2 & 0.0 & 0.0 & 1.0\\end{matrix}\\right]$$\n",
    "\n",
    "This is restated below, with labels.\n",
    "\n",
    "![example tranposed Transition Matrix](illustrations/markov_chains_A.png)\n",
    "\n",
    "\n",
    "*Technical note: there is some some discretion allowed in how you set up your transition matrix.  The key rules to observe are (a) always make sure that the matrix is square, (b) always make sure that the entries along the diagonal refer to self-loops, and (c) in order to make things easier later on, be sure to put all transient states next to each other, and all absorbing states next to each other.*\n",
    "\n",
    "note that this is a column oriented approach -- all columns sum to one.  You may often see it represented in row form of:\n",
    "\n",
    "i.e. all rows sum to one.  The transposed form is consistent with\n",
    "\n",
    "![example tranposed Transition Matrix](illustrations/markov_chains_AT.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\\mathbf{x^T(A^T)}^n = \\mathbf b^T$\n",
    "\n",
    "The answers are identical and you can transpose you result if you'd rather read off your results from a column vector than a row vector.  \n",
    "\n",
    "Either approach is ok, and the transposing of the matrix is an elementary operation.  For the most part, the convention in Linear Algebra, especially when talking about systems of equations or computing products of matrices and vectors is, where possible, to express the the algebra as a matrix vector product, *not* a row vector matrix product.  That is the convention I follow.\n",
    "\n",
    "$\\mathbf A^n\\mathbf x = \\mathbf b$\n",
    "\n",
    "**Long Run Behavior**\n",
    "\n",
    "What we are interested in is:\n",
    "\n",
    "$\\lim_{n\\to \\infty} \\mathbf A^n \\mathbf x = SteadyState$ \n",
    "\n",
    "If there was only one absorbing state, the vector associated with that is what b would become.  If there were no absorbing states (but certain other conditions were met -- e.g. Perron's theorem) then the steady state of the single dominant (eigen) vector is what b would become. \n",
    "\n",
    "Note that in all of the above cases, it does not matter what the vector x has in it, so long as its entries sum to one, and they are all non-negative (reals).  Another way of thinking about that is: in the limit, A itself will be composed solely of column vectors associated with the single dominant vector.  If we instead modelled this as $\\mathbf {x^T(A^T)}^n = \\mathbf b^T$, in the limit we would instead see that A is *row* vectors associated with the single dominant vector -- but again, this is a fairly minor point related to transposition.\n",
    "\n",
    "The basic idea is that $\\mathbf A$ is a map.  It is a map in the classic linear algebra sense that it acts a linear map from the finite vector $\\mathbf x$ to finite vector $\\mathbf b$ which is another way of saying that $\\mathbf A$ acts as a linear function (in some contexts this is acceptable -- definitions of linear function seem to be inconsistent depending on the branch of math involved).  \n",
    "\n",
    "It is also a map, in that once we get comfortable with the notation, we'll recognize the underlying graph here. Indeed in most Comp Sci classes discussing graphs / graph theory, students are instructed to store their graphs in either an adjacency matrix or adjacency list (or variant thereof).  Our graph is effectively just a weighted direction graph (digraph) and we are using an adjacency matrix to store its weights.  (In practice markov chains tend to be very sparse and hence you'd tend to use a sparse matrix implementation which combines a lot of the advantages of an adjacency matrix with those of an adjacency list, but that is beyond the scope of this writeup.) \n",
    "\n",
    "So, for example, say we are on starting from a mixed position that is $\\frac{1}{2}$ T and $\\frac{1}{2}$ U,  then our x vector would look like $\\left[\\begin{matrix}0.5\\\\0.5\\\\0.0\\\\0.0\\\\0.0\\\\0.0\\end{matrix}\\right]$\n",
    "\n",
    "So, if we wanted to predict where we'd be on average 2 steps from now, the matrix algebra is \n",
    "\n",
    "$\\mathbf A^2 \\mathbf x = \\mathbf b$\n",
    "\n",
    "\n",
    "and while matrix vector multiplication does not commute, it is associative, so we can write the above as\n",
    "\n",
    "$\\mathbf A^2 \\mathbf x = \\mathbf b = (\\mathbf{A A)x = A(Ax)}$\n",
    "\n",
    "Thus if we are interested in using iterative methods to approximate the steady ending state, given an initial state, we could use some arbitrarily large number, say n = 10,000, and do $\\mathbf A^{10000} \\mathbf x = \\mathbf b$\n",
    "\n",
    "*Technical note: in practice you'd want to do that as a for loop implementation which is stricly faster, and will preserve sparsity.  Also note that diagonalizaton may be useful in some cases, though it is outside the scope of this writeup.*\n",
    "\n",
    "The main point is that we can think about $A^n$ as being a probabilistic map from your current position to your next position(s), n states in the future.  \n",
    "\n",
    "**Partitioning the Matrix**\n",
    "\n",
    "\n",
    "Now let's revisit the transpose of $\\mathbf A$.\n",
    "\n",
    "![example markov chain](illustrations/AT_partioned__.png)\n",
    "\n",
    "\n",
    "The above map from various starting states to various ending states has been partitioned into four pieces.\n",
    "\n",
    "The Green portion, in the bottom right, references the 2 absorbing states.  Given that you start from Y, you only visit Y.  And given that you start in Z, you only visit Z. One consequence of this is that all values in the Yellow portion must be zero.  \n",
    "\n",
    "The Red portion represents the probabililities associated with visiting one of the absorbing states, given that you currently are not in an absorbing state.  Finally the blue portion represents the 'main game' which are the transient states that you may begin in, but eventually must leave, as n grows large.\n",
    "\n",
    "Based on what we've said so far, your probabilistic map must tell you to head toward the absorbing states as n grows large. \n",
    "\n",
    "That is: even if a 'small' n value like 100, -- if you calculate $(\\mathbf  A^T)^{100}$, you'll notice that the output map tells you you're almost certain to end up in an absorbing state, no matter whether you started at T, U, V, or W.  And of course if you start at Y or Z, you are already in an absorbing state and cannot leave.  Put differently, the values of interest in the map will move from the blue zone to the red zone as n grows large.  \n",
    "\n",
    "- - - - \n",
    "\n",
    "**Expected Time Until Absorbtion**\n",
    "\n",
    "Now if we were interested in figuring out the expected number of steps until being absorbed, we could think about it as follows: \n",
    "\n",
    "Given that you start in T, U, V, or W (or some combination thereof -- again with probabilities summing to one), you know that you have at least one turn until absorbtion.  From there let's look at the blue slice of our map -- we'll call it $\\mathbf{Blue}$.\n",
    "\n",
    "There are mutliple ways to frame this, of course. What follows is one particular way that I find to be helpful.  \n",
    "\n",
    "At time zero, from T, U, V, W: you for sure have at least one turn until absorbtion no matter what.  \n",
    "\n",
    "In our example, let's say you start at W.\n",
    "\n",
    "This means you get that one turn, then 0.6 of the time you get one more turn as shown by the map for $\\mathbf {Blue}^1$ + (0.18 + 0.42 ) of the time you get another turn, as shown by the map at $\\mathbf {Blue}^2$, and (0.054 +0.126 + 0.315) of the time you get another turn as shown by the map at $\\mathbf {Blue}^3$...\n",
    "\n",
    "- - - -\n",
    "\n",
    "![example markov chain partioned](illustrations/markov_3_level_partioned_.jpg)\n",
    "\n",
    "and so on...\n",
    "\n",
    "Thus we have the sketch of a geometric series here.  On average, it seems that our n turns until absorbtion, can be described by the following series:\n",
    "\n",
    "$ \\mathbf { N = I + Blue + Blue^2 + Blue^3 + Blue^4 + Blue^5 ...}$\n",
    "\n",
    "note that matrix multiplication distributes, so we can factor this out as follows.  \n",
    "\n",
    "$\\mathbf { N = I + Blue^1(I + Blue + Blue^2 + Blue^3 + Blue^4 + Blue^5 ...}$\n",
    "\n",
    "$ \\mathbf {IN = I + Blue(N)}$\n",
    "\n",
    "$\\mathbf { IN - BlueN = I}$\n",
    "\n",
    "$\\mathbf {(I - Blue)N = I}$\n",
    "\n",
    "$\\mathbf {N = (I - Blue)^{-1}I}$\n",
    "\n",
    "$\\mathbf {N = (I - Blue)^{-1}}$\n",
    "\n",
    "- - - -\n",
    "*Technical note: As outlined at the beginning of the post, we are interested in behavior in the limit and we know that everything ends up in an absorbing state. Thus everything must leave Blue, which implies*  $\\lim_{n\\to \\infty} \\mathbf Blue^n = \\mathbf 0$ *(where $\\mathbf 0$ is just a matrix filled with zeros).  We could also approach our problem as solving a finite geometric series with n terms, then use telescoping (read: mass cancelation) to get to a position equivalent to where we left off above :  * $\\mathbf {N = (I + Blue + Blue^2 + Blue^3 + Blue^4 + Blue^5 + ... + Blue^{n-1})}$.  Then multiply both sides by $(\\mathbf {I - Blue})$, and we get: $\\mathbf {(I - Blue) N = (I - Blue)(I + Blue + Blue^2 + Blue^3 + Blue^4 + Blue^5 + ... + Blue^{n-1}) = I} - \\mathbf{Blue}^n$* and then take* $\\lim_{n\\to \\infty} \\mathbf {Blue}^n = \\mathbf 0$, *which makes the $\\mathbf {Blue}^n$ *term disapear for the infinite series case, leaving us with $\\mathbf {(I - Blue) N = I}$, or   $\\mathbf {N = (I - Blue)}^{-1}$\n",
    "- - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*begin interlude*  \n",
    "*Note*: the fact that   \n",
    "$\\lim_{n\\to \\infty} \\mathbf {Blue}^n = \\mathbf 0$   \n",
    "implies there is *no vector* $\\mathbf x \\neq \\mathbf 0$ such that  \n",
    "$\\mathbf {Blue}\\cdot \\mathbf x = \\mathbf x$  (rationale stated below)  \n",
    "\n",
    "but this is equivalent to  \n",
    "$\\mathbf {Blue}\\cdot \\mathbf x = \\mathbf I \\mathbf x$  or  \n",
    "$\\big(\\mathbf {Blue}-\\mathbf I\\big)\\cdot \\mathbf x = \\mathbf 0$  \n",
    "which is to say that the square matrix $\\big(\\mathbf {Blue}-\\mathbf I\\big)$ has a trivial nullspace and hence is invertible.  \n",
    "\n",
    "*rationale:*   \n",
    "$\\mathbf {Blue}\\cdot \\mathbf x = \\mathbf x$  \n",
    "implies  \n",
    "$\\mathbf {Blue}^n\\cdot \\mathbf x = \\mathbf x$   \n",
    "\n",
    "because otherwise, using associativity we'd have  \n",
    "$ \\mathbf x = \\lim_{n\\to \\infty} \\underbrace{ \\big(\\mathbf {Blue}\\big(\\mathbf {Blue}\\big(...\\big(\\mathbf {Blue}}_{\\text{n times}} \\cdot \\mathbf x\\big)\\big)\\big)...\\big) = \\lim_{n\\to \\infty} \\big(\\mathbf {Blue}^n\\big)\\mathbf x  = \\mathbf 0$   \n",
    "\n",
    "or in somewhat *fancier form*:  \n",
    "$\\mathbf {Blue}\\cdot \\mathbf x = \\mathbf x$ implies an eigenvalue of 1 which is a lower bound on e.g. the Frobenius norm of a matrix -- see \"shur's inequality\" notebook in linear algebra folder for more information, hence existence of eigenvalue of one implies we cannot have $\\big \\Vert \\mathbf {Blue}^n - \\mathbf 0 \\big \\Vert_F \\lt \\epsilon$ for any $\\epsilon \\gt 0$ for large enough $n$ \n",
    "(i.e. the defition of $\\lim_{n\\to \\infty} \\mathbf {Blue}^n = \\mathbf 0$)\n",
    "because  \n",
    "$1 \\leq \\big \\Vert \\mathbf {Blue}^n \\big \\Vert_F = \\big \\Vert \\mathbf {Blue}^n - \\mathbf 0 \\big \\Vert_F$ for all $n$ so, e.g. selecting $\\epsilon := \\frac{1}{2}$ breaks the definition of a limit.      \n",
    "\n",
    "an almost verbatim argument shows that the maximal magnitude eigenvalue of $\\mathbf {Blue}$ cannot be $\\gt 1$ hence it must be $\\lt 1$  \n",
    "*end interlude*  \n",
    "- - - - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we left off with  \n",
    "$\\mathbf {N = (I - Blue)^{-1}}$\n",
    "or  \n",
    "$\\mathbf N^{-1} = \\mathbf {(I - Blue)}$\n",
    "\n",
    "Thus we have $\\mathbf N^{-1}$, which is easy to compute as follows.\n",
    "\n",
    "$\\mathbf N^{-1} = \\begin{bmatrix}\n",
    "1 & 0 & 0 & 0\\\\ \n",
    "0 & 1 & 0 & 0 \\\\ \n",
    "0 & 0 & 1 & 0\\\\ \n",
    "0 & 0 & 0 & 1\n",
    "\\end{bmatrix} - \\begin{bmatrix}\n",
    "0.3 & 0.7 & 0.0 &  0.0\\\\ \n",
    "0.0 & 0.0 & 0.75 &0.0 \\\\ \n",
    "0.1 & 0.0 & 0.0 & 0.3\\\\ \n",
    "0.6 & 0.0 & 0.0 & 0.0\n",
    "\\end{bmatrix} = \\left[\\begin{matrix}0.7 & -0.7 & 0.0 & 0.0\\\\0.0 & 1.0 & -0.75 & 0.0\\\\-0.1 & 0.0 & 1.0 & -0.3\\\\-0.6 & 0.0 & 0.0 & 1.0\\end{matrix}\\right]$\n",
    "\n",
    "\n",
    "So once we invert the above matrix we get\n",
    "\n",
    "$\\mathbf  N = (\\mathbf N^{-1})^{-1} = \\left[\\begin{matrix}1.80831826401447 & 1.26582278481013 & 0.949367088607595 & 0.284810126582278\\\\0.379746835443038 & 1.26582278481013 & 0.949367088607595 & 0.284810126582278\\\\0.506329113924051 & 0.354430379746835 & 1.26582278481013 & 0.379746835443038\\\\1.08499095840868 & 0.759493670886076 & 0.569620253164557 & 1.17088607594937\\end{matrix}\\right]$\n",
    "\n",
    "This is a (messy) aggregation of all the portions of times we will, on average, be in a given transient state.  If we sum accross the above matrix's columns, we can get the number of times, on average, we are able to move around before being absorbed, given some starting state(s) in T, U, V, and/or W.  From a Linear Algebra perspective, we'll use matrix vector products to do this. Let's use the one's vector, denoted by: \n",
    "\n",
    "$\\mathbf 1 = \\begin{bmatrix}\n",
    "1\\\\ \n",
    "1\\\\ \n",
    "1\\\\ \n",
    "1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "We can write our expected time until absorbtion as $\\mathbf{ N1 = m} = \\begin{bmatrix} m_T\\\\\n",
    "m_U\\\\ \n",
    "m_V\\\\ \n",
    "m_W\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "Where m denotes our mean time till absorbtion, given a particular starting state. Alternatively we multiply both sides of the equation by $\\mathbf N^{-1}$ and we have\n",
    "\n",
    "$\\mathbf N^{-1}\\mathbf m = \\mathbf 1$\n",
    "\n",
    "In this situation, we in effect have a case where we need to solve a system of linear equations -- i.e. solve for $m_T, m_U, m_V, m_W$.  In reality, this should get us the same answer as when we inverted $N^{-1}$ but it is actually a better approach because solving a system of linear equations is generally expected to be faster than inverting a matrix, and in some cases it has much better numeric stability (though I've heard explicit matrix inversion numeric stability is much better than it once was).\n",
    "\n",
    "To conclude: We don't need to calculate $N$, we just need to solve the following system of equations $\\mathbf N^{-1}\\mathbf m = \\mathbf {(I- Blue)m}= \\mathbf 1$\n",
    "- -  -\n",
    "But why might this work?  Here is an alternative interpretation of the above.  \n",
    "\n",
    "Let's say we are starting at T.  \n",
    "\n",
    "To evaluate where we go from there, we could simply the reference original markov chain, or our map Blue.  Either one is useful here. \n",
    "\n",
    "![choose a guide](illustrations/markov_either_guide__.jpg)\n",
    "\n",
    "So if we wanted to know our expected time until absorbtion, we can approach it in a way that is similar to how we might tackle a simpler discrete distribution that exhibits memorylessness.  We could write this as follows\n",
    "\n",
    "$E\\big[AbsorbTime \\big|Start = T\\big] = 1 + 0.3 *E\\big[AbsorbTime \\big|Start = T\\big] + 0.7 *E\\big[AbsorbTime \\big|Start = U\\big]$\n",
    "\n",
    "using our symbols from the m vector we would re-write this as \n",
    "\n",
    "$m_T = 1 + 0.3 m_T + 0.7 m_U$\n",
    "\n",
    "rearranging terms we get \n",
    "\n",
    "$ 1 m_T - 0.3 m_T  - 0.7 m_U = 1$\n",
    "\n",
    "and further\n",
    "\n",
    "$ 0.7 m_T - 0.7 m_U + 0 m_V + 0 m_W = 1$\n",
    "\n",
    "We can now repeat this process for all other transient states as follows. (Note I've indented it in a block quote in case the reader would like to skip ahead.) \n",
    "\n",
    "> Similarly for our expected time until absorbtion from U we could write: $ m_U = 1+  0.75 m_V$\n",
    "rearrange terms $ 0m_T  + 1m_U - 0.75 m_V + 0 m_W = 1$\n",
    "\n",
    "> For v, we could write it as: $ 1m_V = 1 + 0.1m_T + 0.3m_W $\n",
    "re arrange terms and we get: $ -0.1m_T + 0 m_U + 1m_V - 0.3m_W  = 1 $\n",
    "\n",
    ">  Finally for W we could write is as $m_W = 1 + 0.6 m_T$\n",
    "re arrange terms and we get: $- 0.6 m_T + 0 m_U +_ 0m_V + 1m_W = 1$\n",
    "\n",
    "\n",
    "if we collect the final equations we get\n",
    "$$ 0.7 m_T - 0.7 m_U + 0 m_V + 0 m_W = 1$$\n",
    "$$ 0m_T  + 1m_U - 0.75 m_V + 0 m_W = 1$$\n",
    "$$ -0.1m_T + 0 m_U + 1m_V - 0.3m_W  = 1 $$\n",
    "$$- 0.6 m_T + 0 m_U + 0 m_V + 1m_W = 1$$\n",
    "\n",
    "But notice that if we work through the matrix vector multiplication, we see that the above statement is identical to saying \n",
    "$$\\mathbf N^{-1}\\mathbf m =\\left[\\begin{matrix}0.7 & -0.7 & 0.0 & 0.0\\\\0.0 & 1.0 & -0.75 & 0.0\\\\-0.1 & 0.0 & 1.0 & -0.3\\\\-0.6 & 0.0 & 0.0 & 1.0\\end{matrix}\\right] \\begin{bmatrix}\n",
    "m_T\\\\ \n",
    "m_U\\\\ \n",
    "m_V\\\\ \n",
    "m_W\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "1\\\\ \n",
    "1\\\\ \n",
    "1\\\\ \n",
    "1\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "or more succinctly\n",
    "\n",
    "$\\mathbf N^{-1}\\mathbf m =  \\mathbf 1$\n",
    "\n",
    "The only difference is, in the second case our answer comes in a container called a vector. \n",
    "\n",
    "Thus we have two different interpretations of solving for expected time until absorbtion.  In one case we think of N as a geometric series, summing the the value of all possible iterations of our abstract object -- a map -- named Blue.  In the other case we find $\\mathbf N^{-1}$ to be useful as it simply organizes a system of linear equations that we'd like to solve, even if we know very little about matrices. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Absorbtion Probabilities** \n",
    "\n",
    "The above represents a comprehensive look at expected time until absorbtion for Markov Chains.  Another key question people may ask is:  Given a particular starting state, we know absorbtion will eventually occur, but in which state will it be?  It could be Z or Y, so what kind of splits should we expect?\n",
    "\n",
    "Again, there are very clear iterative approaches here, but let's look back at our partitioned map.\n",
    "\n",
    "![example markov chain partioned](illustrations/AT_partioned__.png)\n",
    "\n",
    "Let's look at the various ways we can become absorbed, using the Red portion of that map, and call it the matrix, R\n",
    "\n",
    "![R as matrix](illustrations/markov_R__.png)\n",
    "\n",
    "Now if only we had a map that listed, on average, the amount of times we visited every other transient state, given some starting position...\n",
    "\n",
    "If we had such an object, we could just look up on average how many times we visit T, U, V, W given some starting position and then use our above map, R, to multiply the respective percentages.  This would allow us to calculate the portion of times that we end up in Y, and Z.  \n",
    "\n",
    "Luckily for us, we do have such a map -- it is called $\\mathbf N$.\n",
    "\n",
    "let $\\mathbf S = \\begin{bmatrix} \n",
    "s_{T,Y} & s_{T,Z} \\\\\n",
    " s_{U,Y} & s_{U,Z}\\\\ \n",
    "s_{V,Y} & s_{V,Z} & \\\\ \n",
    "s_{W,Y} & s_{W,Z} & \n",
    "\\end{bmatrix}$\n",
    "\n",
    "(Here $\\mathbf S$ is used in nod to a 'sink' in the graph.  In some sense, a sink is quite similar to an absorbing state, though there are some major technical differences relating to nilpotence of a graph connected to a sink vs an abosrbing state.  So, for example, $\\mathbf S_{T,Y}$ refers to the portion of time you'll end up in the sink called Y, given a starting position in T, if we repeated this experiment many times.)\n",
    "\n",
    "We then see that $\\mathbf {S = NR}$  \n",
    "\n",
    "And again, since inverting $\\mathbf N^{-1}$ into $\\mathbf N$ is costly, we may instead set this up as \n",
    "$\\mathbf N^{-1}\\mathbf S = \\mathbf R$ and then solve the embedded system of equations to get $\\mathbf S$.  \n",
    "\n",
    "\n",
    "Alternatively, if we didn't know much about matrices, we might look back to our original picture, or look to our handy blue and red maps. \n",
    "\n",
    "![choose a guide](illustrations/markov_either_guide2b.jpg)\n",
    "\n",
    "For example, given that we start in V, what is the probability of ultimately being absorbed into Y?  We know it has to be at least 0.4, because V goes directly to Y with probability 0.4.  But there is also the probability that you end up in Y from visiting W at the end of your first turn or even T.  So, using the notation above, we could say \n",
    "\n",
    "$\\mathbf S_{V,Y} = 0.4 + 0.3 \\mathbf S_{W,Y} + 0.1 \\mathbf S_{T,Y}$\n",
    "\n",
    "and if we wanted to know the expected portion of times we'd end up in Z, we could write this as \n",
    "$\\mathbf S_{V,Z} = 0.2 + 0.3 \\mathbf S_{W,Z}+ 0.1 \\mathbf S_{T,Z}$\n",
    "\n",
    "re arranging terms we get \n",
    "$$-0.1 \\mathbf S_{T,Y} + 0 + 1\\mathbf S_{V,Y} - 0.3 \\mathbf S_{W,Y} = 0.4$$\n",
    "$$-0.1 \\mathbf S_{T,Z} + 0 + 1\\mathbf S_{V,Z} - 0.3 \\mathbf S_{W,Z} = 0.2$$\n",
    "\n",
    "we can repeat this process for all 3 other transient starting states and we get a system of equations that is identical to \n",
    "$\\mathbf N^{-1}\\mathbf S = \\mathbf R$, except, once again, when we use the matrix algebra formulation our answer comes in a container-- this time in two column vectors stacked next to each other -- and we call this a matrix. \n",
    "\n",
    "So there you have it, two different ways to think about time until absorbtion, and respective absorbtion probabilities, all using straightforward matrix algebra.  \n",
    "\n",
    "- - - \n",
    "Techincal note:  Since all probaiblities must sum to one, if you have k absorbing states, you only need to solve for the proportion of times you'll end up in specific absorbing states for k - 1 of them, and then recognize that that sum of the respective probabilities for those k - 1 terms is in fact the complement to the probability for the respective kth value.  \n",
    "\n",
    "- - - -  \n",
    "*remark:*  \n",
    "\n",
    "The first approach to getting the absorbing probabilities was a heuristic approach for calculating the absorbtion probabilities (though it may potentially be rigorized via a renewal-rewards argument which is an open item)  \n",
    "\n",
    "The second approach of solving systems of equations was a (sketch) of a rigorous approach.  Part 1 of this writeup showed that there is a finite first moment for time until absorbtion from any given starting state. We didn't dwell on it but there are many different ways to prove that $\\mathbf {Blue}^n \\to \\mathbf 0$, some purely algebraic, some leaning on the notion of stochastic dominance by some geometric random variable.  Among other things this implies that an absorbing state is reached with probability 1 (e.g. by application of Markov's Inequality).    \n",
    "\n",
    "Since we know absorbtion occurs with probability 1, we may consider applying \"**first step analysis**\" to see the probability of being absorbed in any given absorbing state, for any given starting state.  This tells us that \n",
    "\n",
    "$\\mathbf S_{i,j} = \\mathbf e_i^T \\mathbf{Blue}\\mathbf S \\mathbf e_j + \\mathbf e_i^T \\mathbf R \\mathbf e_j $\n",
    "\n",
    "considering this over all starting (transient) states $i$, and all absorbing states   \n",
    "\n",
    "$\\mathbf I \\mathbf S =\\mathbf S  = \\mathbf{Blue}\\mathbf {S}+ \\mathbf R $  \n",
    "$\\big(\\mathbf I -\\mathbf{Blue}\\big)\\mathbf S = \\mathbf R $   \n",
    "$\\mathbf S = \\big(\\mathbf I -\\mathbf{Blue}\\big)^{-1}\\mathbf R $  \n",
    "$\\mathbf S = \\mathbf {NR} $  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*note:* A fancier finish is to look at this one column at time, i.e.  \n",
    "\n",
    "$\\mathbf s_k = \\mathbf S\\mathbf e_k = \\mathbf {NR}\\mathbf e_k $   \n",
    "\n",
    "and to recognize that  \n",
    "$\\mathbf v^{(k)} = \\begin{bmatrix} \n",
    "\\mathbf s_k \\\\\n",
    " \\mathbf e_k\\\\ \n",
    "\\end{bmatrix}$  \n",
    "\n",
    "gives a right eigenvector (with eigenvalue 1), or equivalently a martingale, for our original/whole transition matrix, i.e.  \n",
    "\n",
    "$\\mathbf A^T \\mathbf v^{(k)} = \\mathbf v^{(k)}$  and  \n",
    "$\\big(\\mathbf A^T\\big)^n \\mathbf v^{(k)} = \\mathbf v^{(k)}$  \n",
    "\n",
    "but starting in transient state $i$ we have  \n",
    "$\\mathbf e_i^T \\big(\\mathbf A^T \\mathbf v^{(k)} = \\mathbf e_i^T \\mathbf v^{(k)} = s_{i,k} $  \n",
    "\n",
    "The probability of being absorbed into state k ($p_{i,k}^{(n)}$) is monotone non-decreasing in $n$ and bounded above by $s_{i,k}$ -- why? because for each natural number $n$ we have this same fixed point that is equal to a convex combination of real non-negative values, that includes ($p_{i,k}^{(n)} \\cdot 1$).  \n",
    "\n",
    "Hence by monotone convergence we know  \n",
    "$\\lim_{n\\to \\infty}p_{i,k}^{(n)} = L_{i,k} \\leq s_{i,k} $  \n",
    "that is the limit exists, and is bounded above by the relevant value in $\\mathbf s$.  And this holds for all transient states $i$.  But these values must obey first step analysis, which means that it must be the case that the upper bound is met with equality, i.e.  \n",
    "$\\mathbf l_{k} =\\mathbf s_k$  \n",
    "because as we've shown above, $\\mathbf S$ is the unique solution to the first step analysis problem.  \n",
    "\n",
    "(The above argument is a succinct 'nuts and bolts' approach to evaluating the absorbtion probabilities.  The argument can be streamlined by explicit reference to martingale arguments -- see 'martingales.ipynb' for a look at these and related arguments.)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "In the last post, we worked through the some expected value calculations, including the expected time until abosrbtion, for absorbing state markov chains.\n",
    "\n",
    "In this post we'll build on that analysis to calculate the variance of time until absorbtion which is a more challenging calculation.  \n",
    "\n",
    "Note: this post considers general cases involving finite variance.  Infinite Variance and Undefined Variance is outside the scope of this post.  You could, for example, have problematic variance calculations if some portion of the graph formed a cycle that never reaches an absorbing state.  There are workarounds from here, of course, but those too are outside the scope.\n",
    "\n",
    "First we'll re-visit and re-interpret some of the fomula's from the prior post. \n",
    "![choose a guide](illustrations/markov_either_guide2b.jpg)\n",
    "\n",
    "\n",
    "\n",
    "$\\mathbf 1 = \\begin{bmatrix}\n",
    "1\\\\ \n",
    "1\\\\ \n",
    "1\\\\ \n",
    "1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$ \\mathbf N^{-1} = \\mathbf I - \\mathbf{Blue}$ \n",
    "\n",
    "$\\mathbf {N1} = \\mathbf m = \\begin{bmatrix} m_T\\\\\n",
    "m_U\\\\ \n",
    "m_V\\\\ \n",
    "m_W\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This vector, m, had the mean time until absorbtion given a start in T, U, V, or W, and these are listed as $m_T, m_U, m_V,$ and $m_W$ respectively\n",
    "\n",
    "Recall we originally defined N as\n",
    "\n",
    "$ \\mathbf {N = I + Blue + Blue^2 + Blue^3 + Blue^4 + Blue^5 ...}$\n",
    "\n",
    "$\\mathbf {N = (I - Blue)}^{-1}$\n",
    "\n",
    "\n",
    "*note: for purposes of this writeup any square matrix to the zero power is the idenity matrix of comparable size.*\n",
    "\n",
    "Indeed all we had to do was multiply it by the the ones vector h, i.e. sum accross its column space (I may want to put a picture in here) in order to get expected time until absobrtion.  We denote this as m.  We can also call this $E[n]$. \n",
    "\n",
    "In general we know that to get variance of some variable x, we can calulate this by $var(X) = E[X^2] - (E[X])^2 = E[X^2] - E[X]E[X] $.  So in our case, it *seems* like we'd want $var(n) = E[n^2] - E[n]E[n]$.  We basically already have the second term (note there is a technical issue involved with 'squaring' vectors that we'll address at the end).  For now we need to calculate $E[n^2]$. \n",
    "\n",
    "How might we go about this?  The key thing to think about this is: (a) we are interested only in starting in ${T,U,V,W}$ -- starting from an absorbing state is trivial -- and thus the number of iterations until absorbtion must be an integer  $\\geq 1$, though technically there is no upper bound on this number -- it can go on until $\\infty$.  Thus we might first try to reformulate our expected value calculation (and in turn our varaiance calculation) as one that we'd do for say the  geometric distribution (or a Poisson distribution).\n",
    "\n",
    "For a distribution like this, we'd say:\n",
    "\n",
    "$E[X] = \\sum_{0}^{\\infty}x * prob(x) = \\sum_{1}^{\\infty}x * prob(x) $\n",
    "\n",
    "$E[X^2] = \\sum_{0}^{\\infty}x^2 * prob(x) = \\sum_{1}^{\\infty}x^2 * prob(x) $\n",
    "\n",
    "so for our absorbing state distribution, we can use something along the lines of : \n",
    "\n",
    "$E[n] = \\sum_{1}^{\\infty}n * prob(n) $\n",
    "\n",
    "$E[n^2] = \\sum_{1}^{\\infty}n^2 * prob(n) $\n",
    "\n",
    "That is what we want.  What we currently have is:\n",
    "\n",
    "$ \\mathbf {N = I + Blue + Blue^2 + Blue^3 + Blue^4 + Blue^5 ...}$\n",
    "\n",
    "rewritten slightly as \n",
    "\n",
    "$ \\mathbf N = \\mathbf{Blue}^0 + \\mathbf{Blue}^1 + \\mathbf{Blue}^2 + \\mathbf{Blue}^3 + \\mathbf{Blue}^4 + \\mathbf{Blue}^5 ...$\n",
    "\n",
    "where $\\mathbf{Blue}^0: = \\mathbf I$, or in sigma notation we can write that as:\n",
    "\n",
    "$\\mathbf N = \\sum_{k = 0}^{\\infty} \\mathbf {Blue}^k$\n",
    "\n",
    "yet what we want is something like: \n",
    "$\\mathbf N = \\sum_{n = 1}^{\\infty}n * prob(n) $\n",
    "\n",
    "aka:\n",
    "\n",
    "$\\mathbf N = 1 * prob(n=1) + 2 * prob(n=2) + 3 * prob(n=3) + 4 * prob(n=4) + ... $\n",
    "\n",
    "\n",
    "First, how can we get to the probability of failing in the first trial?  That would be the case if our map said we were off the Blue teritory at time 1, but there at time 0.  And what about the probability of failing in the second trial (i.e. prob(n=2)? That would occur if we were off the blue map at time 2, but there at time 1.  And for the probability of failing in the third trial (i.e. prob(n=3)? That would occur if we were off the blue map at time 3, but there at time 2.  This line of reasoning continues for the probability of being absorbed at time 4, 5, 6.... \n",
    "\n",
    "We can write this out as \n",
    "\n",
    "$\\mathbf {N1} = 1 * prob(n=1) + 2 * prob(n=2) + 3 * prob(n=3) + 4 * prob(n=4) + ... $\n",
    "\n",
    "$\\mathbf {N1} = 1 * (\\mathbf {Blue}^0 - \\mathbf {Blue}^1)\\mathbf 1 + 2 * (\\mathbf {Blue}^1 - \\mathbf {Blue}^2)\\mathbf 1 + 3 * (\\mathbf {Blue}^2 - \\mathbf {Blue}^3)\\mathbf 1 + 4 * (\\mathbf {Blue}^3 - \\mathbf {Blue}^4)\\mathbf 1 + 5*(\\mathbf {Blue}^4  ... $\n",
    "\n",
    "- - - -\n",
    "*However, in the interest of readability, we will drop the $\\mathbf 1$ for now.  We could easily factor it out from the line below, and the line below that and so on, but it becomes tedious and less readible without adding any offseting value.  note: that (where i is a scalar and h is a ones vector) * $\\mathbf m = \\mathbf N\\mathbf 1 = E[n] = \\big(\\sum_{i = 1}^{\\infty} i(\\mathbf {Blue}^{i-1} - \\mathbf {Blue}^i)\\mathbf 1 \\big) = \\big(\\sum_{i = 1}^{\\infty} i(\\mathbf{Blue}^{i-1} - \\mathbf {Blue}^i) \\big)\\mathbf 1$ \n",
    "\n",
    "*and similarly with our squared calculation *$ E[n^2] = \\big(\\sum_{i = 1}^{\\infty} i^2(\\mathbf {Blue}^{i-1} - \\mathbf {Blue}^i)\\mathbf 1 \\big) = \\big(\\sum_{i = 1}^{\\infty} i^2(\\mathbf {Blue}^{i-1} - \\mathbf {Blue}^i) \\big)\\mathbf 1$ \n",
    "\n",
    "*Accordingly we'll drop the ones vector for now, and re-introduce it at during the final steps of calculating any expected values.*\n",
    "\n",
    "- - -- \n",
    "$\\mathbf{N} = 1 * (\\mathbf {Blue}^0 - \\mathbf {Blue}^1) + 2 * (\\mathbf {Blue}^1 - \\mathbf {Blue}^2) + 3 * (\\mathbf {Blue}^2 - \\mathbf {Blue}^3) + 4 * (\\mathbf {Blue}^3 - \\mathbf {Blue}^4) + 5(\\mathbf {Blue}^4  ... $\n",
    "\n",
    "\n",
    "\n",
    "$\\mathbf N = \\mathbf {Blue}^0 - \\mathbf {Blue}^1 + 2\\mathbf {Blue}^1 - 2\\mathbf {Blue}^2 + 3\\mathbf {Blue}^2 - 3\\mathbf {Blue}^3 + 4\\mathbf {Blue}^3 - 4\\mathbf {Blue}^4 + 5\\mathbf {Blue}^4 + ... $\n",
    "\n",
    "if we simplify the above what we get is:\n",
    "\n",
    "$\\mathbf N = \\mathbf {Blue}^0 + \\mathbf {Blue}^1 + \\mathbf {Blue}^2 + \\mathbf {Blue}^3 + \\mathbf {Blue}^4 + ... $\n",
    "\n",
    "which is the same thing as our original equation for the expected time until absorbtion (where $\\mathbf m = \\mathbf{N1}$ i.e. $\\mathbf m$ = the sum accross the column space of $\\mathbf N$).\n",
    "\n",
    "\n",
    "Now let's take this approach and apply it to our $E[n^2]$ calculation.  Recall that the only difference between these calculations is that, for $E[n]$ we multiply each probability by $n$, but for $E[n^2]$ we mutiply each probability by $n^2$.\n",
    "\n",
    "Let's denote $E[n^2] = \\mathbf{L1}$ (as we are close to running out of letters in the alphabet!).  As mentioned above, we'll drop the h for now, but promise to incorporate it later at the appropriate stage.  \n",
    "\n",
    "$\\mathbf L = 1^2 (\\mathbf {Blue}^0 - \\mathbf {Blue}^1) + 2^2 (\\mathbf {Blue}^1 - \\mathbf {Blue}^2) + 3^2 (\\mathbf {Blue}^2 - \\mathbf {Blue}^3) + 4^2 (\\mathbf {Blue}^3 - \\mathbf {Blue}^4) + 5^2(\\mathbf {Blue}^4  ... $\n",
    "\n",
    "$\\mathbf L = 1(\\mathbf {Blue}^0 - \\mathbf {Blue}^1) + 4(\\mathbf {Blue}^1 - \\mathbf {Blue}^2) + 9(\\mathbf {Blue}^2 - \\mathbf {Blue}^3) + 16(\\mathbf {Blue}^3 - \\mathbf {Blue}^4) + 25(\\mathbf {Blue}^4  ... $\n",
    "\n",
    "if we distribute the scalars we get\n",
    "\n",
    "$\\mathbf L = 1 \\mathbf {Blue}^0 - 1\\mathbf {Blue}^1 + 4\\mathbf {Blue}^1 - 4\\mathbf {Blue}^2 + 9\\mathbf {Blue}^2 - 9\\mathbf {Blue}^3 + 16\\mathbf {Blue}^3 - 16\\mathbf {Blue}^4 + 25\\mathbf {Blue}^4  ... $\n",
    "\n",
    "this simplifies to:\n",
    "\n",
    "$\\mathbf L = 1\\mathbf {Blue}^0 + 3\\mathbf {Blue}^1 + 5\\mathbf {Blue}^2 + 7\\mathbf {Blue}^3 + 9\\mathbf {Blue}^4  ... $\n",
    "\n",
    "- - - -\n",
    "*Technical note: \n",
    "With respect to the scalar coefficients for any given $\\mathbf {Blue}^i$ above (except the single case of the Identity Matrix), what you have is a negative coefficient at time i and a positive coefficient at i + 1.  Because we square the coefficient value (before applying the negative or positive sign) the value for the negative coefficient is $i^2$, while the positive coefficient is* $(i+1)^2 = i^2 + 2i + 1$ *Thus when we add the two terms we get* $(i+1)^2 - i^2 = (i^2 + 2i + 1) - i^2 = 2i + 1$. *Incidentally, this equation also holds for the base case of one Identity Matrix, aka * $\\mathbf {Blue}^0$\n",
    "\n",
    "It is also (perhaps) worth noting that the largest eigenvalue for $\\mathbf {Blue}$ has magnitude less than 1 (as the absorbing state(s) have the largest magnitude eigenvalue of 1), and hence the above infinite series, in effect has an arithmetic progression along with a geometric decline -- which is another way of saying that it is in the radius of convergence associated with a geometric series.  \n",
    "- - - -\n",
    "**(Subsequent note: there is a section tacked on at the end, \"A Much Simpler...\", that derives these results in a better mannner.  Nevertheless, what is shown below is your author's original take on the problem.  In general having multiple ways of solving a problem can be quite helpful so the original approaches below have been left in.)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our equation is thus\n",
    "\n",
    "$\\mathbf {L} = 1\\mathbf {Blue}^0 + 3\\mathbf {Blue}^1 + 5\\mathbf {Blue}^2 + 7\\mathbf {Blue}^3 + 9\\mathbf {Blue}^4 + ... $\n",
    "\n",
    "This is easy to tackle if we do a small decomposition.\n",
    "\n",
    "Let $L = Line_{0} + 2Line_{1} + 2Line_{2} + 2Line_{3} + 2Line_{4} + ...$\n",
    "\n",
    "Now the values of these lines are shown below\n",
    "\n",
    "\n",
    "$\\left.\\begin{matrix}\n",
    "1Line_{0}= & 1\\mathbf {Blue}^0 +&  1\\mathbf {Blue}^1 +& 1\\mathbf {Blue}^2 + & 1\\mathbf {Blue}^3+ & 1\\mathbf {Blue}^4+ & ... \\\\ \n",
    "2Line_{1}= & 0 + &  2\\mathbf {Blue}^1 + & 2\\mathbf {Blue}^2 + & 2\\mathbf {Blue}^3 + & 2\\mathbf {Blue}^4 + & ...\\\\\n",
    "2Line_{2}= & 0+ &  0+ & 2\\mathbf {Blue}^2 + & 2\\mathbf {Blue}^3+ & 2\\mathbf {Blue}^4+ & ... \\\\\n",
    "2Line_{3}= & 0+ &  0+ & 0 + & 2\\mathbf {Blue}^3+ & 2\\mathbf {Blue}^4+ & ...\\\\\n",
    "2Line_{4}= & 0+ &  0+ & 0 + & 0+ & 2\\mathbf {Blue}^4+ & ...\\\\\n",
    "\\vdots = & \\vdots+ &  \\vdots+ & \\vdots + & \\vdots+ & \\ddots + & \\ddots\\\\\n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "In general, for natural numbers i = 0, 1, 2, 3, 4...:\n",
    "\n",
    "$Line_i = \\sum_{k = i}^{\\infty} \\mathbf {Blue}^k$\n",
    "\n",
    "to be clear: all lines have a scalar coefficient of 2, except $Line_0$\n",
    "\n",
    "\n",
    "Notice that for any arbitrary line number, i, one can construct such an upper triangular matrix with the $Line_{i}$ being in the bottom row (much as $Line_{4}$ is above).  Since we start counting at zero, such a triangular matrix will have i + 1 rows and i + 1 columns. And the final column on the right will have i + 1 entries in it of $\\mathbf {Blue}^i$, with the top entry having an associated scalar of 1 and the other i entries having a scalar of 2.  Thus if we sum up each entry in the ith column we find that the aggregate scalar value for $\\mathbf {Blue}^i$ must be $1(1)+ 2(i + 1 - 1) = 2i + 1$, which conforms to our original equation for L.  \n",
    "\n",
    "\n",
    "- - - \n",
    "\n",
    "We'll use this new definition of L, As it is, every line on the right hand side has a scalar value of 2, except $Line_{0}$,  we can balance this out by adding $Line_{0}$ to both sides.  Thus we have:\n",
    "\n",
    "$L + Line_{0} = 2*Line_{0} + 2*Line_{1} + 2*Line_{2} + 2*Line_{3} + 2*Line_{4} + ...$\n",
    "\n",
    "Then we can divide both sides by the scalar value of 2, and we get:\n",
    "\n",
    "$\\frac{1}{2}(L + Line_{0}) = Line_{0} + Line_{1} + Line_{2} + Line_{3} + Line_{4} + ...$\n",
    "\n",
    "\n",
    "Now moments ago, we stated:\n",
    "\n",
    "$Line_i = \\sum_{k = i}^{\\infty} \\mathbf {Blue}^k$\n",
    "\n",
    "which is the same thing as writing:\n",
    "\n",
    "$Line_i = \\Big(\\sum_{k = i}^{\\infty} \\mathbf {Blue}^k \\Big)$\n",
    "\n",
    "we can rewrite this as:\n",
    "\n",
    "$Line_i = \\Big(\\sum_{k = i}^{\\infty} \\mathbf {Blue}^{k-i} \\mathbf {Blue}^{i}\\Big) $\n",
    "\n",
    "we can actually pull $\\mathbf {Blue}^{i}$ outside of the summation as shown below:\n",
    "\n",
    "$Line_i = \\Big(\\sum_{k = i}^{\\infty} \\mathbf {Blue}^{k-i} \\Big) \\mathbf {Blue}^{i}$\n",
    "\n",
    "and further simplify this to:\n",
    "\n",
    "$Line_i = \\Big(\\sum_{k = 0}^{\\infty} \\mathbf {Blue}^{k} \\Big) \\mathbf {Blue}^{i}$\n",
    "\n",
    "recall from the beginning of this post:\n",
    "\n",
    "$\\mathbf {N} = \\sum_{k = 0}^{\\infty} \\mathbf {Blue}^k$\n",
    "\n",
    "Thus we can make the substitution \n",
    "\n",
    "$Line_i = \\big(N \\big) \\mathbf {Blue}^{i} = \\mathbf{N} \\mathbf {Blue}^{i}$\n",
    "- - - -\n",
    "In case the above summation notation seems opaque, here are a couple of examples using '...' notation form.\n",
    "\n",
    "For $Line_{0}$:\n",
    "\n",
    "$Line_{0} = \\mathbf {Blue}^0 + \\mathbf {Blue}^1 + \\mathbf {Blue}^2 + \\mathbf {Blue}^3 + \\mathbf {Blue}^4 + ... $\n",
    "\n",
    "we can re-write this as $ Line_{0} = (\\mathbf {Blue}^0 + \\mathbf {Blue}^1 + \\mathbf {Blue}^2 + \\mathbf {Blue}^3 + \\mathbf {Blue}^4 + ... )\\mathbf {Blue}^0 = (\\mathbf N)\\mathbf {Blue}^0 = (\\mathbf N)\\mathbf I = \\mathbf N $\n",
    "\n",
    "For another example, let's look at $Line_{3}$, we can rewrite this as:\n",
    "\n",
    "$Line_{3} = \\mathbf {Blue}^3 + \\mathbf {Blue}^4 + \\mathbf {Blue}^5 + \\mathbf {Blue}^6 + \\mathbf {Blue}^7...  $ $= (\\mathbf {Blue}^0 + \\mathbf {Blue}^1 + \\mathbf {Blue}^2 + \\mathbf {Blue}^3 + \\mathbf {Blue}^4 + ... )\\mathbf {Blue}^3 = (\\mathbf N)\\mathbf {Blue}^3= \\mathbf{N Blue}^3$\n",
    "- - - -\n",
    "So Let's go back to our core equation for L:\n",
    "\n",
    "$\\frac{1}{2}(L + Line_{0}) = Line_{0} + Line_{1} + Line_{2} + Line_{3} + Line_{4} + ...$\n",
    "\n",
    "and make substitutions\n",
    "\n",
    "\n",
    "$\\frac{1}{2}(\\mathbf L + (\\mathbf N)) = (\\mathbf{NBlue}^0) + (\\mathbf{NBlue}^1) + (\\mathbf {NBlue}^2) + (\\mathbf{NBlue}^3) + (\\mathbf{NBlue}^4) +  ... $\n",
    "\n",
    "factor out the common $\\mathbf {N}$ on the right side of the equation and we get\n",
    "\n",
    "$\\frac{1}{2}(\\mathbf{L + N}) = \\mathbf N(\\mathbf {Blue}^0 + \\mathbf {Blue}^1 + \\mathbf {Blue}^2 + \\mathbf {Blue}^3 + \\mathbf {Blue}^4 + ... $\n",
    "\n",
    "The terms inside the parenthesis on the right looks very familiar.... \n",
    "Indeed we know that series itself sums to N.  We can make a substitution, accordingly.\n",
    "\n",
    "$\\frac{1}{2}(\\mathbf{L + N}) = \\mathbf {NN}$\n",
    "\n",
    "$\\frac{1}{2}(\\mathbf {L + N}) = \\mathbf N^2$\n",
    "\n",
    "$\\mathbf {L + N} = 2\\mathbf N^2$\n",
    "\n",
    "$\\mathbf L = 2\\mathbf N^2 - \\mathbf N$\n",
    "\n",
    "phew! that was a lot of work.  We are almost done with the variance calculation.  \n",
    "\n",
    "so we just need to add up all of these squared values...\n",
    "\n",
    "recall that we said \n",
    "$\\mathbf 1 = \\begin{bmatrix}\n",
    "1\\\\ \n",
    "1\\\\ \n",
    "1\\\\ \n",
    "1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "so $E[n^2] = \\mathbf{L1} = (2\\mathbf N^2 - \\mathbf N)\\mathbf1= (2\\mathbf{NN1} - \\mathbf{N1})$\n",
    "\n",
    "now recall that we said \n",
    "\n",
    "$E[n] = \\mathbf m = \\mathbf{N1}$\n",
    "\n",
    "where \n",
    "\n",
    "$\\mathbf{N1} = \\mathbf m = \\begin{bmatrix} m_T\\\\\n",
    "m_U\\\\ \n",
    "m_V\\\\ \n",
    "m_W\n",
    "\\end{bmatrix} $ and in example this is $\\approx \\begin{bmatrix} 4.308\\\\\n",
    "2.880\\\\ \n",
    "2.051\\\\ \n",
    "3.585\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "so we can make further substitutions here:\n",
    "\n",
    "$E[n^2] = 2\\mathbf{NN1} - \\mathbf{N1} = 2\\mathbf{N(N1)} - (\\mathbf{N1}) = 2\\mathbf{N(m)} - (\\mathbf m) = 2\\mathbf{Nm - m}$\n",
    "\n",
    "\n",
    "where $VarianceVector = E[n^2] - E[n]E[n]= (2\\mathbf{Nm - m}) - \\mathbf m*\\mathbf m$\n",
    "\n",
    "But what does $\\mathbf m*\\mathbf m$ actually mean?  It is easy to get lost in the mechanics of Linear Algebra.  It is better to first start with our knowledge of probability theory and our goals.  Our goal is to calculate variance and use a basic formula that applies to individual scalars in general.  Our formula is: $var(X) = E[X^2] - E[X]E[X]$.  The fact is, we have computed all of these things already for many different scalar values for all relevant starting states and we made use of vectors as containers in the process. So we have done the hard work already and have a container that has a scalar value corresponding to $E[X^2]$ for each and every nontrivial starting state.  (And for the trivial cases we know variance is zero.)  We also have $E[X]$ for each and every starting state.  \n",
    "\n",
    "So to take a step back: if we were just interested in knowing the variance in time until absorbtion, given starting state T, we would look back to our map (and the blue partition in particular) and see that T is on the top row.\n",
    "\n",
    "![example markov chain](illustrations/AT_partioned__.png)\n",
    "\n",
    "Then would look in the vector $(2\\mathbf{Nm - m})$ and grab the top entry.  That corresponds to the $E[X^2]$ value for starting at T.  From this we'd subtract $m_T^2$.  And the difference would the be the variance in time until absorbtion, given a starting state = T.  \n",
    "\n",
    "Similarly, if we were interested only in variance of time until absorbtion, given a start in U, we would look at our blue map, see U is in the second to top row, then look in the vector $(2\\mathbf{Nm - m})$ and grab the second to top entry and from this subtract $m_U^2$. That gives us our the variance in time until absorbtion, given a start in U. And this process could be applied by looking in the 3rd to top row for V, and 4th to top row for W... \n",
    "\n",
    "So when we wrote down $m*m$ what we were actually interested in doing is computing a vector $= \\begin{bmatrix} m_T\\cdot m_T\\\\\n",
    "m_U\\cdot m_U\\\\ \n",
    "m_V\\cdot m_V\\\\ \n",
    "m_W\\cdot m_W\n",
    "\\end{bmatrix} = \\begin{bmatrix} m_T^2\\\\\n",
    "m_U^2\\\\\n",
    "m_V^2\\\\ \n",
    "m_W^2\\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Put differently we were interested in doing element-wise multiplication within the vector.  This is *not* the canonical type of matrix multiplication or vector multiplication.  It goes by the name of Hadamard product though it can be thought of as a diagonal matrix with the m vector along the diagonal, and then squaring that diagonal matrix, as discussed in the technical note below.\n",
    "\n",
    "*techincal note:* we know that $E[n] = \\mathbf m$, but under the traditional definition of matrix multiplication (and where a vector is interpretted as a 1 column matrix)* $\\mathbf m * \\mathbf m$* would not be an allowed operation, as the k inner lenghts do not line up.  I.e. one could mutliply* $\\mathbf m^{T}\\mathbf m$ or $\\mathbf {mm}^{T}$ though not  $mm$.  However these allowed operations do not make sense given our underlying understanding of the definition of variance.  What we really want to do here is square every element inside the vector. There are two ways to effect this.  The first is by representing this with: $\\mathbf m \\circ \\mathbf m$ where* $\\circ$ represents the Hadamard  product (i.e. element wise mutliplication) between these two vectors.  An alternative representation is to let $\\mathbf D = diag(\\mathbf m)$.\n",
    "\n",
    "*It is a matter of personal preference as to how to represent this.  For computational purposes, using the Hadamard  product is the most straightforward.*\n",
    "\n",
    "So we can then re-write our equation as:\n",
    "\n",
    "$VarianceVector = 2\\mathbf{Nm - m - D}^2 \\mathbf1$\n",
    "\n",
    "where \n",
    "\n",
    "$\\mathbf D = diag(\\mathbf m) = \\begin{pmatrix}\n",
    "m_T & 0 & 0 & 0\\\\ \n",
    "0 & m_U &  0& 0\\\\ \n",
    "0 & 0 &  m_V& 0\\\\ \n",
    "0 & 0 &  0& m_W\n",
    "\\end{pmatrix}$\n",
    "\n",
    "*note that any diagonal matrix times a diagonal matrix is itself a diagonal matrix, and the ones vector, h, 'extracts' the non-zero elements of any diagonal matrix (i.e. elements along the diagonal) and puts them in a vector, thus* $\\mathbf{D1} = \\mathbf m$ and $\\mathbf D^2\\mathbf h = \\mathbf m \\circ \\mathbf m  $.  *Technically there are nuances around using non-square diagonal matrices, but this post only needs to consider the nice form of a diagonal matrix -- i.e. a square matrix with all non diagonal entries equal to 0.*\n",
    "\n",
    "\n",
    "And this is our answer\n",
    "\n",
    "$VarianceVector = 2\\mathbf{Nm - m - D}^2 \\mathbf 1 = 2\\mathbf{Nm - m - (m \\circ m)}$\n",
    "\n",
    "- - - -\n",
    "\n",
    "From a linear algebra perspective we are done.  \n",
    "\n",
    "However, if, as part of our computations, we did not want to explicitly invert $\\mathbf N^{-1}$, then we could a small bit more algebra to get the following:\n",
    "\n",
    "$VarianceVector = 2\\mathbf{Nm  + (- m - D^2 1)}$\n",
    "\n",
    "$VarianceVector = part1_{vector} + part2_{vector}$ i.e. we can split the VarianceVector into two separate parts, where \n",
    "\n",
    "$part1_{vector} = 2 \\mathbf{Nm}$\n",
    "\n",
    "and $part2_{vector} = \\mathbf{- m - D}^2 \\mathbf 1 = -\\mathbf{m - (m \\circ m)}$\n",
    "\n",
    "$part2_{vector}$ is thus easy to calculate.  \n",
    "\n",
    "for $part1_{vector}$, we can multiply both sides by $\\mathbf{N}^{-1}$\n",
    "\n",
    "$\\mathbf N^{-1}part1_{vector} = 2 \\mathbf m$\n",
    "\n",
    "which is the same as solving a system of linear equations, except the right hand side is now $\\mathbf m$ --not $\\mathbf 1$ or $\\mathbf R$, as it was in the original posting here on expected values.\n",
    "\n",
    "Hence we solve for $part1_{vector}$ and add $\\big(- \\mathbf{m - (m \\circ m)}\\big)$ to this to get our VarianceVector.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for some thoughts on expected values and variances with a mixed starting state:\n",
    "\n",
    "If instead of starting at state T with 100% certainty, what if we started at state T with 55% confidence and state U with 45% confidence?  (Alternatively put, what if we on average were at state T 55/100 times and state U 45/100 times, when this process starts?)  In order to solve for this, we use the Law of Total Expectation (or Law of Iterated Expecations) and the Law of Total Variance.  \n",
    "\n",
    "![markov chains 3 parts](illustrations/markov_3_map.png)\n",
    "# the above, bottom right picture should be updated to use a bold one, not an h to denote the ones vector\n",
    "\n",
    "The Law of Iterated Expectations tells us that we can simply look at the above and compute the expected value of such a situation as being $ \\approx 0.55 * 4.3083 + 0.45* 2.8797 \\approx 3.6654$.  \n",
    "\n",
    "And what if we had 30% confidence we were on T and 70% confidence we were on U?  Then the expected time until absorbtion $\\approx  0.3 * 4.3083 + 0.7 * 2.8797 \\approx 3.3083$. This number appears to be expected value given a start at T, but minus one, hmmm... \n",
    "\n",
    "Now what if we were 60% confident that we were in T and 40% confident that we were in Y?  Then our expected time until absorbtion $\\approx 0.6*4.3083 + 0.4 *0 \\approx 2.585$ which appears to be exactly one less than the expected value if we started at W.  \n",
    "\n",
    "These are not coincidences. Indeed if we look back at our map (using the Blue and Red portions above or the graphical representation), we see that if we start at W, then on average, we will be 60% in T and 40% in (absorbing state) Y after one turn.  Thus the 0.6 T, 0.4 Y splits are the longrun equivalent to the position of having started at W and now being at time + 1.  \n",
    "\n",
    "If we look at the below side by side distributions -- either the Probability Mass Functions (PMFs), or the Cumulative Distribution Functions (CDFs),  we notice that they are identical except the second one (Start One Turn After W) has been shifted to the left by one.  That is, if one starts at T, U, V, or W, the first time some portion is absorbed must be at a step $\\geq 1$ -- i.e. nothing interesting happens at time zero..  If we start looking at time 1,  then all of the same 'interesting' things happen, just one step sooner -- which is another way of saying that the second distribution has been shifted one step to the left.  \n",
    "\n",
    "\n",
    "![distribution comparison](illustrations/markov_dist_comparison.png)\n",
    "\n",
    "*technical note: the probability of being 'fully' absorbed goes to 1 in the limit of $\\lim_{n \\to \\infty}$.  When using iterative methods we naturally run for only a finite number of steps (akin to truncating an infinite series).  However the process moves very quickly.  Discussions of convergence have many different tools that may be used including Chebyshev's inequality, or use of the largest eigenvalue less than one.  Your author has determined that ~ 50 iterations is enough for this example.*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "So when we look at these left hand side vs right hand side  absorbtion probability distributions , it makes sense that they have identical means, except the one on the right is $mean_{left} - 1$.  The  more subtle point is that the variance is the same in both cases.  Since left vs right the distributions above are identical except one has been shifted, then the dispersion around the mean value is identical in both cases.  And that is all a variance calculation is: a dispersion around the mean calculation using, essentially, a length norm 2 (i.e. squared distance measure).  \n",
    "\n",
    "Note that you can verify this from defintions of variance.  Alternatively we can check this using the Law of Total Variance.  \n",
    "\n",
    "Indeed the Law of Total Variance allows us calcuate the variance from a mixture of any and all starting states (though I will note that we don't need it for a starting state that is that only has a degenerate form of randomness afterwards, including all absorbing states, since those definitionally have zero variance in absorbtion time).\n",
    "\n",
    "The Law of Total Variance:\n",
    "\n",
    "$TotalVariance = ExpectedVariance + VarianceOfExpectations$\n",
    "\n",
    "So if the reader was interested in finding the variance in time until absorbtion when we were 25% confident we are in T, 25% confident in U, 25% confident in V, and 25% confident in W, we could solve for this using the Law of Total Variance.  \n",
    "\n",
    "\n",
    "\n",
    "**Alternative Derivation of VarianceVector using Law of Total Variance**\n",
    "\n",
    "# Note: This section was written a while back using somewhat different notation.  It serves as a rather unpleasant second derivation of variance -- the slightly different notation (esp. use of h for ones vector, and non bolded matrices and vectors) has not been updated.  This section is of interest as a way to check the work above, but other than that, it may be safely skipped.  If you do skip it, go straight to the section titled \"Conclusion\"\n",
    "\n",
    "What is interesting is that if we were unable to derive $VarianceVector$ at the beginning of this article, we could exploit the Law of Total Variance evaluated at time = 1, in order to solve for the $VarianceVector$.  Your author found this approach to be a bit tedious, and unfortunately it involves manipulating quite a few symbols, though a sketch of the derivation will be provided.  \n",
    "\n",
    "If we looked at this say row W at a time zero, for $Variance(start = W)$\n",
    "\n",
    "recalling our defintion of the Law of Total Variance: $TotalVariance = ExpectedVariance + VarianceOfExpectations$\n",
    "\n",
    "\n",
    "The equation would be \n",
    "\n",
    "$TotalVariance(start = W) = \\big(1*TotalVariance(start = W) \\big) + \\big(1 * (m_W - m_W)^2 \\big)$\n",
    "\n",
    "$TotalVariance(start = W) = \\big(TotalVariance(start = W) \\big) + \\big(1 * (0)^2 \\big)$\n",
    "\n",
    "$TotalVariance(start = W) = TotalVariance(start = W) + 0$\n",
    "\n",
    "thus \n",
    "\n",
    "$TotalVariance(start = W) = TotalVariance(start = W)$\n",
    "\n",
    "which is not very helpful.  \n",
    "\n",
    "\n",
    "However when we looked at the side by side distributions, we observed that Variance for W is the same at time 0 and time 1.... let's substitute in on the right hand side the variance calculation at time 1, again using the Law of Total Variance\n",
    "\n",
    "$TotalVariance(start = W) = \\big(0.6*TotalVariance(start = T) + 0.4*(TotalVariance(start = Y)  \\big) + VarianceOfExpectations$\n",
    "\n",
    "we know that total variance for starting at  Y is zero, so we can remove it from the equation. Then we can simplify this to:\n",
    "\n",
    "$-0.6*TotalVariance(start = T) + 1 * TotalVariance(start = W) =  VarianceOfExpectations$\n",
    "\n",
    "If we repeat this for T, U, V, we get:\n",
    "\n",
    "$N^{-1}VarianceVector = VarianceOfExpectationsVector$\n",
    "\n",
    "**Unfortunately, working through the $VarianceOfExpectationsVector$ involves a somewhat tedious number of symbol manipulations.  A walkthrough is outlined below.  If you are not interested in such a derivation, consider this the end of the post!**\n",
    "\n",
    "- - - \n",
    "First, here is a quick overview of symbols that will be used for the rest of the article.  Note: everything below is substantially the same as before, however some tweaks had to be made.  There is no point in solving for the variance given a start at time Y or variance given a start at time Z -- we know that in both of those cases the variance is zero because they are absorbing states. However, if we work through the terms in VarianceOfExpectations we at least initially need some references to Y and Z in certain vectors.  Thus what we are left with is below.  \n",
    "\n",
    "![final symbols review](illustrations/markov_final_symbols_review2.png)\n",
    "\n",
    "In short, $A^{T}$ had the green and yellow portions sliced off, we have our existing ones vector, h, but there is a new longer ones vector, q, that is just h with two more ones tacked on (i.e. to accomdate a larger mutliplying matrix that contemplates T, U, V, W, Y, Z), and similarly our mean vector m is the same, but we also needed to put in place vector p, which is m, along with the means of Y and Z tacked on (which are of course both zero).  Finally, because this approach involves dealing with the Variance of Expectations, there are a lot of squared terms that must be dealth with.\n",
    "- - -\n",
    "\n",
    "$VarianceOfExpectationsVector$ has 4 rows, just like $N^{-1}$ and $VarianceVector$.  We approach $VarianceOfExpectationsVector$ by examining some arbitrary row, i, in it.  (As a review: looking back at our map, we see that T is in row 0, U is in row 1 and so on -- thus $a_3^{T}$ refers to the bottom row of our blue and red map in $A^T$ and $m_3$ refers to the scalar value in the bottom slot of m, aka $m_W$.  Also note that there we have $\\beta$ states in total (where $\\beta = 6$ in our example) )\n",
    "\n",
    "Thus for the value of the scalar in the ith row of $VarianceOfExpectationsVector$, we have:\n",
    "\n",
    "$VarianceOfExpectationsVector_i = \\sum_{j = 0}^{\\beta - 1} (p_j - (p_i - 1))^2 * ProbabilityOfGoingInOneStep_{fromState_i->State_j}$\n",
    "\n",
    "or a touch more simply:\n",
    "$VarianceOfExpectationsVector_i = \\sum_{j = 0}^{\\beta - 1} (p_j - m_i - 1)^2 * ProbabilityOfGoingInOneStep_{fromState_i->State_j}$\n",
    "\n",
    "\n",
    "We can rewrite the above expression using vector operations: \n",
    "\n",
    "$VarianceOfExpectationsVector_i = a_i^{T}\\big([p - (m_i - 1)q\\big) \\circ ([p - (m_i - 1)q\\big)]$\n",
    "\n",
    "\n",
    "\n",
    "note the squaring operation in the center as denoted by $\\circ$ which stands for the Hadamard product.  We need to be careful with this and evaluate it first. Thus our arbitrary row becomes:\n",
    "\n",
    "$ = a_i^{T}\\big(r - (2m_i)p + 2p + (m_i)^2q - (2m_i)q + q \\big)$\n",
    "\n",
    "Then distribute $a_i^{T}$ to all terms inside the parenthesis:\n",
    "\n",
    "$ = a_i^{T}r - (2m_i)a_i^{T}p + 2a_i^{T}p + (m_i)^2a_i^{T}q - (2m_i)a_i^{T}q + a_i^{T}q$\n",
    "\n",
    "This is all looks rather awful, but there is hope! There are three key subsitutions we may make.\n",
    "\n",
    "\n",
    "$1)$ $ a_i^{T}q = 1$ \n",
    "\n",
    "**Justification**: That scalar value of 1 results whenever we compute a dot product involving $a_i^{T}$ and a ones vector. Why? Because all probabilites in a given row of $A^T$ must sum to a scalar value of one, as outlined in the original post on absorbing state markov chains.  More simplistically: all 'outbound' probabilities must sum to one from a given starting state.\n",
    "\n",
    "$2)$ $a_i^{T}p =  m_i - 1$\n",
    "\n",
    "**Justification**: The above can be restated as $m_i - a_i^{T}p  = 1$.  As stated when we discussed the side by side probability mass functions of time until absorbtion, the expected time until absorbtion is the same at time zero as it is at time one, except the expected value is exactly 1 larger when at time zero.  \n",
    "\n",
    "Alternatively, if we go back to the original article on absorbing state markov chains, it said:\n",
    "\n",
    "> Finally for W we could write is as $m_W = 1 + 0.6 m_T$\n",
    "> re arrange terms and we get: $- 0.6 m_T + 0 m_U + 0 m_V + 1m_W = 1$\n",
    "\n",
    "That second line is identical to saying: $ m_3 - a_3^{T}p = 1$ and this holds for any arbitrary row i in $A^T$.  (For the avoidance of doubt: note: that this is not relevant for the trivial case of starting in an absorbing state --- we know that expected time until absorbtion is zero there).  Also note that we can say: $DotProduct(blue_i, m) = m_i - 1$\n",
    "\n",
    "$3)$ $a_i^{T}r = DotProduct(Blue_i, k)$\n",
    "\n",
    "**Justification**: the transition probabilies in the red partition of $A^T$ are always mutliplied by scalar values of zero in r.  This must be true as when you look on the red portion of your map, you are looking at the probability of entering absorbing states, and the expected time till absorbtion from an absorbing state is zero.   Thus you are left with zero plus the dot product operation of the ith row vector in blue, and k.  As in the other 2 rules, this returns a scalar result.  \n",
    "\n",
    "Thus our $row_i$ \n",
    "$= DotProduct(Blue_i, k) - (2m_i)(m_i - 1) + 2(m_i - 1) + (m_i)^2(1) - (2m_i)(1) + 1$\n",
    "\n",
    "$= DotProduct(Blue_i, k) -2(m_i)^2 + 2m_i + 2m_i - 2 + (m_i)^2 - 2m_i + 1 = DotProduct(Blue_i, k) -2(m_i)^2 + (m_i)^2 + 2m_i + 2m_i - 2m_i - 2 + 1$\n",
    "\n",
    "$= DotProduct(Blue_i, k) -(m_i^2) + 2m_i - 1$\n",
    "\n",
    "If we then step back from our arbitrary row i and look at all rows of VarianceOfExpectationsVector at once, we see:\n",
    "\n",
    "$N^{-1}VarianceVector = VarianceOfExpectationsVector = (Blue)k - k + 2m - h$\n",
    "\n",
    "\n",
    "$N^{-1}VarianceVector = (Blue)k - (I)k + 2m - h = (Blue - I)k + 2m - h = -(I - Blue)k + 2m - h$\n",
    "\n",
    "$N^{-1}VarianceVector = -(N^{-1})k + 2m - h$\n",
    "\n",
    "multiply both sides by $N$ on the left and get\n",
    "\n",
    "$NN^{-1}VarianceVector = -(NN^{-1})k + 2Nm - Nh$\n",
    "\n",
    "simplify and we get\n",
    "\n",
    "$VarianceVector = -k + 2Nm - (m) = -(m \\circ m) + 2Nm - m$\n",
    "\n",
    "which is equivalent to our earlier definition of $VarianceVector = 2Nm - m - (m \\circ m)$\n",
    "\n",
    "- - - - \n",
    "\n",
    "# Conclusion\n",
    "Both of these methods are valid ways of deriving the (time until absorbtion) variance vector for absorbing state markov chains, though the number of different symbols and terms that had to be manipulated in the second case, and different dimensions that needed to be managed, made the second derivation more tedious and error prone in your author's view. To derive the first approach one merely needed to understand that our transition matrix is an object called a map, to understand how to relate that to $variance(X) = E[X^2] - E[X]E[X]$, and to understand the underlying geometric series. (Note this first method also allows use to explicitly check for convergence issues with via use of eigenvalues and relating either a diagonalized form of our transition matrix to the geometric series, or if our matrix is defective, we could make the argument via an upperbound -- i.e. use a matrix arbitrarily close to ours but that decays even slower than ours, and demonstrate that we're within the radius of conververgence of a geometric series.  The second method is based on assuming convergence only.)\n",
    "\n",
    "The second approach required understanding the Law of Total Variance, manipulating quite a few different symbols of varying dimension in the general case of row i, and recognizing 3 subtle relations that allowed the inconvenient symbols to be reduced into more usable ones.  \n",
    "\n",
    "All that said, this is a tricky subject.  When trying to solve probability problems, it is great to be able to calculate things using two very different valid methods and get the same end result.  \n",
    "\n",
    "Finally, note that this writeup did not explicitly treat the case of variance in probability of landing in state Y vs state Z.  Why?  Because this is the probability of being absorbed in some state Y vs some some other absorbing states (in this case Z only) is a Bernouli probability. Thus if you know that the chances of being absorbed in Y is given by probability p, then you the variance of being absorbed in Y, vs not, is given by p(1-p).  \n",
    "\n",
    "- - - - \n",
    "\n",
    "# A Much Simpler (and Better) Look at the 2nd Moment\n",
    "\n",
    "At the beginning of the derivation we had:  \n",
    "\n",
    "$\\mathbf L = 1\\mathbf {Blue}^0 + 3\\mathbf {Blue}^1 + 5\\mathbf {Blue}^2 + 7\\mathbf {Blue}^3 + 9\\mathbf {Blue}^4  ...$  \n",
    "\n",
    "$\\mathbf L = \\Big(\\sum_{k=0}^\\infty \\big(2k + 1\\big)\\mathbf {Blue}^k\\Big) = \\Big(\\sum_{k=0}^\\infty 2k\\mathbf {Blue}^k\\Big) + \\Big(\\sum_{k=0}^\\infty \\mathbf {Blue}^k\\Big) = 2\\Big(\\sum_{k=0}^\\infty k\\mathbf {Blue}^k\\Big) + \\Big(\\mathbf N\\Big) $   \n",
    "\n",
    "- - - - - \n",
    "note: if we defined  \n",
    "$\\mathbf Y:= \\Big(\\sum_{k=0}^\\infty k\\mathbf {Blue}^k\\Big)$\n",
    "\n",
    "i.e. by trying to use the complementary CDF that underlies $\\mathbf {Blue}$ and weighting it by $k$ to in an attempt to get a second moment, we would have \n",
    "\n",
    "$\\mathbf L = 2\\mathbf Y + \\mathbf N$  \n",
    "\n",
    "or, when we collapse / sum this into an actual CDF with the use of the ones vector, we have  \n",
    "\n",
    "$\\mathbf L\\mathbf 1 = 2\\mathbf {Y1} + \\mathbf m$  \n",
    "\n",
    "which, for each row $i$ reads as:  \n",
    "\n",
    "$E\\big[X_i^2\\big] = 2\\big(\\sum_{k=0}^\\infty k Pr\\{X_i \\gt k\\}\\big) + \\mu_i$  \n",
    "\n",
    "if we were to reference the complementary CDF and try to use the second moment to get our result, we would see, using the derivation here:  \n",
    "\n",
    "https://stats.stackexchange.com/questions/143790/second-moment-from-survival-function\n",
    "\n",
    "(note: the below renders locally but seems to have problems rendering on github for unknown reasons)  \n",
    "\n",
    "$Z_i := X_i^2$  \n",
    "$E[Z_i] = \\int_0^\\infty Pr\\{Z_i > t\\} dt$  \n",
    "$\\text{    because integrating over the complementary CDF of real non-negative r.v. gives the first moment.  Now make the substitution.}$    \n",
    "$E[X_i^2] = \\int_0^\\infty Pr\\{X_i^2 > t\\} dt$   \n",
    "$= \\int_0^\\infty Pr\\{X_i > \\sqrt{t}\\}dt\\\\$  \n",
    "$= \\int_0^\\infty 2kPr\\{X_i > k\\}dk\\text{        i.e. substitute}~k^2 = t, ~~2y\\mathrm dy = \\mathrm {dk}$      \n",
    "$= 2\\int_0^\\infty k Pr\\{X_i > k\\}dk $    \n",
    "\n",
    "If we compare this integral over the complementary CDF vs the above summation, we see they are exactly the same in the discrete case, except we have a step function at each k, so for each each interval, e.g. $[0,1]$ or $[1,2]$ or $[2,3]$ or ..., we see that the integral form gives 'half credit' (i.e. half of a triangle in each interval) i.e. the integral amount is  $0.5$ in $[0,1]$, $ 1.5$ in $[1,2]$ , $ 2.5$ in $[2,3]$ and so on whereas in the summation form, the sum gives a payoff of $0$ in $[0,1]$, $ 1$ in $[1,2]$ , $ 2$ in $[2,3]$ and so on, thus the discrepancy in each case is exactly $\\frac{1}{2}$ with a weighting of $ Pr\\{X_i > k\\}$ for $k= \\{0, 1, 2, 3,...\\}$.  Two notes: (a) n actual sketch turns out to be quite helpful (vital?) here, and (b) it's a subtle almost nitpicky issue that comes up elsewhere, e.g. on p. 257 of Gallagher's Stochastic Processes when discussing limmitting ensemble averages for arithmetic renewal processes \n",
    "\n",
    "over the entire summation this gives \n",
    "\n",
    "$ \\sum_{k=0}^\\infty \\frac{1}{2} Pr\\{X_i > k\\} = \\frac{1}{2}\\sum_{k=0}^\\infty Pr\\{X_i > k\\} = \\frac{1}{2} \\mu_i$  \n",
    "\n",
    "i.e. recognizing that summing over the complementary probabilities corresponds to integrating over the complementary CDF which gives the first moment \n",
    "\n",
    "hence \n",
    "\n",
    "$E\\big[X_i^2\\big]=  2\\Big(\\int_0^\\infty k Pr\\{X_i > k\\}dk\\Big) = 2\\Big(\\big(\\sum_{k=0}^\\infty k Pr\\{X_i \\gt k\\}\\big) + \\frac{1}{2} \\mu_i \\Big)  = 2\\big(\\sum_{k=0}^\\infty k Pr\\{X_i \\gt k\\}\\big) + \\mu_i$   \n",
    "\n",
    "which agrees with what we derived above \n",
    "- - - - - \n",
    "returning to the problem, we have \n",
    "\n",
    "$\\mathbf L = \\Big(\\sum_{k=0}^\\infty \\big(2k + 1\\big)\\mathbf {Blue}^k\\Big) = \\Big(\\sum_{k=0}^\\infty 2k\\mathbf {Blue}^k\\Big) + \\Big(\\sum_{k=0}^\\infty \\mathbf {Blue}^k\\Big) = 2\\Big(\\sum_{k=0}^\\infty k\\mathbf {Blue}^k\\Big) + \\Big(\\mathbf N\\Big) $   \n",
    "\n",
    "$ = 2\\Big(\\big(\\sum_{k=0}^\\infty (k+1) \\mathbf {Blue}^k\\big) - \\big(\\sum_{k=0}^\\infty \\mathbf {Blue}^k\\big)\\Big) + \\mathbf N = 2\\big(\\sum_{k=0}^\\infty (k+1) \\mathbf {Blue}^k\\big) + -2\\big(\\sum_{k=0}^\\infty \\mathbf {Blue}^k\\big) + \\mathbf N $  \n",
    "\n",
    "$ = 2\\big(\\sum_{k=0}^\\infty (k+1) \\mathbf {Blue}^k\\big) + -2\\big(\\mathbf N \\big) + \\mathbf N =  2\\Big(\\sum_{k=0}^\\infty (k+1) \\mathbf {Blue}^k\\Big) - \\mathbf N = 2\\Big(\\mathbf N^2 \\Big) - \\mathbf N = 2\\mathbf N^2 - \\mathbf N$  \n",
    "\n",
    "For avoidance of doubt with respect to $\\mathbf N^2$ we recall that   \n",
    "\n",
    "$\\mathbf { N = I + Blue + Blue^2 + Blue^3 + Blue^4 + Blue^5 +...}= \\Big(\\sum_{k=0}^\\infty \\mathbf {Blue}^k\\Big)$ \n",
    "\n",
    "if we square the above, the claim is:   \n",
    "\n",
    "$\\mathbf { N}^2 =  \\Big(\\sum_{k=0}^\\infty \\mathbf {Blue}^k\\Big)\\Big(\\sum_{k=0}^\\infty \\mathbf {Blue}^k\\Big)  = \\mathbf I + 2\\mathbf {Blue} + 3 \\mathbf {Blue}^2 + 4\\mathbf {Blue}^3 + 5\\mathbf {Blue}^4 + 6\\mathbf {Blue}^5 + ... = \\Big(\\sum_{k=0}^\\infty (k+1) \\mathbf {Blue}^k\\Big)$    \n",
    "\n",
    "To clarify this relation, we can introduce artificial subscripts \n",
    "\n",
    "$= \\Big(\\sum_{k=0}^\\infty \\mathbf {Blue_a}^k\\Big)\\Big(\\sum_{k=0}^\\infty \\mathbf {Blue_b}^k\\Big)$  \n",
    "\n",
    "and in the resulting product we have the convolution, which states that the $i$ th term (where counting starts at one) is given by \n",
    "\n",
    "$\\text{ith term} = \\mathbf {Blue_a}^{0}\\mathbf {Blue_b}^{i-1}  + \\mathbf {Blue_a}^{1}\\mathbf {Blue_b}^{i-2} + \\mathbf {Blue_a}^{2}\\mathbf {Blue_b}^{i-3} + ... + \\mathbf {Blue_a}^{i-2}\\mathbf {Blue_b}^{1} + \\mathbf {Blue_a}^{i-1}\\mathbf {Blue_b}^{0}$  \n",
    "\n",
    "there are $\\text{i}$ terms in total and the exponents add in all case to $i-1$.  This gives us   \n",
    "\n",
    "$\\text{ith term} = i \\mathbf{Blue}^{i-1}$   \n",
    "\n",
    "hence the series is  \n",
    "\n",
    "$ \\sum_{i=1}^\\infty (i) \\mathbf {Blue}^{i-1}  = \\sum_{k=0}^\\infty (k+1) \\mathbf {Blue}^k = \\mathbf { N}^2$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below sections have code to do the above calculations. \n",
    "\n",
    "The code was originally done in Python 2.x, but subsequently revised to run in Python 3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3  0.   0.1  0.6  0.   0.  ]\n",
      " [0.7  0.   0.   0.   0.   0.  ]\n",
      " [0.   0.75 0.   0.   0.   0.  ]\n",
      " [0.   0.   0.3  0.   0.   0.  ]\n",
      " [0.   0.25 0.4  0.4  1.   0.  ]\n",
      " [0.   0.   0.2  0.   0.   1.  ]]\n",
      "\n",
      " [1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "A = np.zeros((6,6))\n",
    "ones_v = np.ones(6)\n",
    "\n",
    "A[0,0] = 0.3\n",
    "A[1,0] = 0.7\n",
    "A[2,1] = 0.75\n",
    "A[4,1] = 0.25\n",
    "A[4,1] = 0.25\n",
    "A[0,2] = 0.1\n",
    "A[3,2] = 0.3\n",
    "A[4,2] = 0.4\n",
    "A[5,2] = 0.2\n",
    "A[0,3] = 0.6\n",
    "A[4,3] = 0.4\n",
    "A[4,4] = 1\n",
    "A[5,5] = 1\n",
    "\n",
    "print(A)\n",
    "\n",
    "print(\"\\n\", ones_v @ A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.7  -0.7   0.    0.  ]\n",
      " [ 0.    1.   -0.75  0.  ]\n",
      " [-0.1   0.    1.   -0.3 ]\n",
      " [-0.6   0.    0.    1.  ]] \n",
      "\n",
      "(4, 4) the shape\n",
      "answer of E[T]\n",
      "[4.30831826 2.87974684 2.50632911 3.58499096]\n",
      "\n",
      " [[0.   0.  ]\n",
      " [0.25 0.  ]\n",
      " [0.4  0.2 ]\n",
      " [0.4  0.  ]] That's R \n",
      "\n",
      "for absorbtion destination, the solution is\n",
      "[[0.81012658 0.18987342]\n",
      " [0.81012658 0.18987342]\n",
      " [0.74683544 0.25316456]\n",
      " [0.88607595 0.11392405]]\n",
      "[[1.80831826 1.26582278 0.94936709 0.28481013]\n",
      " [0.37974684 1.26582278 0.94936709 0.28481013]\n",
      " [0.50632911 0.35443038 1.26582278 0.37974684]\n",
      " [1.08499096 0.75949367 0.56962025 1.17088608]]\n",
      "[4.30831826 2.87974684 2.50632911 3.58499096]\n",
      "[6.8031312  6.1908863  6.68409367 8.53666423]\n",
      " \n",
      "[6.8031312  6.1908863  6.68409367 8.53666423]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def partion_transition_matrix(A, first_absorbing_col=-2, last_absorbing_col = -1):\n",
    "    \"\"\"\n",
    "    this takes a transition matrix and slices it into two slices.  see dartmoth_markov_Lecture14.pdf page 11.\n",
    "    Also note that the slides use a tranposed A... (see how their rows sum to one, not the cols)\n",
    "    \n",
    "    Note, for now I just have this setup to accomodate absorbing states in EITHER the beginning of the matrix or the end,\n",
    "    but NOT both\n",
    "    \n",
    "    note this is back to using a zero indexing convention\n",
    "    \n",
    "    A must be square\n",
    "    \"\"\"    \n",
    "    assert A.shape[0] == A.shape[1]\n",
    "    m = A.shape[0]\n",
    "    \n",
    "    if first_absorbing_col < 0:\n",
    "        first_absorbing_col += m\n",
    "    if last_absorbing_col < 0:\n",
    "        last_absorbing_col += m\n",
    "    newA = A.T \n",
    "    assert first_absorbing_col <= last_absorbing_col, \"check that first_absorbing_col and last_absorbing_col are sensible\"\n",
    "    if first_absorbing_col == 0:\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        N_inv = np.identity(first_absorbing_col) - np.zeros((first_absorbing_col, first_absorbing_col), dtype= np.float64)\n",
    "        R = np.zeros((first_absorbing_col - 0, last_absorbing_col - first_absorbing_col + 1), dtype= np.float64)\n",
    "        for i in range(first_absorbing_col):\n",
    "            for j in range(first_absorbing_col):\n",
    "                N_inv[i,j] -= newA[i,j]\n",
    "        for i in range(first_absorbing_col): \n",
    "            for j in range(last_absorbing_col - first_absorbing_col + 1):\n",
    "                R[i, j] += newA[i, first_absorbing_col + j]\n",
    "    return N_inv, R\n",
    "\n",
    "N_inv, R = partion_transition_matrix(A, first_absorbing_col=-2, last_absorbing_col=-1) \n",
    "\n",
    "print(N_inv, \"\\n\")\n",
    "\n",
    "\n",
    "print(N_inv.shape, \"the shape\")\n",
    "ones_v = np.ones(N_inv.shape[0])\n",
    "\n",
    "print(\"answer of E[T]\")\n",
    "the_mean_vector =  np.linalg.solve(N_inv, ones_v)\n",
    "print(the_mean_vector)\n",
    "\n",
    "print(\"\\n\", R, \"That's R\",\"\\n\")\n",
    "print(\"for absorbtion destination, the solution is\")\n",
    "print(np.linalg.solve(N_inv, R))\n",
    "\n",
    "\n",
    "def get_variance(N):\n",
    "    local_ones_v = np.ones(N.shape[0])\n",
    "    e_sqd_x = 2 * N @ N - N\n",
    "    the_variance = e_sqd_x @ (local_ones_v) - (N @ (local_ones_v))**2\n",
    "    return the_variance\n",
    "\n",
    "def get_variance_alternative(N_inverse, mean_vector):\n",
    "    \"\"\"\n",
    "    This returns a variance vector for a start at one of the non-absorbing states... and for starting 100% of the time \n",
    "    only at that state, no where else....\n",
    "    \n",
    "    This seems much more opaque... \n",
    "    but it is computationally better has the benefit of not needing to explicitly invert any matrices\n",
    "    \n",
    "    See writeup for derivation\n",
    "    \"\"\"    \n",
    "    right_side = 2 * mean_vector - N_inverse @ (mean_vector + mean_vector**2) \n",
    "    a = np.linalg.solve(N_inverse, 2 * mean_vector)\n",
    "    b = - (mean_vector + mean_vector**2)\n",
    "    x = a + b\n",
    "    return x\n",
    "  \n",
    "\n",
    "N = np.linalg.inv(N_inv)\n",
    "print(N)\n",
    "print(N.dot(ones_v))\n",
    "\n",
    "\n",
    "print(get_variance(N))\n",
    "print(\" \")\n",
    "print(get_variance_alternative(N_inv, the_mean_vector))\n",
    "print( \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.5       , 0.5       , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.66666667, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.99799599,\n",
       "        0.00200401],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 1.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 500\n",
    "A = np.zeros((m,m))\n",
    "for j in range(1,m-1):\n",
    "    smaller = 1 / (j+1)\n",
    "    A[j-1,j] = 1- smaller\n",
    "    A[j-1,j+1]= smaller\n",
    "A[-2,-2] = 1\n",
    "A[-1,-1] = 1\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -0.5        -0.5        ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          1.         -0.66666667 ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          1.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  1.         -0.99798793\n",
      "  -0.00201207]\n",
      " [ 0.          0.          0.         ...  0.          1.\n",
      "  -0.99799197]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   1.        ]] \n",
      "\n",
      "(498, 498) the shape\n",
      "answer of E[T]\n",
      "[492.63789472 492.00577416 491.27001528 490.47729193 489.64818534\n",
      " 488.79371828 487.92052064 487.03290414 486.13383611 485.2254484\n",
      " 484.30932547 483.3866777  482.45845095 481.52539866 480.58813082\n",
      " 479.64714836 478.70286771 477.75563883 476.80575868 475.85348144\n",
      " 474.89902632 473.94258376 472.9843202  472.02438201 471.06289856\n",
      " 470.09998477 469.1357432  468.17026572 467.20363501 466.23592569\n",
      " 465.26720536 464.29753547 463.32697201 462.35556617 461.38336485\n",
      " 460.41041114 459.43674472 458.46240221 457.48741748 456.51182191\n",
      " 455.53564464 454.55891277 453.58165153 452.60388451 451.62563371\n",
      " 450.64691974 449.66776195 448.68817845 447.7081863  446.72780155\n",
      " 445.7470393  444.76591383 443.78443859 442.80262631 441.82048905\n",
      " 440.83803823 439.85528465 438.87223859 437.88890982 436.9053076\n",
      " 435.92144076 434.93731772 433.95294648 432.96833468 431.98348962\n",
      " 430.99841827 430.01312729 429.02762305 428.04191164 427.0559989\n",
      " 426.06989044 425.0835916  424.09710755 423.11044323 422.12360337\n",
      " 421.13659255 420.14941514 419.16207537 418.1745773  417.18692484\n",
      " 416.19912176 415.21117168 414.22307811 413.23484442 412.24647389\n",
      " 411.25796964 410.26933473 409.28057209 408.29168455 407.30267488\n",
      " 406.31354572 405.32429963 404.33493912 403.34546659 402.35588438\n",
      " 401.36619474 400.37639988 399.38650191 398.3965029  397.40640485\n",
      " 396.4162097  395.42591935 394.43553561 393.44506028 392.45449507\n",
      " 391.46384168 390.47310172 389.4822768  388.49136845 387.50037819\n",
      " 386.50930746 385.51815771 384.52693031 383.53562661 382.54424794\n",
      " 381.55279557 380.56127075 379.5696747  378.5780086  377.58627363\n",
      " 376.59447089 375.60260151 374.61066654 373.61866705 372.62660406\n",
      " 371.63447856 370.64229153 369.65004393 368.65773669 367.66537072\n",
      " 366.67294691 365.68046613 364.68792923 363.69533704 362.70269037\n",
      " 361.70999003 360.71723678 359.7244314  358.73157462 357.73866717\n",
      " 356.74570977 355.75270312 354.75964789 353.76654477 352.77339441\n",
      " 351.78019744 350.7869545  349.79366621 348.80033317 347.80695598\n",
      " 346.81353521 345.82007143 344.82656521 343.83301709 342.83942761\n",
      " 341.84579729 340.85212666 339.85841622 338.86466646 337.87087788\n",
      " 336.87705095 335.88318615 334.88928393 333.89534476 332.90136908\n",
      " 331.90735731 330.9133099  329.91922727 328.92510982 327.93095798\n",
      " 326.93677213 325.94255266 324.94829998 323.95401445 322.95969645\n",
      " 321.96534635 320.9709645  319.97655127 318.98210699 317.98763202\n",
      " 316.99312669 315.99859134 315.00402628 314.00943184 313.01480834\n",
      " 312.02015609 311.02547539 310.03076654 309.03602984 308.04126559\n",
      " 307.04647406 306.05165555 305.05681032 304.06193866 303.06704083\n",
      " 302.07211711 301.07716774 300.08219299 299.08719311 298.09216836\n",
      " 297.09711898 296.1020452  295.10694728 294.11182545 293.11667993\n",
      " 292.12151096 291.12631876 290.13110356 289.13586557 288.14060502\n",
      " 287.1453221  286.15001704 285.15469004 284.1593413  283.16397103\n",
      " 282.16857942 281.17316667 280.17773298 279.18227853 278.18680351\n",
      " 277.1913081  276.1957925  275.20025687 274.2047014  273.20912627\n",
      " 272.21353164 271.21791769 270.22228458 269.22663249 268.23096158\n",
      " 267.235272   266.23956392 265.24383751 264.2480929  263.25233027\n",
      " 262.25654975 261.26075151 260.26493568 259.26910242 258.27325187\n",
      " 257.27738417 256.28149946 255.28559789 254.28967959 253.2937447\n",
      " 252.29779335 251.30182567 250.3058418  249.30984187 248.31382599\n",
      " 247.31779431 246.32174694 245.32568401 244.32960564 243.33351195\n",
      " 242.33740306 241.34127908 240.34514014 239.34898635 238.35281783\n",
      " 237.35663468 236.36043701 235.36422495 234.36799858 233.37175804\n",
      " 232.37550341 231.3792348  230.38295232 229.38665608 228.39034617\n",
      " 227.39402269 226.39768574 225.40133542 224.40497183 223.40859507\n",
      " 222.41220522 221.41580239 220.41938667 219.42295814 218.42651691\n",
      " 217.43006305 216.43359666 215.43711783 214.44062665 213.44412319\n",
      " 212.44760756 211.45107982 210.45454007 209.45798839 208.46142485\n",
      " 207.46484955 206.46826256 205.47166396 204.47505383 203.47843225\n",
      " 202.48179929 201.48515503 200.48849955 199.49183292 198.49515521\n",
      " 197.49846651 196.50176687 195.50505638 194.50833511 193.51160312\n",
      " 192.51486048 191.51810727 190.52134355 189.52456939 188.52778485\n",
      " 187.53099001 186.53418494 185.53736968 184.54054432 183.5437089\n",
      " 182.54686351 181.5500082  180.55314302 179.55626805 178.55938335\n",
      " 177.56248897 176.56558497 175.56867142 174.57174837 173.57481589\n",
      " 172.57787402 171.58092283 170.58396237 169.5869927  168.59001388\n",
      " 167.59302595 166.59602898 165.59902302 164.60200812 163.60498434\n",
      " 162.60795172 161.61091033 160.61386021 159.61680141 158.61973399\n",
      " 157.62265799 156.62557346 155.62848046 154.63137904 153.63426924\n",
      " 152.63715111 151.64002469 150.64289005 149.64574721 148.64859624\n",
      " 147.65143717 146.65427005 145.65709493 144.65991186 143.66272087\n",
      " 142.66552201 141.66831533 140.67110087 139.67387867 138.67664877\n",
      " 137.67941122 136.68216606 135.68491334 134.68765308 133.69038534\n",
      " 132.69311016 131.69582757 130.69853762 129.70124034 128.70393578\n",
      " 127.70662397 126.70930495 125.71197877 124.71464546 123.71730505\n",
      " 122.71995759 121.72260311 120.72524165 119.72787325 118.73049794\n",
      " 117.73311575 116.73572674 115.73833092 114.74092834 113.74351903\n",
      " 112.74610303 111.74868037 110.75125108 109.7538152  108.75637276\n",
      " 107.7589238  106.76146834 105.76400643 104.76653809 103.76906336\n",
      " 102.77158227 101.77409485 100.77660113  99.77910114  98.78159492\n",
      "  97.7840825   96.78656391  95.78903917  94.79150832  93.79397139\n",
      "  92.79642841  91.7988794   90.8013244   89.80376344  88.80619655\n",
      "  87.80862375  86.81104507  85.81346054  84.81587019  83.81827405\n",
      "  82.82067215  81.82306451  80.82545115  79.82783212  78.83020743\n",
      "  77.83257711  76.83494119  75.83729969  74.83965265  73.84200008\n",
      "  72.84434201  71.84667847  70.84900949  69.85133508  68.85365528\n",
      "  67.85597011  66.85827959  65.86058375  64.86288261  63.8651762\n",
      "  62.86746454  61.86974766  60.87202558  59.87429832  58.8765659\n",
      "  57.87882836  56.8810857   55.88333797  54.88558517  53.88782733\n",
      "  52.89006448  51.89229663  50.89452382  49.89674605  48.89896336\n",
      "  47.90117576  46.90338327  45.90558593  44.90778374  43.90997673\n",
      "  42.91216493  41.91434834  40.916527    39.91870093  38.92087013\n",
      "  37.92303465  36.92519448  35.92734967  34.92950021  33.93164615\n",
      "  32.93378748  31.93592424  30.93805645  29.94018412  28.94230727\n",
      "  27.94442593  26.9465401   25.94864981  24.95075509  23.95285594\n",
      "  22.95495238  21.95704444  20.95913213  19.96121547  18.96329449\n",
      "  17.96536918  16.96743959  15.96950571  14.97156757  13.9736252\n",
      "  12.97567859  11.97772778  10.97977278   9.98181361   8.98385027\n",
      "   7.9858828    6.98791121   5.98993551   4.99195572   3.99397186\n",
      "   2.99598394   1.99799197   1.        ]\n",
      "\n",
      " [[0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.         0.        ]\n",
      " [0.00200803 0.        ]\n",
      " [0.99799599 0.00200401]] That's R \n",
      "\n",
      "for absorbtion destination, the solution is\n",
      "[[0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800001 0.00199999]\n",
      " [0.99800002 0.00199998]\n",
      " [0.99799599 0.00200401]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N_inv, R = partion_transition_matrix(A.T, first_absorbing_col=-2, last_absorbing_col=-1) \n",
    "\n",
    "print(N_inv, \"\\n\")\n",
    "\n",
    "\n",
    "print(N_inv.shape, \"the shape\")\n",
    "ones_v = np.ones(N_inv.shape[0])\n",
    "\n",
    "print(\"answer of E[T]\")\n",
    "the_mean_vector =  np.linalg.solve(N_inv, ones_v)\n",
    "print(the_mean_vector)\n",
    "\n",
    "print(\"\\n\", R, \"That's R\",\"\\n\")\n",
    "print(\"for absorbtion destination, the solution is\")\n",
    "print(np.linalg.solve(N_inv, R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.362105278535182"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(m-2) - the_mean_vector[0]\n",
    "# tbc, but I think average amount of time in state 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
