{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to *considerable overlap* with work in other texts, e.g. Gallagher, Feller vol 1 chapter 15, and Blitzstein, most of the problems in this chapter were skipped.  \n",
    "\n",
    "On the whole your author quite liked the development in this chapter, including the coupling argument to prove steady state for positive recurrent chains, and the build-up which included a lot of basic materials that may be developped elsewhere, e.g. in renewal theory in some texts. Strictly speaking the chapter does not prove that null recurrent chains have a limit of zero, though the approach on the chapter of focusing on transient then positive recurrent seems smart.  \n",
    "\n",
    "The coverage toward the end of the chapter on reversibility of select positive recurrent chains is quite good and a few of the exercises associated with reversibility are the focus of this notebook.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3, 4, 5  -- Symmetric Random Walks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lemma:**   \n",
    "\n",
    "for (finite )$a_n, b_n \\gt 0$  we have $a_n \\sim b_n$ (i.e. $\\lim_{n \\to \\infty} \\frac{a_n}{b_n} \\to 1$) then \n",
    "$\\sum_{n=1}^\\infty a_n \\lt \\infty $  *iff* $\\sum_{n=1}^\\infty b_n \\lt \\infty $  \n",
    "\n",
    "*proof:*  \n",
    "The claim is very simple.  \n",
    "\n",
    "what the above limit tells us is that for any $\\epsilon \\gt 0$, for all $n \\geq N$,  \n",
    "\n",
    "$1 - \\epsilon \\leq \\frac{a_n}{b_n} \\leq 1 + \\epsilon$  \n",
    "\n",
    "i.e. \n",
    "\n",
    "$(1 - \\epsilon)b_n \\leq a_n \\leq (1 + \\epsilon) b_n$    \n",
    "\n",
    "hence if the series of b_n converges, it is enough to sum over the tail, i.e. over all $n \\geq N$  (as the finite number of terms before N must necessarily have a finite sum)  \n",
    "\n",
    "so consider the point wise bound \n",
    "\n",
    "$a_n \\leq (1 + \\epsilon) b_n$  \n",
    "\n",
    "and sum over it to get  \n",
    "$\\sum_{n \\geq N} a_n \\leq \\sum_{n \\geq N} (1 + \\epsilon) b_n= (1 + \\epsilon)\\sum_{n \\geq N}  b_n =  (1 + \\epsilon)\\cdot c \\lt \\infty$  \n",
    "\n",
    "which shows $a_n$ converges.  \n",
    "\n",
    "For the other leg, consider when the series of $b_n$ diverges, again using the pointwise bound for $n \\geq N$  \n",
    "$(1 - \\epsilon)b_n \\leq a_n $   \n",
    "and summing over the bound  \n",
    "\n",
    "$\\infty =(1 - \\epsilon) \\big(\\infty\\big) = (1 - \\epsilon) \\sum_{n\\geq N}b_n = \\sum_{n\\geq N}(1 - \\epsilon)b_n \\leq \\sum_{n\\geq N} a_n $   \n",
    "\n",
    "and hence the series of $a_n$ is lower bounded by positive infinity, which means the series of $a_n$ must diverge as well.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The problem with my approach is for 4 and 5, they don't seem to match exactly to the stated problem format... my modelling for 4 seems to map with the equivalent formulation from Polya (based on middle paragraph in Feller) but my 3 dimension version doesn't really map to anything....  it seems that the formulation in Mosteller was a bit different  **  \n",
    "\n",
    "also see page 67 - 69 of Karlin & Taylor  \"A First Course in Stochastic Processes\"  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It isn't clear how we are supposed to prove the result for $4$ and $5$ using techniques from *this chapter*.  However your author went and revisited page 361 of Feller vol 1 (3rd edition) to unpack the standard combinatorial argument for this problem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 \n",
    "Show that the symmetric random walk where $(P_{i,i+1} = p = 0.5 = P_{i,i-1})$ is (null) recurrent, while for $p \\neq 0.5$ it is transient.  \n",
    "\n",
    "At this point, we know many different approaches to proving the 1-D result.  A favorite of your authors involves using the Wald Equation for the truncated (gamblers ruin) random walk, and other martingale methods for $p \\neq 0.5$.  \n",
    "\n",
    "The very simple fact is that SLLN implies transience for any non-zero drift (i.e. $p \\neq 0.5$ in this problem), though mechanically we can see this below as well.  \n",
    "\n",
    "The method that will generalize to higher dimensions, however is to count the expected number of renewals our starting position (which is arbitrary).  We have a non-transient chain **iff** the expected number of visits is infinite (this is the key underlying idea from strong markov property or renewal theorem Theorem 2 (3.4) in Feller Vol 1 XIII).  \n",
    "\n",
    "So in the simple random walk, we may only return to the origin on even turns (we have a chain with period 2 here), so looking in turn of (rademacher) coin tossing we see that we visit the origin on the $2n$ turn *iff* exactly $n$ of the $2n$ 'coin tosses' are $+1$ (which implies $2n -n =n$ are $-1$), so we have \n",
    "\n",
    "$Pr\\{A_{2n}\\} = \\binom{2n}{n}p^n(1-p)^n = \\binom{2n}{n}\\big(p(1-p)\\big)^n = \\binom{2n}{n}\\big(pq\\big)^n$  \n",
    "\n",
    "hence with large enough $n$ we may use Stirling's approximation to see \n",
    "\n",
    "$a_n = \\binom{2n}{n}\\big(pq\\big)^n \\sim \\frac{\\sqrt{2\\pi 2n}\\cdot (2n)^{2n}e^{-2n}}{\\sqrt{2\\pi n}\\cdot (n)^{n}e^{-n}\\sqrt{2\\pi n}\\cdot (n)^{n}e^{-n}}\\cdot(pq)^{n}= \\frac{ 2^{2n}n^{2n}}{\\sqrt{\\pi n}\\cdot (n)^{n}\\cdot (n)^{n}}\\cdot(pq)^{n} = \\frac{1 }{\\sqrt{\\pi n}}\\cdot(4pq)^{n} $  \n",
    "\n",
    "*remark*  \n",
    "we hence have a series where \n",
    "\n",
    "$a_n \\sim \\frac{1 }{\\sqrt{\\pi}}\\cdot(4pq)^{2n} \\cdot \\frac{1}{\\sqrt{n}}$  \n",
    "\n",
    "From $\\text{GM} \\leq \\text{AM}$ we know $pq = p(1-p) \\leq \\frac{1}{4}$ with equality **iff** $p = q = \\frac{1}{2}$. \n",
    "\n",
    "i)  For the *non-symmetric* case we have:    \n",
    "$a_n \\sim \\frac{1 }{\\sqrt{\\pi}}\\cdot(2pq)^{n} \\cdot \\frac{1}{\\sqrt{n}} \\leq \\frac{1 }{\\sqrt{\\pi}}\\cdot(2pq)^{n} = \\frac{1 }{\\sqrt{\\pi}}\\cdot(\\delta)^{n} $  \n",
    "\n",
    "for some $\\delta \\in (0,1)$. Summing over the bound we have \n",
    "\n",
    "$\\sum_{n=1}^\\infty a_n  \\sim \\sum_{n=1}^\\infty \\frac{1 }{\\sqrt{\\pi}}\\cdot(2pq)^{n} \\cdot \\frac{1}{\\sqrt{n}}  \\leq  \\sum_{n=1}^\\infty\\frac{1 }{\\sqrt{\\pi}}\\cdot(\\delta)^{n} = \\frac{\\delta }{\\sqrt{\\pi}}\\sum_{n=0}^\\infty \\delta^{n} = \\frac{\\delta }{\\sqrt{\\pi}}\\cdot \\frac{1}{1-\\delta} \\lt \\infty$   \n",
    "\n",
    "since the series converges, the assymetric case is transient.  \n",
    "\n",
    "ii) For the *symmetric* case of a simple random walk we have:    \n",
    "$a_n \\sim \\frac{1 }{\\sqrt{\\pi}}\\cdot(4pq)^{2n} \\cdot \\frac{1}{\\sqrt{n}} = \\frac{1 }{\\sqrt{\\pi}}\\cdot(4\\frac{1}{4})^{2n} \\cdot \\frac{1}{\\sqrt{n}} = \\frac{1 }{\\sqrt{\\pi n}}$  \n",
    "\n",
    "summing over the results, we have \n",
    "\n",
    "$\\sum_{n=1}^\\infty a_n \\sim \\sum_{n=1}^\\infty \\frac{1 }{\\sqrt{\\pi n}} =  \\frac{1 }{\\sqrt{\\pi}}\\sum_{n=1}^\\infty \\frac{1}{\\sqrt{n}} = \\infty$  \n",
    "(i.e. we have a series that is bounded below by the harmonic series and hence it diverges)  \n",
    "\n",
    "this confirms that the origin (and all states that communicate with it-- which is all possible state) is persistent / recurrent.  We can verify that this is in fact null recurrent since chain is time reversible and every steady state value is implied to be equal -- the sum of which is $\\infty$ and hence we cannot get a normalizing constant $\\to $ null recurrent.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  \n",
    "\n",
    "Here we have a 2-D symmetric simple random walk given by \n",
    "\n",
    "$P_{(i,j),(i,j+1)} = P_{(i,j),(i+1,j)}= P_{(i,j),(i-1,j)}= P_{(i,j),(i,j+1)}= \\frac{1}{4}$  \n",
    "\n",
    "equivalently, we can think of a random vector \n",
    "\n",
    "$\\mathbf v^{(0)} := \\begin{bmatrix}\n",
    "v_1\\\\ \n",
    "v_2\\\\ \n",
    "\\end{bmatrix}^{(0)}=\\begin{bmatrix} \n",
    "0\\\\ \n",
    "0\\\\ \n",
    "\\end{bmatrix}$  \n",
    "\n",
    "and   \n",
    "$\\mathbf v^{(n)} = \\mathbf v^{(n-1)} + \\mathbf x^{(n)} $  \n",
    "\n",
    "where with uniform probability of $\\frac{1}{4}$ we have $\\mathbf x$ takes on values of $\\{\\mathbf e_1, - \\mathbf e_1, \\mathbf e_2, - \\mathbf e_2\\}$  (where $\\mathbf e_j$ is the 2-d standard basis vector)  \n",
    "\n",
    "and we want to compute the expected value of \n",
    "\n",
    "where as always, partial sums of \"counting\" random variables (in discrete / positive integer time with no bulk arrivals) is given by  \n",
    "\n",
    "- - - - - \n",
    "*begin interlude of some bigger theoretical points*  \n",
    "$N(t) = \\sum_{n=1}^t \\mathbb I_{\\mathbf v^{(n)} = \\mathbf 0}$   \n",
    "\n",
    "and as $t\\to \\infty$  \n",
    "\n",
    "$N(\\infty) = \\sum_{n=1}^\\infty \\mathbb I_{\\mathbf v^{(n)} = \\mathbf 0}$   \n",
    "\n",
    "note: by monotone convergence theorem (and in particular non-negativity of indicator r.v.'s)  \n",
    "\n",
    "$E\\big[N(\\infty)\\big] = \\big[\\sum_{n=1}^\\infty \\mathbb I_{\\mathbf v^{(n)} = \\mathbf 0}\\big] = \\sum_{n=1}^\\infty \\big[\\mathbb I_{\\mathbf v^{(n)} = \\mathbf 0}\\big]$  \n",
    "\n",
    "though since renewals may only happen on even iterations, we may write this instead as  \n",
    "\n",
    "$E\\big[N(\\infty)\\big]  = \\sum_{n=1}^\\infty \\big[\\mathbb I_{\\mathbf v^{(2n)} = \\mathbf 0}\\big]$\n",
    "\n",
    "via application of Borell-Cantelli, or Markov Inequality, if $E\\big[N(\\infty)\\big] = \\mu \\lt \\infty$ then we may say  \n",
    "$Pr\\{ N(\\infty)\\gt c)\\} \\leq \\frac{\\mu}{c}$, for some $c \\gt 0$, which tends to zero as $c \\to \\infty$ which tells us there is zero probability of an infinite number of arrivals in this case.   For the converse, we need to exploint memorylessness / Strong Markov Property, and compute the expected number of renewals another way.  Letting $f_i$ be the probability of renewal given a start at the origin, supposing that $f_i \\in [0,1)$, we may compute the expected number of renewals as \n",
    "\n",
    "$E\\big[N(\\infty)\\big] = f_i + f_i^2 + f_i^3 + f_i^4 + .... = \\frac{f_i}{1-f_i}$  \n",
    "\n",
    "which reads as the probability of the first renewal is $f_i$, and the probability of the $jth$ renewal is the probability of having the $j-1$th renewal (given by $f_i^{j-1}$ by inductive hypothesis), and by strong markov property at the time of the $j-1$th renewal, the probability of the next renewal occuring is given by $f_i$ hence the probability of the $jth$ renewal is $f_i^{j}$.  This expected value is finite for all  $f_i \\in [0,1)$ hence if we compute an infinite number of expected renewals, this occurrs **iff** $f_i = 1$.   \n",
    "\n",
    "We can equivalently note that for $f_i \\in [0,1)$, the distribution of the number of renewals is geometric, indexing at zero, with probability of \"success\" (i.e. terminating the process aka never having another renewal) given by $1-f_i$.   In the case of $f_i = 1$ the random variable is defective and in fact has all probability mass at $\\infty$. \n",
    "\n",
    "*end interlude of some bigger theoretical points*  \n",
    "- - - - - \n",
    "now to compute \n",
    "\n",
    "$u_{_{2n}} = Pr\\{{\\mathbf v^{(2n)} = \\mathbf 0}\\big\\} = E\\big[\\mathbb I_{\\mathbf v^{(2n)} = \\mathbf 0}\\big]$  \n",
    "\n",
    "we have work through one term at at time to in effect recover a multinomial probability  \n",
    "\n",
    "$u_{_{2n}} = 4^{-2n}\\cdot \\sum_{k=0}^n \\binom{2n}{2k}\\binom{2k}{k}\\binom{2n-2k}{n-k}$  \n",
    "In words: we start with the zero vector, and the only way the $2n$th vector is the zero vector is if of our $2n$ events $2k$ of them are changes in the $x_1$ coordinate, with exactly $k$ of those being $+1$, and we chain on the residual requirement that of the $2n-2k$ changes in the $x_2$ coordinate, exactly $n-k$ of them are $+1$   -- and we then sum over all $k$. Each sample path here has probability of $\\big(\\frac{1}{4}\\big)^{2n}$ (i.e. independent events with each simple outcome has $\\frac{1}{4}$ probability, and there are $2n$ events here)  \n",
    "\n",
    "From here we simplify and compute:  \n",
    "\n",
    "$u_{_{2n}} $  \n",
    "$= 4^{-2n}\\cdot \\sum_{k=0}^n \\binom{2n}{2k}\\binom{2k}{k}\\binom{2n-2k}{n-k}$  \n",
    "$= 4^{-2n}\\cdot \\sum_{k=0}^n \\frac{(2n!)}{(2n-2k)!(2k)!}\\frac{(2k)!}{k! k!}\\frac{(2n-2k)!}{(n-k)!(n-k)!}$  \n",
    "$= 4^{-2n}\\cdot \\sum_{k=0}^n (2n!)\\frac{1}{k! k!}\\frac{1}{(n-k)!(n-k)!}$  \n",
    "$= 4^{-2n}\\cdot \\sum_{k=0}^n \\frac{1}{k! k!}\\frac{1}{(n-k)!(n-k)!}n!n!\\frac{(2n)!}{n!n!}$  \n",
    "$= 4^{-2n}\\binom{2n}{n}\\cdot \\sum_{k=0}^n \\frac{n!n!}{k! k!(n-k)!(n-k)!}$  \n",
    "$= 4^{-2n}\\binom{2n}{n}\\cdot \\sum_{k=0}^n \\binom{n}{k}\\binom{n}{k}$  \n",
    "$= 4^{-2n}\\binom{2n}{n}\\cdot \\binom{2n}{n}$  \n",
    "$= \\Big(2^{-2n}\\binom{2n}{n}\\Big)^2$  \n",
    "$\\sim \\big(\\frac{1 }{\\sqrt{\\pi}}\\frac{1}{\\sqrt{n}}\\big)^2 $  \n",
    "$\\frac{1}{\\pi n}$  \n",
    "\n",
    "note: going from the 5th to 4th last line uses a standard combinatorial result.  For a picture proof, see here:  \n",
    "https://github.com/DerekB7/probability/blob/master/binomial_mutliplication_proof.ipynb  \n",
    "\n",
    "Now summing over this result, we have \n",
    "\n",
    "$\\sum_{n=1}^\\infty u_{_{2n}} \\sim \\sum_{n=1}^\\infty \\frac{1}{\\pi n} = \\frac{1}{\\pi}\\sum_{n=1}^\\infty \\frac{1}{ n} = \\infty$  \n",
    "\n",
    "the series diverges and hence we have proven that the simple 2-D random walk is recurrent.  \n",
    "\n",
    "*remark: *  \n",
    "The Harmonic Series is at the 'cross-over' point of convergent vs divergent series. We should anticipate that higher dimensional random walks diverge because they tend to a sequence involving $\\big(\\frac{1}{n}\\big)^\\alpha$ for some $\\alpha \\gt 1$.  Based on what we've seen thus far, we'd guess $\\alpha = \\frac{3}{2}$, though any value $\\gt 1$ would suffice.  (The guess is correct though it takes a lot of work to show it.)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remark: on null recurrence**    \n",
    "Both of the above i.e. 2-d and 1-d random walks are null recurrent.  The easiest way to see this is to use integral estimates for their expected number of renewals at time $t$  \n",
    "\n",
    "in the 1-d case we have   \n",
    "$m_{d=1}(t) \\approx \\int_{1}^t \\frac{1}{\\sqrt{n}} dn = 2(\\sqrt{t}-1)$    \n",
    "and in the 2-d case we have  \n",
    "$m_{d=2}(t) \\int_{1}^t \\frac{1}{n} dn = \\log(t)$  \n",
    "\n",
    "notice that these both grow sub-linearly.  The Elemetnary Renewal Theorem tells us the time averaged expected number of renewals tends to $\\frac{1}{\\bar{X}}$ (where $\\bar{X}$ is expected time until renewal).  The easy part of this theorem is the lower bound and that is all we need here. I.e. the easy part of the  Elemetnary Renewal Theorem says  \n",
    "\n",
    "$\\frac{1}{\\bar{X}}  - \\frac{1}{t} \\leq \\frac{m(t)}{t}$  \n",
    "and hence for any finite process with $\\bar{X}\\lt \\infty$, $c -\\epsilon \\leq m(t)$, for some fixed finite $c\\gt 0$ and any $\\epsilon \\gt 0$ for large enough $t$.  \n",
    "\n",
    "Equivalently, for a finite expected renewal time, this implies positive constant that we can lower bound the time averaged expected renewals by.  \n",
    "\n",
    "But in our random walks  \n",
    "\n",
    "$\\frac{m_{d=1}(t)}{t} = 2\\big(\\frac{1}{\\sqrt{t}}-\\frac{1}{t}\\big)$    \n",
    "$\\frac{m_{d=2}(t)}{t} \\int_{1}^t \\frac{1}{n} dn = \\frac{\\log(t)}{t}$  \n",
    "\n",
    "but both may be made arbitrarily close to 0 with large enough $t$ and hence cannot be lower bounded by some fixed positive constant.  This means our walks are null recurrent.  \n",
    "\n",
    "\n",
    "*further remark:*  for these particular simple random walks a much easier way to prove null recurrence is to note that the markov chains are time reversible, and a solution of the detailed balance equations would imply that *all* states have the same invariant measure -- but there are a countably infinite number of these and hence we cannot possibly normalize the distribution -- thus while recurrent, these cannot be positive recurrent (aka they are null recurrent).  \n",
    "\n",
    "The elementary renewal theorem approach, however applies to all sort of chains and processes that may not be time reversible.  Further, it points to the key feature of null recurrent chains -- while their expected number of renewals is infinite, that summation grows at a sublinear level (see above logarithms and square roots), hence the time averaged expected renewals tends to zero for null recurrent chains.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 \n",
    "\n",
    "The approach for $5$ are in a similar vein, though the calculations are more involved.  The end result is we prove that the the 3-D simple random walk is transient.   \n",
    "\n",
    "Here we have again a markov chain representation where the value at the current state is incremented by 1 or decremented by one (and only one) of 3 different coordinates.   The alternative representation as a sum of coordinate vectors is \n",
    "\n",
    "$\\mathbf v^{(0)} := \\begin{bmatrix}\n",
    "v_1\\\\ \n",
    "v_2\\\\ \n",
    "v_3\\\\\n",
    "\\end{bmatrix}^{(0)}=\\begin{bmatrix} \n",
    "0\\\\ \n",
    "0\\\\ \n",
    "0\\\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "and   \n",
    "$\\mathbf v^{(n)} = \\mathbf v^{(n-1)} + \\mathbf x^{(n)} $  \n",
    "\n",
    "where with uniform probability of $\\frac{1}{6}$ we have $\\mathbf x$ takes on values of $\\{\\mathbf e_1, - \\mathbf e_1, \\mathbf e_2, - \\mathbf e_2, \\mathbf e_3, - \\mathbf e_3 \\}$  (where $\\mathbf e_j$ is the 3-d standard basis vector) \n",
    "\n",
    "as before, we can only have renewals on even iterations and we need to compute \n",
    "\n",
    "$u_{_{2n}} = Pr\\{{\\mathbf v^{(2n)} = \\mathbf 0}\\big\\} = E\\big[\\mathbb I_{\\mathbf v^{(2n)} = \\mathbf 0}\\big]$  \n",
    "\n",
    "so, working through the combinatorics, we have \n",
    "\n",
    "$u_{_{2n}} = 6^{-2n}\\cdot\\big\\{ \\sum_{k=0}^n \\binom{2n}{2k}\\binom{2k}{k}\\big\\}\\sum_{j=0}^{n-k} \\binom{2n-2k}{2j}\\cdot \\binom{2j}{j}\\cdot \\binom{2n-2k-2j}{n-k-j}$   \n",
    "\n",
    "\n",
    "in words: each sample path has probability $6^{-2n}$ and the bracketted term is the same as in the selection for $x_1$  (in part 4), from here we chain on our selection of $x_2$ consists of selecting an even $2j$ $x_2$ values from the remaining $(2n-2k)$ events, of which exactly $j$ of the $2j$ are $+1$'s and then chaining on the residual for $x_3$, i.e. that there are $(2n-2k-2j)$ events left which must occur in the $x_3$ coordinate, and exactly $(n-k-j)$ of them must be $+1$ in order for a renewal to occur.  From here we simplify to  \n",
    "\n",
    "$u_{_{2n}} $  \n",
    "$= 6^{-2n}\\cdot\\big\\{ \\sum_{k=0}^n \\binom{2n}{2k}\\binom{2k}{k}\\big\\}\\sum_{j=0}^{n-k} \\binom{2n-2k}{2j}\\cdot \\binom{2j}{j}\\cdot \\binom{2n-2k-2j}{n-k-j}$   \n",
    "$= 6^{-2n}\\cdot\\big\\{ \\sum_{k=0}^n \\frac{(2n!)}{(2n-2k)!(2k)!}\\frac{(2k)!}{k! k!}\\big\\}\\sum_{j=0}^{n-k} \\frac{(2n-2k)!}{(2j)!(2n-2k-2j)!}\\cdot\\frac{(2j)!}{j!j!}\\cdot \\binom{(2n-2k-2j)!}{(n-k-j)!(n-k-j)!}$  \n",
    "$= 6^{-2n}\\cdot \\sum_{k=0}^n \\sum_{j=0}^{n-k} \\frac{(2n!)}{(2n-2k)!(2k)!}\\frac{(2k)!}{k! k!} \\cdot \\frac{(2n-2k)!}{1}\\cdot\\frac{1}{j!j!}\\cdot \\frac{1}{(n-k-j)!(n-k-j)!}$  \n",
    "$= 6^{-2n}\\cdot \\sum_{k=0}^n \\sum_{j=0}^{n-k} (2n!)\\frac{1}{k! k!} \\cdot\\frac{1}{j!j!}\\cdot \\frac{1}{(n-k-j)!(n-k-j)!}\\cdot \\frac{n!n!}{n!n!}$  \n",
    "$= 2^{-2n}\\binom{2n}{n}\\cdot \\sum_{k=0}^n \\sum_{j=0}^{n-k} 3^{-2n}\\cdot\\frac{n!n!}{k! k!j!j!(n-k-j)!(n-k-j)!}$  \n",
    "$= 2^{-2n}\\binom{2n}{n}\\cdot \\sum_{k=0}^n \\sum_{j=0}^{n-k} \\big(3^{-n}\\cdot\\frac{n!}{k! j!(n-k-j)!}\\big)^2$  \n",
    "$\\leq 2^{-2n}\\binom{2n}{n}\\cdot \\big(3^{-n}  \\text{max:  }\\frac{n!}{k! j!(n-k-j)!}\\big)$  \n",
    "\n",
    "where the maximization is understood to be amongst all feasible $k$ and $j$ for all $n$  \n",
    "justification:  either Hoelder's Inequlity where $\\langle \\mathbf a, \\mathbf a \\rangle \\leq \\big \\Vert \\mathbf a\\big \\Vert_1 \\big \\Vert \\mathbf a\\big \\Vert_\\infty $, or merely observing a point-wise bound and summing over the bound, i.e.  \n",
    "$\\big(3^{-n}\\cdot\\frac{n!}{k! j!(n-k-j)!}\\big)^2 \\leq \\big(3^{-n}\\cdot\\frac{n!}{k! j!(n-k-j)!}\\big) \\cdot \\big(3^{-n}  \\text{max:  }\\frac{n!}{k! j!(n-k-j)!}\\big) $  \n",
    "\n",
    "and summing over the bound gives   \n",
    "$\\sum_{k=0}^n \\sum_{j=0}^{n-k} \\big(3^{-n}\\cdot\\frac{n!}{k! j!(n-k-j)!}\\big)^2 $  \n",
    "$\\leq \\sum_{k=0}^n \\sum_{j=0}^{n-k}\\big(3^{-n}\\cdot\\frac{n!}{k! j!(n-k-j)!}\\big) \\cdot \\big(3^{-n}  \\text{max:  }\\frac{n!}{k! j!(n-k-j)!}\\big) $  \n",
    "$= \\big(3^{-n}  \\text{max:  }\\frac{n!}{k! j!(n-k-j)!}\\big) \\sum_{k=0}^n \\sum_{j=0}^{n-k}\\big(3^{-n}\\cdot\\frac{n!}{k! j!(n-k-j)!}\\big)$  \n",
    "$= 3^{-n}  \\text{  max:  }\\frac{n!}{k! j!(n-k-j)!}$  \n",
    "\n",
    "(which is equal to $ 3^{-n}  \\text{  max:  }\\binom{n}{k}\\binom{n-k}{j} $)  \n",
    "\n",
    "Here we recognize the terms of the uniform trinomial distribution in \n",
    "$\\sum_{k=0}^n \\sum_{j=0}^{n-k}\\big(3^{-n}\\cdot\\frac{n!}{k! j!(n-k-j)!}\\big)$  \n",
    "and hence they sum to one.  \n",
    "\n",
    "\n",
    "*remarks on the close:*  \n",
    "we intend to proceed by  \n",
    "$(i)\\max \\longrightarrow (ii)\\sim \\text{Stirling approximation  }\\longrightarrow (iii)\\max$  \n",
    "That is, we look at the first max above, then pass to Stirling approximation, and then upper bound that via another maximization.  For the above, we focus our attention on $n\\geq 3$ (the first two terms cannot matter for convergence and may be neglected for the rest of the argument).  For each term, the maximum must exist for any given $n$ as there are finitely many configurations.  We merely note for our terms now, whatever the maximum is, it is positive integer valued -- i.e. it does not have a zero in the denominator.  (Rationale: 0! = 1!, but there is an exchange argument where incrementing the $0!\\to 1!$ and decrementing some over term of $r \\geq 2$ \n",
    "(which must exist, or else $n = k +  j+ (n-k-j)+ \\leq 1+1+0 = 2\\lt n$) \n",
    "\n",
    "and decrementing from $r! \\to (r-1)!$ decreases the product in the denominator and hence increases the value of the fraction for any $n \\geq 3$. Thus we've established that each term is a positive integer $\\in \\{1, 2, ..., n-2\\}$.  After we pass to Stirling approximation, we will maximize once again, *and* extend the domain for each term to $[1, n-2]$  (where the former domain is a proper subset of this and hence the domain extension further increases the maximum attainable value). \n",
    "\n",
    "To clean up notation, we'll do a change of variables, where $m: = (n-k-j)$ (and of course $\\geq 1$).      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing to striling approximation, our maximal term is given by \n",
    "\n",
    "$\\frac{\\sqrt{2\\pi n}\\cdot (n)^{n}e^{-n}}{\\sqrt{2\\pi j}\\cdot (j)^{j}e^{-j}\\cdot \\sqrt{2\\pi k}\\cdot (k)^{k}e^{-k}\\cdot \\sqrt{2\\pi m}\\cdot (m)^{m}e^{-m}}$\n",
    "\n",
    "where we have natural numbers $j, k, m$ and $j + k + m = n$    \n",
    "\n",
    "$=\\frac{\\sqrt{2\\pi}}{\\sqrt{2\\pi }\\sqrt{2\\pi }\\sqrt{2\\pi}}\\cdot \\frac{\\sqrt{n}}{\\sqrt{jkm}} \\cdot \\frac{(n)^{n}}{(j)^{j}(k)^{k}(m)^{m}}$\n",
    "\n",
    "$=\\frac{1}{2\\pi}\\cdot \\sqrt{n} \\cdot n^n  \\cdot \\Big\\{ j^{-j}\\frac{1}{\\sqrt{j}}\\cdot k^{-k}\\frac{1}{\\sqrt{k}}\\cdot m^{-m}\\frac{1}{\\sqrt{m}}\\Big\\}$  \n",
    "\n",
    "$\\leq \\frac{1}{2\\pi}\\cdot \\sqrt{n} \\cdot n^n  \\cdot \\Big\\{ \\big(\\frac{n}{3}\\big)^{-\\frac{n}{3}}\\frac{1}{\\sqrt{\\frac{n}{3}}}\\cdot \\big(\\frac{n}{3}\\big)^{-\\frac{n}{3}}\\frac{1}{\\sqrt{\\frac{n}{3}}}\\cdot \\big(\\frac{n}{3}\\big)^{-\\frac{n}{3}}\\frac{1}{\\sqrt{\\frac{n}{3}}}\\Big\\}$  \n",
    "\n",
    "justification: working in logspace consider the concavitity of  \n",
    "https://www.wolframalpha.com/input/?i=2nd+derivative++x+log(1%2Fx)%2B+log(1%2Fsqrt(x))\n",
    "for $x \\geq 1$.  From here wield Jensen's Inequality (with uniform distribution), or Schur concavity of the symmetric function to attain the above upper bound, which is the maximum attaninable value for values $\\in [1, n-2]$.   \n",
    "\n",
    "$= \\frac{1}{2\\pi}\\cdot \\sqrt{n} \\cdot n^n  \\cdot  \\Big\\{ \\frac{n^{-n}}{3^{-n}} \\frac{3^\\frac{3}{2}}{n^\\frac{3}{2}}\\Big\\}$  \n",
    "$= \\frac{1}{2\\pi}3^\\frac{3}{2}\\cdot  \\Big\\{ {3^{n}} \\frac{1}{n}\\Big\\}$  \n",
    "\n",
    "Taking this result and plugging it back in:  \n",
    "$ 3^{-n}\\cdot   \\text{  max:  }\\frac{n!}{k! j!(n-k-j)!}$   \n",
    "$\\sim 3^{-n} \\cdot \\frac{\\sqrt{2\\pi}}{\\sqrt{2\\pi }\\sqrt{2\\pi }\\sqrt{2\\pi}}\\cdot \\frac{\\sqrt{n}}{\\sqrt{jkm}} \\cdot \\frac{(n)^{n}}{(j)^{j}(k)^{k}(m)^{m}}$   \n",
    "$\\leq  3^{-n}\\cdot\\frac{1}{2\\pi}3^\\frac{3}{2}\\cdot  \\Big\\{ {3^{n}} \\frac{1}{n}\\Big\\}$  \n",
    "$ = \\frac{1}{2\\pi}3^\\frac{3}{2}\\cdot  \\frac{1}{n}$  \n",
    "\n",
    "hence we now know our upper bound   \n",
    "$\\leq 2^{-2n}\\binom{2n}{n}\\cdot \\big(3^{-n}  \\text{max:  }\\frac{n!}{k! j!(n-k-j)!}\\big)$  \n",
    "is $\\sim$ to a number that is bound above by  \n",
    "$\\leq \\Big\\{\\frac{1 }{\\sqrt{\\pi n}}\\Big\\}\\cdot \\Big\\{\\frac{1}{2\\pi}3^\\frac{3}{2}\\cdot  \\frac{1}{n}\\Big\\}$  \n",
    "$= \\frac{1 }{2}\\big(\\frac{3}{\\pi}\\big)^\\frac{3}{2}\\cdot \\big( \\frac{1}{n}\\big)^\\frac{3}{2}$  \n",
    "\n",
    "and summing over this upper bound (and ignoring the first two terms in the series), we have  \n",
    "\n",
    "$\\sum_{n=3}^\\infty \\frac{1 }{2}\\big(\\frac{3}{\\pi}\\big)^\\frac{3}{2}\\cdot \\big( \\frac{1}{n}\\big)^\\frac{3}{2} $   \n",
    "$= \\frac{1 }{2}\\big(\\frac{3}{\\pi}\\big)^\\frac{3}{2}\\cdot  \\sum_{n=1}^\\infty \\big(\\frac{1}{n}\\big)^\\frac{3}{2}$  \n",
    "$\\lt \\infty$  \n",
    "\n",
    "Hence by Borel-Cantelli (or Markov Inequality or the underlying geometric distribution of renewals), we've proven that the 3-d simple random walk is a transient chain.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** remark** This automatically gives a proof of transience for all dimension $\\geq 3$.   In particular consider the 4 dimension case (and $d\\geq 5$ as an obvious extension).  Here we have  \n",
    "\n",
    "$\\mathbf v^{(0)} := \\begin{bmatrix}\n",
    "v_1\\\\ \n",
    "v_2\\\\ \n",
    "v_3\\\\\n",
    "v_4\\\\\n",
    "\\end{bmatrix}^{(0)}=\\begin{bmatrix} \n",
    "0\\\\ \n",
    "0\\\\ \n",
    "0\\\\\n",
    "0\\\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "and   \n",
    "$\\mathbf v^{(n)} = \\mathbf v^{(n-1)} + \\mathbf x^{(n)} $  \n",
    "\n",
    "where with uniform probability of $\\frac{1}{8}$ we have $\\mathbf x$ takes on values of $\\{\\mathbf e_1, - \\mathbf e_1, \\mathbf e_2, - \\mathbf e_2, \\mathbf e_3, - \\mathbf e_3, \\mathbf e_4, -\\mathbf e_4 \\}$  (where $\\mathbf e_j$ is the 3-d standard basis vector).  \n",
    "\n",
    "But over any and all sample paths, this is dominanted by \n",
    "where with uniform probability of $\\frac{1}{8}$ we have $\\mathbf x$ takes on values of $\\{\\mathbf e_1, - \\mathbf e_1, \\mathbf e_2, - \\mathbf e_2, \\mathbf e_3, - \\mathbf e_3, \\mathbf 0, -\\mathbf 0\\}$  \n",
    "because anytime the former has a renewal $\\subset$  any time the latter has a renewal.  This is a stochastic dominance relation.  \n",
    "\n",
    "But this 'new' model is equivalent to doing nothing $\\frac{1}{4}$ of the time and $\\frac{3}{4}$ of the time choosing one of the non-zero vectors which are equally likely -- note these may be equivalently interpretted as being in $\\mathbb R^3$ or $\\mathbb R^4$ -- the renewal occurs the same in each case.  However, is equivalent to our earlier problem in $d = 3$, except all transition probabilities have been decremented pro-rata, so that a selfloop exists at each state.  However, we know that while self loops may impact periodicity, they do not impact transience.  We can argue this two different ways:  \n",
    "\n",
    "first, our prior $d=3$ results tells us that upon leaving the origin (and by symmetry this applies to each exit path) there is some non-zero probability of never returning.  Having a self-loop means our exit from the origin is geometrically distributed -- but exit occurs WP1, and then we still have some non-zero probability of never returning for each exit point and hence the origin is still transient.  \n",
    "\n",
    "Alternatively referencing the criterion at the top of Feller vol 1 (3rd ed) page 402, if we consider our (countably infinite) 3-d transition matrix, $\\mathbf P$ *but we delete the row and column associated with the origin and call that matrix * $\\mathbf Q$, we know the origin is not persistent (aka recurrent) **iff** there is some \n",
    "\n",
    "$\\mathbf x \\neq \\mathbf 0$ such that $\\mathbf {Qx} = \\mathbf x$ where each $x_i \\in [0,1]$  (equivalently -- up to rescaling we can say that the x_i are non-negative and bounded above). Our earlier results proved that this $\\mathbf x$ exists.  The new chain with self loops added to each state is given by $\\frac{3}{4}\\mathbf P + \\frac{1}{4}\\mathbf I$, which, with the origin's row and column deleted is  \n",
    "\n",
    "$\\big(\\frac{3}{4}\\mathbf Q + \\frac{1}{4}\\mathbf I\\big)$  \n",
    "\n",
    "but  \n",
    "$\\big(\\frac{3}{4}\\mathbf Q + \\frac{1}{4}\\mathbf I\\big)\\mathbf x$  \n",
    "$= \\frac{3}{4}\\mathbf Q\\mathbf x + \\frac{1}{4}\\mathbf I\\mathbf x$  \n",
    "$= \\frac{3}{4}\\mathbf x + \\frac{1}{4}\\mathbf x$  \n",
    "$\\mathbf x$  \n",
    "\n",
    "and hence this chain has the origin as a transient state.  This means that we have a finite expected number of renewals in our dominating chain which means the 4d chain has an expected number of renewals bounded above by some finite number, and hence the 4d chain is transient too.   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**10 might be interesting **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13  \n",
    "Hastings-Metopolis (countable state chain)  \n",
    "*problem:*  \n",
    "given an irreducible Markov Chain with transition probabilities $P_{i,j}$ and *any* desired probability vector $\\pi$ (not just the stationary distribution for $P$) show that the Markov Chain with transition probabilities \n",
    "\n",
    "$Q_{i,j} = \\min\\big(P_{i,j}, \\frac{\\pi_j}{\\pi_i} P_{j,i}\\big)$ if $i\\neq j$   and  \n",
    "$Q_{i,i} = 1 - \\sum_{j=\\neq i} Q_{i,j}$  \n",
    "which is to say that the diagonal elements are 'filler' to make sure that $Q$ is row stochastic.  \n",
    "\n",
    "*remark:*  \n",
    "We can get some additional insights when $P$ is already symmetric (which is common in applications) or perhaps already 'merely' reversible-- in this case Metropolis Hastings can be interpretted as a form of tilting.  Using the transition matrix $Q$ we can see that under our original transition probabilities we go from $i \\to j$, and if this is chosen, we 'accept' this transition with probability $1$ if $p_j \\geq \\pi_i$, otherwise we 'toss a coin' that has probability $\\frac{\\pi_j}{\\pi_i}$ of heads -- if the coin toss result is heads we progress to state $j$ and if it is tails, we stay at state $i$.  \n",
    "\n",
    "*solution:*  \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "357, 407\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 \n",
    "\n",
    "There are various strategies to this problem.  One is guess the equilibrium distribution (in full, with normalizing constant), then show that chain is time reversible and consistent with this distribution.  An alternative approach is to show the time reversibility, and then via inspiration or perhaps very small exampes (m=2 and m=3 say) get a handle on the equilibrium -- and since it is well known, either look it up or use a combinatorial identity to get the normalizing constant.  \n",
    "\n",
    "The key idea is that time reversibility is a super-power, so we want to prove\n",
    "\n",
    "$\\pi_iP_{i,j} = \\pi_jP_{j,i} $ \n",
    "\n",
    "Your author originally tripped up (and fell) on indexing and state labelling issues.  Thankfully a solution to model from was found -- and the indexing modelling is awfully similar to that found in e.g. CS-Masterclass for elementary symmetric functions.  \n",
    "\n",
    "\n",
    "so a better approach is to map the above to   \n",
    "$\\pi(n_1, ...,n_i,..., n_j,....n_m) P_{i,j} = \\pi(n_1, ...,n_{i-1},..., n_{j +1},....n_m)P_{j,i} $   \n",
    "where the transition $P_{i,j}$ is not per se an exact index component of the transition matrix, but instead is interpretted as one ball from urn $i$ is taken and palced in urn $j$.  \n",
    "\n",
    "so given the problem specifications we have   \n",
    "$\\pi(n_1, ...,n_i,..., n_j,....n_m) P_{i,j}  $  \n",
    "$=\\pi(n_1, ...,n_i,..., n_j,....n_m) \\big(\\frac{n_i}{n}\\cdot\\frac{1}{M-1}\\big)  $  \n",
    "$= \\pi(n_1, ...,n_{i-1},..., n_{j +1},....n_m)\\big(\\frac{n_j+1}{n}\\cdot \\frac{1}{M-1}\\big)$  \n",
    "$= \\pi(n_1, ...,n_{i-1},..., n_{j +1},....n_m)P_{j,i} $   \n",
    "\n",
    "which, after simplifying, becomes:  \n",
    "$=\\pi(n_1, ...,n_i,..., n_j,....n_m) \\big(n_i\\big)  = \\pi(n_1, ...,n_{i-1},..., n_{j +1},....n_m)\\big(n_{j+1}\\big)$  \n",
    "\n",
    "from here, via pattern recognition or inspired guessing (or perhaps working through $n=2$ and $n=3$ cases in full detail) we are supposed to recognize the outlines of the multinomial distribution and see that \n",
    "\n",
    "$\\pi(n_1, ...,n_i,..., n_j,....n_m) \\propto \\frac{M!}{n_1!\\cdot n_2!...\\cdot n_m!}$  \n",
    "\n",
    "so substituting that in, we have   \n",
    "$\\frac{M!}{n_1! ...  n_{i}!...\\cdot n_m!} \\cdot n_i = \\frac{M!}{n_1!... (n_i-1)!...(n_j+1)!\\cdot n_m!}\\cdot n_{j+1} $  \n",
    "which simplifies to \n",
    "\n",
    "$\\frac{1}{(n_{i}-1)!n_j!} = \\frac{1}{(n_i-1)!n_j!} $  \n",
    "as desired  \n",
    "\n",
    "recognizing that our labelling of the urns was arbitrary (a graph isomorphism) that doesn't change the probability mapping, we see a symmetric function and hence recognize that the normalizing constant for our multinomial distribution is \n",
    "\n",
    "$\\big(\\frac{1}{m}\\big)^M$   \n",
    "\n",
    "with a steady state distribution of  \n",
    "$\\pi(n_1, ...,n_i,..., n_j,....n_m) = \\frac{M!}{n_1!\\cdot n_2!...\\cdot n_m!}\\big(\\frac{1}{m}\\big)^M$  \n",
    "\n",
    "\n",
    "**remark:**  \n",
    "The normalizing constant is nice to have for completeness... however we have a finite number of states, so we know that it exists (and that the underlying chain is positive recurrent).  One of the ideas from exercises 13 and 16 is that we frequently don't needed the normalizing constant (just want that it is finite) -- the values of $\\frac{\\pi_j}{\\pi_i}$, equivalently that $\\mathbf \\pi \\propto \\mathbf x$ is what matters.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
