{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem's official solution is found here: https://fivethirtyeight.com/features/can-you-rule-riddler-nation/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As I noted in the comments here, http://www.laurentlessard.com/bookproofs/microorganism-multiplication/\n",
    "\n",
    "this problem can also be interpretted as being the Cliff-Hanger (problem 35) in Mosteller's Fifty Challenging Problems in Probability.  \n",
    "\n",
    "As stated in Mosteller, the problem reads \n",
    "\n",
    "> From where he stands, one step twoard the cliff would send the drunken man over the edge.  He takes random steps, either toward or away from the cliff. At any step his probability of taking a step away is $\\frac{2}{3}$, of a step toward the cliff $\\frac{1}{3}$.  What is his chance of escaping the cliff?\"  \n",
    "\n",
    "The problem then goes on to generalize for any arbitrary probability of p and 1 - p.\n",
    "\n",
    "(Note that this problem also shows up in a *slightly* different guise in Gallager's *Stochastic Processes*, first as \"Example 5.5.4 (Stop when you are ahead in coin tossing)\", in the section on renewal theory and stopping trials.  It appears later in countable state markov chains, and finally in the section on martingales and randomwalks -- as of the time of this posting, I have only read through the renewal theory section though.   \n",
    "\n",
    "I start by taking a radically simpler approach than what I've seen elsewhere for the cliff-hanger problem -- consider a simple finite iterative.  Then consider limmitting behavior at the end with the help of the weak law of large numbers.  \n",
    "\n",
    "--------\n",
    "I start by noting that the answer of 100% chance of moving toward the cliff clearly leads to falling over the cliff with certainty.  And the case of 0% chance of stepping toward the cliff leads to sure escape. These are degenerate probabilities, and not particularly interesting.  Thus for the rest of this posting, I assert that $p \\neq 0$ and $p \\neq 1$.  We can simply eyeball those cases.  **I will also suggest that we can think of this game (the 'Cliff-Hanger game') as being a finite iterative one having n + 2 states: where the different states** are: $\\{-1, 0, 1, 2, ..., n-1, n\\}$, **and we play it for n iterations**.  (E.g. if n = 1, we have states {-1, 0, 1,}, we start at state 0, and play for 1 iteration.) In all cases we start at state zero.  If we ever visit state $-1$ during an iteration, the game is a failure.  \n",
    "\n",
    "The final game that is posed then asked for what happens $\\lim_{n \\to \\infty}$.  Note that the official problem merely suggests starting with countably infinite number of states, but I generally find that starting with a finite number and considering the limitting process can help clarify your thinking.  (As I recall Jaynes would *insist* that we try to do this.)  If people are worried about pathologies, they can consider approaching this limit from different orientations to confirm.  \n",
    "\n",
    "For my setup, there are some underlying ideas of linear independence and requiring the possibility of traversing the graph for n iterations without having a cycle that motivate this approach for this problem. (That is, for a finite graph, the state n will in effect be viewed as an absorbing state.  But since we always start at state 0 and only allow n iterations, we can never hit state n on anything except the final iteration -- so transition probabilities from state n are not relevant, by design.  If we were for some reason starting on state 1 instead, I would insist that the there be n + 3 states with n iterations... in any case the translation needed to accomodate a different starting point, or different failure point is quite simple.)  \n",
    "\n",
    "**The interesting thing about this approach is it clarifies the mechanics of what is going on in the process --- in effect it acts like a decomposition.  **\n",
    "\n",
    "So first, consider our expected position after n iterations.  Note that expected position change per iteration is $2p - 1$, and we have $n$ i.i.d. processes going on here.  It's not really needed, but I'll mention that the variance of position change per iteration is given by $4(p - p^2)$.\n",
    "\n",
    "The expected position / drift, at $n$ iterations is given by $\\mu = n\\big(1*p + -1*(1-p)\\big) = n (2p - 1)$. Similarly we can say that the expected position is given by $\\mu = ExpectedUpSteps - ExpectedDownSteps$, where $np = ExpectedUpSteps$ and $n(1-p) = ExpectedDownSteps$.  There is probably a touch more care needed at this stage than I'm willing to put in this post, to highlight that I will also look at $\\bar X$ which is the average result per iteration (aka $\\frac{\\mu}{n}$). For the most part, using $\\bar X$ is preferable to $\\mu$ because it converges -- but I'll toggle the representation as expediency indicates I should. I'm inclined to call $\\mu$ drift and $\\bar X$ average drift.  \n",
    "\n",
    "From here we can simply use the weak law of large numbers and notice that as n grows large, there is a vanishly small chance of having an average move size other than $\\bar X$.\n",
    "\n",
    "Thus we can decompose this problem into $(a)$ looking at the average drift of the process and $(b)$ looking at the chances of hitting the state $\\{-1\\}$ during an intermediate step.  Loosely speaking $(a)$ refers to long-run behavior, while $(b)$ is oriented to problems that occur in the short-run.  Our interest is solely what happens when $n$ grows large (and not, for instance, in the rate of this convergence). \n",
    "\n",
    "From this we can immediately see from our formula $\\bar X = (2p - 1) \\lt 0$, if $p \\lt 0.5$. From $(a)$ alone we can see that the probability of failure goes to 1 as $n$ grows large, for any $p \\lt 0.5$.  Put most simply, and accurately, if our average results converge on $\\bar X$, and $\\bar X \\lt 0$ then that implies we must have more -1s than +1s in our sample path at some point.  This means a failure must have occurred.  The intricacies of $(b)$ simply are not relevant here.  (The underlying idea I want to emphasize here is that the long-run behavior given by component $(a)$-- the average drift-- dominates and causes failure for $p \\lt 0.5$.)  \n",
    "- - - - \n",
    "\n",
    "Now consider the case where $p \\geq 0.5$.  The $p = 0.5$ case indicates zero drift, and the $p \\gt 0.5$ case indicates positive drift.  In these cases, $\\mu$ alone won't cause failure as n grows large, so we need to consider $(b)$.  What's interesting is that $(b)$ is a ballot problem!  Put differently, we can use the reflection principle here.  A sketch follows:\n",
    "\n",
    "where you have $ r = StepsDown$, $k = StepsUp$ and $s = DownStepsTillFailureGivenStart$.  Note for our problem $s := 1$ but I'll consider a more general formula. \n",
    "\n",
    "The idea is that for any given full iteration of the model, total moves $ = n = r + k$.  There are $\\binom{n}{k} = \\binom{r + k}{k}$ ways to choose any particular sequence of moves.  So given that you are in some particular position (specifically your expected position), the probability of failure is given by the number of moves that cause failure and get you to your position divided by total number of possible ways to get to your position. \n",
    "\n",
    "The formula for this is: $\\binom{r + k}{r - s} \\binom{r + k}{k} ^{-1} = \\frac{\\binom{r + k}{r - s}}{\\binom{r + k}{k}}$  \n",
    "\n",
    "Since in our case $s$ is always equal to 1 (i.e. we always start at state zero, and a failure is one downward step away), we can rewrite this as:\n",
    "\n",
    "$\\binom{r + k}{r - 1} \\binom{r + k}{k} ^{-1} = \\frac{k!r!}{(r - 1)! (k+1)!} = \\frac{r}{k+1}$\n",
    "\n",
    "\n",
    "so the formula for probability of failure in the case of non-negative drift is given by: $\\frac{r}{k+1}$\n",
    "\n",
    "It's worth highlighting that, for ballot problems the size of r and k matter!  Inserting something akin to $\\bar X$ instead of $\\mu$ does not line up with the physical meaning of the ballot problem.  It truly wants the total number of yays vs nays that exist (for any given n).  \n",
    "\n",
    "Now it's a matter of considering that as n grows large, the average value we see with probability one is $\\bar X$, and we can decompose this into the average of up steps and average downs steps.  To align with the physical meaning of the ballot problem, we will scale these values up appropriately (i.e. by $n$).  Thus we make the following two assignments:\n",
    "\n",
    "$r:= ExpectedDownsteps = n(1-p) $\n",
    "\n",
    "$k:= ExpectedUpsteps = np$\n",
    "\n",
    "$\\frac{(r)}{(k)+1} = \\frac{\\big(n(1-p)\\big)}{\\big(np \\big)+1}$.  \n",
    "\n",
    "Again recalling that I've banished degenerate probabilities from this post, distilling the answer just requires a simple limit:\n",
    "\n",
    "$Probability_{Failure} \\big|  p \\geq 0.5 = \\lim_{n \\to \\infty} \\frac{n(1-p)}{np+1} = \\lim_{n \\to \\infty}\\frac{(1-p)}{p + \\frac{1}{n}} = \\lim_{n \\to \\infty}\\frac{(1-p)}{p + O \\big(\\frac{1}{n}\\big)} = \\frac{1-p}{p}$\n",
    "\n",
    "From here it's just a small bit of work to consider the general case of $s$ moves until failure from a given start.  For any natural number $s \\geq 1$, this general failure probability equals\n",
    "\n",
    "$Probability_{Failure} \\big|  p \\geq 0.5 = \\lim_{n \\to \\infty}\\frac{n^s(1-p)^s + O\\big(n^{s-1}\\big)}{n^s p^s + O\\big(n^{s-1}\\big) } = \\lim_{n \\to \\infty}\\frac{(1-p)^s + O \\big(\\frac{1}{n}\\big)}{p^s + O \\big(\\frac{1}{n}\\big)} = \\Big(\\frac{1-p}{p}\\Big)^s$\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Post-script: **\n",
    "A more technically accurate way to deal with the ballot problem at the end of my writeup when I said: \n",
    "$\\binom{r + k}{r - 1} \\binom{r + k}{k} ^{-1} = \\frac{k!r!}{(r - 1)! (k+1)!} = \\frac{r}{k+1}$\n",
    "\n",
    "or in general\n",
    "$\\binom{r + k}{r - s} \\binom{r + k}{k} ^{-1} = \\frac{k!r!}{(r - s)! (k+s)!} = \\frac{r^s + O(r^{s-1})}{k^s + O(k^{s-1})}$\n",
    "\n",
    "From here observe that we are doing this ballot problem for a sequence of modified bernouli trials ($\\{-1,1\\}$ vs $\\{0,1\\}$ is just a small boolean modification).  But what the ballot formula tells us, is for any sequence of iid bernouli-ish trials, with a stationary lower 'knockout' barrier, as $n$ grows large, all that matters is the ratio between $r$ and $s$.  That is\n",
    "\n",
    "$\\lim_{n \\to \\infty} \\frac{r^s + O(r^{s-1})}{k^s + O(k^{s-1})} =\\big( \\frac{r}{k}\\big)^s $\n",
    "\n",
    "To justify this, observe first that $n = k + r$, where $k \\geq 0$, $r \\geq 0$.  Thus for $n$ to grow large, at least one of $k$ and $r$ must grow large.  However, if one of $k$ or $r$ grows large, but not the other, then a contradiction (or really a zero probability event) is raised.  (It is perhaps worth mentioning that in the case of $r \\gt k$, we don't need the formula for the ballot problem -- quite simply if $r \\gt k$ then you have a negative number and any negative number indicates failure.  If this were a binomial option pricing model for barrier options this technical nit would be dealt with via an if statement.  In any case, if as n grows large, and $p \\gt 0.5$, then the probability of $\\frac{r}{k} \\geq 1$ goes to zero.)\n",
    "\n",
    "Specifically, we know $\\lim_{n \\to \\infty} Pr\\big\\{ \\big| SampleMean - \\bar X \\big| \\gt \\epsilon \\big\\} = 0$ for every $\\epsilon \\gt 0$, by Weak Law of Large Numbers.  Hence the only non-zero probability outcome event is when $k$ and and $r$ grow aribtrarily large as $n$ grows arbitrarily large (and do so in proportion to their respective probabilities).  \n",
    "\n",
    "So the idea that the ballot problem tells us, is that in the limit, the relative proportion between $r$ and $k$ is what matters.  Thankfully we know this is baked into $\\bar X$. This is particularly easy to see since we have binary payoffs.  Again, if our mixture of ups vs downs was anything other than p and 1-p, our sample mean in $\\lim_{n \\to \\infty}$ would not equal $\\bar X$.  Hence we can make a substitution into $\\frac{r}{k} = \\frac{1-p}{p}$, and consider in the general case that the answer is given by $\\Big( \\frac{1-p}{p}\\Big) ^s$, for any natural number $s \\geq 1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
