{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOTUS Notes**  \n",
    "*via conditional expectations*   \n",
    "\n",
    "*This is shown for scalar valued random variables though the result follows almost immediately when considered over n dimensional vector spaces for random vectors via essentially the same argument*   \n",
    "\n",
    "the conditional expectations approach is, where  \n",
    "$X = g(Y)$  \n",
    "\n",
    "$E_Y\\Big[E_X\\big[X\\big \\vert Y=y\\big] \\Big]$  \n",
    "$=E_Y\\Big[E\\big[X\\big \\vert Y=y\\big] \\Big]$  \n",
    "$= E\\Big[E\\big[X\\big \\vert Y=y\\big] \\Big]$  \n",
    "$ = E\\Big[X\\Big] $  \n",
    "$=E_X\\Big[X\\Big]$  \n",
    "\n",
    "in integral form, we have  \n",
    "$g(Y=y) = E\\big[X\\big \\vert Y=y\\big] = E_X\\big[X\\big \\vert Y=y\\big]= \\int_X x\\cdot p\\big(X=x\\vert Y= y\\big)dx$   \n",
    "\n",
    "so iterating the expectation we have  \n",
    "$E_Y\\Big[E_X\\big[X\\big \\vert Y=y\\big] \\Big]$       \n",
    "$= E\\Big[E\\big[X\\big \\vert Y=y\\big] \\Big]$  \n",
    "$= \\int_Y p(Y=y)\\big(\\int_X x\\cdot p\\big(X=x\\vert Y= y\\big)dx\\big)dy   $  \n",
    "$= \\int_Y \\int_X x\\cdot p\\big(X=x\\vert Y= y\\big)p(Y=y)dxdy   $  \n",
    "$= \\int_Y \\int_X x\\cdot p\\big(X=x, Y=y\\big)dxdy   $  \n",
    "$= \\int_X \\int_Y x\\cdot p\\big(X=x, Y=y\\big)dydx   $  (Fubini)  \n",
    "$= \\int_X x dx \\int_Y  p\\big(X=x, Y=y\\big)dy $  \n",
    "$= \\int_X x dx \\int_Y p\\big(X=x\\big) p\\big(Y=y \\big \\vert X=x \\big)dy$  \n",
    "$= \\int_X  x\\cdot p\\big(X=x\\big) dx \\int_Y  p\\big(Y=y \\big \\vert X=x \\big)dy$  \n",
    "$= \\int_X x \\cdot p\\big(X=x\\big) dx \\Big(\\int_Y  p\\big(Y=y \\big \\vert X=x \\big)dy\\Big)$  \n",
    "$= \\int_X x \\cdot p\\big(X=x\\big) dx \\Big(1\\Big)$  \n",
    "$= \\int_X x \\cdot p\\big(X=x\\big) dx$  \n",
    "$= E_X\\Big[X\\Big] $  \n",
    "$= E\\Big[X\\Big] $  \n",
    "\n",
    "recalling that an expectation is said to exist *iff* the convergence occurs absolutely (so for convergence issues we may assume WLOG that all values are real non-negative in the above integrals)    \n",
    "- - - -   \n",
    "\n",
    "now for LOTUS in particular, making use of $X = g(Y)$, we have  \n",
    "$E_Y\\Big[X\\Big]$  \n",
    "$=E_Y\\Big[g(Y)\\Big]$  \n",
    "$=\\int_Y  p\\big(Y=y\\big)\\cdot g(y) dy $  \n",
    "$=E_Y\\Big[E\\big[g(y)\\big \\vert Y=y\\big] \\Big]$  \n",
    "$=E_Y\\Big[E\\big[g(Y)\\big \\vert Y=y\\big] \\Big]$  \n",
    "$= E_Y\\Big[E_X\\big[X\\big \\vert Y=y\\big] \\Big]$         \n",
    "$= E_X\\Big[X\\Big]$  (by above result)  \n",
    "\n",
    "so  \n",
    "$E_Y\\Big[X\\Big] = E_X\\Big[X\\Big]$  \n",
    "because $X = g(Y)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fact 1**  \n",
    "The moment generating function, if it exists in some open interval around zero --i.e. $(-a,a)$ for $a\\gt 0$, uniquely characterizes the underlying random variable's CDF at all points of continuity (i.e. almost everywhere).  Typically proven via complex analysis.  Feller volume 2 has a proof of this using SLLN (with the aid of some poisson distribution embedding) for real-nonnegative random variables -- e.g. gamma distributions.  In the case of random vectors, the MGF is said to exist if it exists in some open interval around zero for each component; there are finitely many components, so we take the minimum amongst components and say it exists over $(-a_{min},a_{min})$ for the random vector.  The gaussian has super exponential decay in its pdf and hence exists over $(-\\infty, \\infty)$.     \n",
    "\n",
    "\n",
    "**Fact 2**  \n",
    "a multivariate gaussian random vector  \n",
    "$\\mathbf x =  \\begin{bmatrix} \n",
    "X_{1} \\\\ \n",
    "X_{2}\\\\ \n",
    "\\vdots \\\\ \n",
    "X_{n} \n",
    "\\end{bmatrix}$  \n",
    "has mutually independent components **iff** the covariance matrix is diagonal. \n",
    "\n",
    "Proofs:  \n",
    "i) there is a slick proof of this using information theory plus the Hadarmard determinant inequality.  \n",
    "ii)The result also follows because arbitrary independent random variables with a second moment have zero covariance, and if we compute the MGF for a multivariate gaussian with independent components then its covariance matrix shows up in the mgf via a quadratic form with a diagonal matrix but because the MGF uniquely characterizes a distribution, any gaussian with diagonal covariance matrix must have independent components.  Note: as proven in the \"fun with trace\" notebook in the Linear Algebra folder, two n x n real symmetric matrices are equivalent *iff* their quadratic forms are equivalent over all selections of $\\mathbf t \\in \\mathbb R^n$ )  \n",
    "\n",
    "\n",
    "**Fact 3**  \n",
    "The multivariate mgf is defined as  \n",
    "$M_{\\mathbf x}\\big(\\mathbf t\\big) = E\\big[e^{\\mathbf t^T \\mathbf x}\\big] = E_{\\mathbf x}\\big[e^{\\mathbf t^T \\mathbf x}\\big] = E_{\\mathbf x}\\big[f_{\\mathbf t}\\big(\\mathbf x\\big)\\big] = E\\big[f_{\\mathbf t}\\big(\\mathbf x\\big)\\big]$    \n",
    "where we can think of $f_{\\mathbf t}$ as the mapping, for any $\\mathbf t \\in \\mathbb R^n$ that has $\\mathbf x$ in its domain and it's image points are given by \n",
    "$\\exp\\big(\\sum_{i=1}^n t_i X_i\\big)$.  \n",
    "$f_{\\mathbf t}\\big(\\mathbf x\\big)$ is a new random variable but the expectation may be taken with respect to $\\mathbf x$ itself via LOTUS.  \n",
    "\n",
    "For the multivariate gaussian in particular the MGF is given by  \n",
    "$M_{\\mathbf x}\\big(\\mathbf t\\big) = E\\big[e^{\\mathbf t^T \\mathbf \\mu + \\frac{1}{2}\\mathbf t^T \\Sigma \\mathbf t} \\big]$    \n",
    "\n",
    "with  \n",
    "$\\mathbf \\mu = E\\big[\\mathbf x\\big]$   \n",
    "$\\Sigma = \\text{var}\\big(\\mathbf x\\big)$  \n",
    "(defined below in further detail)  \n",
    "\n",
    "**Fact 4**  \n",
    "The mean for a random vector $\\mathbf x$ is given by $E\\big[\\mathbf x\\big]$  \n",
    "where the expectation is taken component wise \n",
    "\n",
    "The (co)variance for such a random vector is given by  \n",
    "$\\text{var}\\big(\\mathbf x\\big) = E\\big[\\mathbf {xx}^T\\big] - E\\big[\\mathbf x\\big]E\\big[\\mathbf x\\big]^T$   \n",
    "again with the expectation being taken component wise.  In the case of zero mean random variable it is more succinctly given by $E\\big[\\mathbf {xx}^T\\big]$, which in general is the second moment.  \n",
    "\n",
    "if we have some linear mapping of our random vector to another random vector given by a matrix $\\mathbf A$ so   \n",
    "$\\mathbf v = A\\mathbf {x}$   \n",
    "via application of linearity of expectations the mean for $\\mathbf v$ is given by  \n",
    "$E\\big[\\mathbf v\\big] = E\\big[A\\mathbf x\\big] = A \\cdot E\\big[\\mathbf x\\big]$  \n",
    "\n",
    "The (co)variance of $\\mathbf v$ is given by first computing the second moment, so   \n",
    "$E\\big[\\mathbf {vv}^T\\big] = E\\big[A\\mathbf {xx}^T A^T\\big]$  \n",
    "\n",
    "however, making use of associativity and then linearity, we have  \n",
    "$E\\big[\\mathbf {vv}^T\\big]$  \n",
    "$=E\\Big[A\\mathbf {xx}^TA^T\\Big]$  \n",
    "$= E\\Big[\\big(A\\mathbf {xx}^T\\big)A^T\\Big]$  \n",
    "$= E\\Big[A\\mathbf {xx}^T\\Big]\\cdot A^T $  \n",
    "$= E\\Big[A\\big(\\mathbf {xx}^T\\big)\\Big]\\cdot A^T $  \n",
    "$= A \\cdot E\\Big[\\mathbf {xx}^T\\Big]\\cdot A^T $  \n",
    "\n",
    "- - - -  \n",
    "*begin first alternative proof* \n",
    "\n",
    "This can be done via a quadatric form argument and note that for *any* $\\mathbf u \\in \\mathbb R^n$  where   \n",
    "$\\mathbf w := A^T\\mathbf u$  \n",
    "we have again due to linearity of expectations  \n",
    "\n",
    "$\\mathbf u^T E\\big[\\mathbf {vv}^T\\big]\\mathbf u$  \n",
    "$= \\sum_{i=1}^n\\sum_{j=1}^n u_i u_j E[V_i V_j]$  \n",
    "$= \\sum_{i=1}^n\\sum_{j=1}^n  E[u_i V_i  u_jV_j]$  \n",
    "$= E\\big[ \\sum_{i=1}^n \\sum_{j=1}^n  u_i V_i  u_jV_j\\big]$  \n",
    "$= E\\big[ \\sum_{i=1}^n u_i V_i \\sum_{j=1}^n  u_jV_j\\big]$  \n",
    "$= E\\big[ \\big(\\sum_{i=1}^n u_i V_i\\big)\\big( \\sum_{j=1}^n  u_jV_j\\big)\\big]$  \n",
    "$= E\\big[ \\big(\\sum_{i=1}^n u_i V_i\\big)^2\\big]$  \n",
    "$= E\\big[\\big(\\mathbf {u}^T\\mathbf {v}\\big)^2\\big]$  \n",
    "\n",
    "- - - - -  \n",
    "*interlude:*  \n",
    "if we stopped the argument here, and had instead run this on the centered random vector   \n",
    "$\\mathbf v^* := \\mathbf v - E\\big[\\mathbf v\\big]$   \n",
    "we'd have   \n",
    "$\\mathbf u^T \\Sigma_{\\mathbf v^*} \\mathbf u = \\mathbf u^T E\\big[\\mathbf {v^*v^*}^T\\big]\\mathbf u = E\\big[\\big(\\mathbf {u}^T\\mathbf {v}^*\\big)^2\\big]\\geq 0$  \n",
    "\n",
    "where the selection of $\\mathbf u$ was arbitrary.  This proves that the covariance matrix $\\Sigma_{\\mathbf v^*}$ for $\\mathbf v^*$ is positive (semi)definite.   \n",
    "\n",
    "*end interlude*    \n",
    "Returning to where we left off \n",
    "- - - - -    \n",
    "$= E\\big[\\big(\\mathbf {u}^T\\mathbf {v}\\big)^2\\big]$  \n",
    "$= E\\big[\\big(\\mathbf {u}^T (A\\mathbf {x})\\big)^2\\big]$  \n",
    "$= E\\big[\\big((\\mathbf {u}^T A)\\mathbf {x}\\big)^2\\big]$  \n",
    "$= E\\big[\\big(\\mathbf {w}^T \\mathbf {x}\\big)^2\\big]$  \n",
    "$= E\\big[ \\big(\\sum_{i=1}^n w_i X_i\\big)^2\\big]$  \n",
    "$= E\\big[ \\big(\\sum_{i=1}^n w_i X_i\\big)\\big( \\sum_{j=1}^n  w_jX_j\\big)\\big]$   \n",
    "$= E\\big[ \\sum_{i=1}^n w_i X_i \\sum_{j=1}^n  w_j X_j\\big]$   \n",
    "$= E\\big[ \\sum_{i=1}^n  \\sum_{j=1}^n  w_i X_i w_jX_j\\big]$    \n",
    "$= \\sum_{i=1}^n  \\sum_{j=1}^n E\\big[w_i X_i w_jX_j\\big]$  \n",
    "$= \\sum_{i=1}^n  \\sum_{j=1}^n w_i w_j E\\big[ X_i X_j\\big]$  \n",
    "$ =\\mathbf w^T E\\big[\\mathbf {xx}^T\\big]\\mathbf w$  \n",
    "$ =\\mathbf u^TA\\cdot E\\big[\\mathbf {xx}^T\\big]\\cdot A^T\\mathbf u$  \n",
    "\n",
    "\n",
    "hence   \n",
    "$A\\cdot E\\big[ \\mathbf {xx}^T\\big]\\cdot A^T= E\\big[A\\mathbf {xx}^TA^T\\big]  = E\\big[\\mathbf {vv}^T\\big] $  \n",
    "where this last equality comes from the fact that 2 real symmetric nxn matrices are equal *iff* they agree over all quadratic forms.  This is proven in the Linear Algebra folder in the notebook \"Fun_with_Trace_and_Quadratic_Forms_and_CauchySchwarz.ipynb\" about half the way down (do a find / 'ctrl + f' on the words 'Quadratic form equivalence between Hermitian matrices' to skip to it, though it uses the slightly more general hermitian matrices for scalars in $\\mathbb C$ and uses $^H$ to denote the conjugate transpose )   \n",
    "\n",
    "\n",
    "*end first alternative proof* \n",
    "- - - -  \n",
    "\n",
    "*begin second alternative proof* \n",
    "\n",
    "A 3rd way to prove the relation for the covariance matrix, is to linearize the bilinear map, via the use for a Kronecker product, and the Vec operator.  For more information, reference \"More on the Vec() Operator\" in the writeup \"Kronecker_Product.ipynb\" in the Linear Algebra folder.  \n",
    "\n",
    "$E\\Big[A\\mathbf {xx}^TA^T\\Big]$ \n",
    "\n",
    "corresponds with  \n",
    "$\\text{vec}\\Big( E\\big[ {A\\mathbf {xx}^T A^T}\\big]\\Big) $  \n",
    "$= E\\Big[\\text{vec}\\big( {A\\mathbf {xx}^T A^T}\\big)\\Big] $ (since expectation is taken component wise over the same components in each case)   \n",
    "$= E\\Big[\\big( A \\otimes  A\\big)\\text{vec}\\big(\\mathbf {xx}^T \\big)\\Big]$   \n",
    "$= \\big(A \\otimes  A\\big)E\\Big[\\text{vec}\\big(\\mathbf {xx}^T \\big)\\Big]$   which holds by our earlier result with linearity of expectations  \n",
    "$= \\big(A \\otimes  A\\big)\\text{vec}\\Big(E\\big[\\mathbf {xx}^T \\big] \\Big)$  \n",
    "\n",
    "but if we 'undo' the vec operator, we see that this implies   \n",
    "$E\\Big[A\\mathbf {xx}^TA^T\\Big] = A \\cdot E\\Big[\\mathbf {xx}^T\\Big]\\cdot A^T$  \n",
    "\n",
    "\n",
    "*end second alternative proof* \n",
    "- - - -  \n",
    "\n",
    "\n",
    "so we have \n",
    "\n",
    "$=\\text{var}\\big(A\\mathbf x\\big) $  \n",
    "$=\\text{var}\\big(\\mathbf v\\big) $  \n",
    "$= E\\big[\\mathbf {vv}^T\\big] - E\\big[\\mathbf {v}\\big]E\\big[\\mathbf {v}\\big]^T$  \n",
    "$= A \\cdot E\\big[\\mathbf {xx}^T\\big]\\cdot A^T -A\\cdot E\\big[\\mathbf {x}\\big]E\\big[\\mathbf {x}\\big]^T\\cdot A^T$  \n",
    "$= A \\cdot \\Big(E\\big[\\mathbf {xx}^T\\big]- E\\big[\\mathbf {x}\\big]E\\big[\\mathbf {x}\\big]^T\\Big)\\cdot A^T$  \n",
    "$= A \\cdot \\text{var}\\big(\\mathbf x\\big) \\cdot A^T$  \n",
    "\n",
    "- - - - -  \n",
    "note: in the slightly more general case of for some $\\mathbf b \\in \\mathbb R^n$  \n",
    "$\\mathbf v := A\\mathbf x + \\mathbf b$    \n",
    "we have   \n",
    "$E\\big[\\mathbf v\\big] = A\\cdot E\\big[\\mathbf x\\big] + E\\big[\\mathbf b\\big] = A \\cdot \\mathbf \\mu + \\mathbf b$    \n",
    "(where $\\mu \\in \\mathbb R^n$  though the bold doesn't show up that well)  \n",
    "\n",
    "so  \n",
    "$E\\big[\\mathbf v\\big]E\\big[\\mathbf v\\big]^T = A\\cdot E\\big[\\mathbf x\\big]E\\big[\\mathbf x\\big]^T A^T + A \\cdot E\\big[\\mathbf x\\big]E\\big[\\mathbf b\\big]^T + E\\big[\\mathbf b\\big]E\\big[\\mathbf x\\big]^T A^T + E\\big[\\mathbf b\\big]E\\big[\\mathbf b\\big]^T$  \n",
    "$= A\\cdot E\\big[\\mathbf x\\big]E\\big[\\mathbf x\\big]^T A^T +\\big\\{ A \\cdot E\\big[\\mathbf x\\big]\\cdot\\mathbf b^T + \\mathbf b\\cdot E\\big[\\mathbf x\\big]^T \\cdot A^T + \\mathbf b\\mathbf b^T\\big\\}$    \n",
    "\n",
    "and  \n",
    "$E\\big[\\mathbf {vv}^T\\big] = E\\Big[A\\mathbf {xx}^TA^T + A\\mathbf x\\mathbf b^T + \\mathbf b\\mathbf x^T A^T +\\mathbf b\\mathbf b^T  \\Big] $    \n",
    "$= E\\Big[A\\mathbf {xx}^TA^T\\Big] + E\\Big[\\big(A\\mathbf x\\big)\\mathbf b^T\\Big] + E\\Big[\\mathbf b\\big(\\mathbf x^T A^T\\big) \\Big] + E\\Big[ \\mathbf b\\mathbf b^T  \\Big] $    \n",
    "$= A\\cdot E\\Big[\\mathbf {xx}^T\\Big]\\cdot A^T + E\\Big[\\big(A\\mathbf x\\big)\\Big]\\mathbf b^T + \\mathbf b E\\Big[\\big(\\mathbf x^T A^T\\big) \\Big] + \\mathbf b\\mathbf b^T   $    \n",
    "$= A\\cdot E\\Big[\\mathbf {xx}^T\\Big]\\cdot A^T + A \\cdot E\\Big[\\mathbf x\\Big]\\mathbf b^T + \\mathbf b \\cdot E\\Big[\\mathbf x^T \\Big]\\cdot A^T + \\mathbf b\\mathbf b^T   $    \n",
    "$= A\\cdot E\\Big[\\mathbf {xx}^T\\Big]\\cdot A^T + \\big\\{A \\cdot E\\Big[\\mathbf x\\Big]\\cdot\\mathbf b^T + \\mathbf b \\cdot E\\Big[\\mathbf x^T \\Big]\\cdot A^T + \\mathbf b\\mathbf b^T  \\big\\} $    \n",
    "\n",
    "hence  \n",
    "$\\text{var}\\big(\\mathbf v\\big) = E\\big[\\mathbf {vv}^T\\big] - E\\big[\\mathbf v\\big]E\\big[\\mathbf v\\big]^T = A\\cdot E\\Big[\\mathbf {xx}^T\\Big]\\cdot A^T - A\\cdot E\\big[\\mathbf x\\big]E\\big[\\mathbf x\\big]^T A^T = A\\Sigma A^T$  \n",
    "as we'd expect, because affine translations change the mean but not the variance of random variables  \n",
    "\n",
    "*remark*  \n",
    "If we consider  \n",
    "$\\mathbf v^* := \\mathbf v - E\\big[\\mathbf v\\big] = \\mathbf v + \\mathbf b$   \n",
    "we see that they have the same covariance matrix $\\mathbf \\Sigma$.  When we combine this with the earlier interlude, we've proven that every covariance matrix for a random vector is real symmetric positive (semi)definite. \n",
    "\n",
    "The converse that every real symmetric positive (semi)definite matrix is a covariance matrix for some (not unique) multivariate Gaussian distribution will be proven shortly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Claim  \n",
    "\n",
    "suppose we have a random vector $\\mathbf x$ that is multivariate normal.  We want to prove that \n",
    "\n",
    "$\\mathbf v := A\\mathbf x + \\mathbf b$    \n",
    "must be multivariate normal as well\n",
    "\n",
    "**proof:**  \n",
    "since $\\mathbf x$ is mutlivariate normal we know it has an MGF given by  \n",
    "\n",
    "$M_{\\mathbf x}\\big(\\mathbf t\\big)=E_{\\mathbf x}\\big[e^{\\mathbf t^T \\mathbf x}\\big] = e^{\\mathbf t^T \\mathbf \\mu + \\frac{1}{2}\\mathbf t^T \\Sigma \\mathbf t} $    \n",
    "\n",
    "where $\\mathbf t \\in \\mathbf R^n$ (i.e. there are no radius of convergence issues for gaussians)\n",
    "\n",
    "we can also write this as  \n",
    "$M_{\\mathbf x}\\big(\\mathbf s\\big)=E_{\\mathbf x}\\big[e^{\\mathbf s^T \\mathbf x}\\big] =e^{\\mathbf s^T \\mathbf \\mu + \\frac{1}{2}\\mathbf s^T \\Sigma \\mathbf s} $  \n",
    "for any $\\mathbf s \\in \\mathbb R^n$  \n",
    "\n",
    "to avoid degenerate cases (e.g. we may want to require the existence of a valid normalizing constant for our gaussian) we know that  \n",
    "$\\Sigma $ must be nonsingular, so it is real symmetric positive definite.    \n",
    "\n",
    "we diagonalize it as  \n",
    "$\\Sigma = QDQ^T$  \n",
    "with orthogonal matrix $Q$ and consider the random vector \n",
    "\n",
    "$\\mathbf y = Q^T \\mathbf x$  \n",
    "this has mean   \n",
    "$E\\big[ \\mathbf y\\big] = Q^T E\\big[ \\mathbf x\\big] = Q^T \\mathbf \\mu$  \n",
    "$\\text{var}\\big(\\mathbf y\\big) = Q^T \\text{var}\\big(\\mathbf x\\big) Q = Q^T \\Sigma Q = D$  \n",
    "\n",
    "we conjecture that this random vector is multivariate guassian with mutually independent components.  \n",
    "\n",
    "To prove this, consider the inverbile mapping in $\\mathbb R^n$  \n",
    "where  \n",
    "$\\mathbf t = Q \\mathbf s$  \n",
    "then  for *any* $\\mathbf t \\in \\mathbb R^n$  \n",
    "\n",
    "$M_{\\mathbf x}\\big(\\mathbf t\\big)$  \n",
    "$= e^{\\mathbf t^T \\mathbf \\mu + \\frac{1}{2}\\mathbf t^T \\Sigma \\mathbf t}$  (we know $\\mathbf x$ has this form as a multivariate gaussian)   \n",
    "$= e^{(\\mathbf s^T Q^T) \\mathbf \\mu + \\frac{1}{2}(\\mathbf s^T Q^T) \\Sigma (Q \\mathbf s)} $  \n",
    "$= e^{\\mathbf s^T Q^T \\mathbf \\mu + \\frac{1}{2}\\mathbf s^T (Q^T \\Sigma Q) \\mathbf s} $  \n",
    "$= e^{\\mathbf s^T Q^T \\mathbf \\mu + \\frac{1}{2}\\mathbf s^T D \\mathbf s} $  (desired form)  \n",
    "$= E_{\\mathbf x}\\big[e^{\\mathbf t^T \\mathbf x}\\big] $  (definition for $M_{\\mathbf x}\\big(\\mathbf t\\big)$)  \n",
    "$= E_{\\mathbf x}\\big[e^{(\\mathbf s^T Q^T) \\mathbf x}\\big] $  \n",
    "$= E_{\\mathbf x}\\big[e^{\\mathbf s^T (Q^T \\mathbf x})\\big] $  \n",
    "$= E_{\\mathbf x}\\big[e^{\\mathbf s^T \\mathbf y}\\big]$  \n",
    "$= E_{\\mathbf y}\\big[e^{\\mathbf s^T \\mathbf y}\\big]$  (LOTUS)  \n",
    "$=M_{\\mathbf y}\\big(\\mathbf s\\big) $    \n",
    "for *any* $\\mathbf s \\in \\mathbb R^n$  \n",
    "\n",
    "hence we know $\\mathbf y$ is multivariable gaussian because its MGF is in the required form involving its means and variance and this uniquely specifies the distribution.    \n",
    "\n",
    "by *Fact 2*, since the covariance matrix for $\\mathbf y$ is diagonal $D$ we know its components are mutually independent.  \n",
    "\n",
    "*commentary*  \n",
    "The above structure is extremely close to a working argument for $\\mathbf v = A\\mathbf x + \\mathbf b$ however the matrix $A$ here may not be square and hence not invertible, so a bit more care is needed. With some cleverness we *could* resurrect the above approach to get the more general proof. \n",
    "\n",
    "However the above approach of mapping a multivariate gaussian to its independent 'equivalent' random vector is of particular interest in its own right *and* this approach allows us to use the independent case as a fundamental bridge between our starting case and any arbitrary transformation $\\mathbf x \\to \\mathbf v$ given by $A\\mathbf x + \\mathbf b$.  I.e in effect we proceed by considering:  $\\mathbf x \\to \\mathbf y \\to \\mathbf v$.  \n",
    "\n",
    "*remark*  \n",
    "(this is the converse to the remark stated just before this \"Main Claim\")  \n",
    "consider any $n$ x $n$ real symmetric positive (semi)definite matrix $\\mathbf A$.  We know it is orthogonaly diagonalizable \n",
    "$S = QDQ^T$  \n",
    "we may select $n$ mutually independent guassians with zero mean, and variances given by \n",
    "$d_{1,1}, d_{2,2},..., d_{n,n}$  \n",
    "this random vector is given by a random vector $\\mathbf y$ and by the above, we may find another vector of jointly normal random variables given by   \n",
    "\n",
    "$\\mathbf x = Q\\mathbf y$    \n",
    "but the above tells us  \n",
    "$\\text{var}\\big(\\mathbf x\\big) = Q \\text{var}\\big(\\mathbf x\\big) Q^T = Q DQ^T = S$  \n",
    "and we see that $S$ is a covariance matrix for some multivariate Gaussian.   \n",
    "\n",
    "Because the multivariate Gaussian is the maximum entropy distribution over all distribution (where variance is finite), this allows for some interesting links between matrix theory/inequalities and information theory.     \n",
    "\n",
    "- - - - \n",
    "**the close**  \n",
    "so consider  \n",
    "$\\mathbf v := A\\mathbf x + \\mathbf b$  \n",
    "by our earlier work we know  \n",
    "$E\\big[\\mathbf v \\big] =  A \\cdot E\\big[\\mathbf x \\big] + \\mathbf b = A \\cdot \\mathbf \\mu + \\mathbf b$  \n",
    "(again with $\\mathbf \\mu \\in \\mathbb R^n$ and constant vector $\\mathbf b \\in \\mathbb R^n$) \n",
    "and  \n",
    "\n",
    "$\\text{var}\\big(\\mathbf v\\big) = A\\Sigma A^T$   \n",
    "if we are concerned about degeneracies we may insist on $\\det\\big(A\\Sigma A^T\\big)\\gt 0$ which is the same as insisting on $A^T$ having full column rank, or equivalently $\\det\\big(A A^T\\big)\\gt 0 $, as we've already insisted that $\\Sigma$ is nonsingular.  \n",
    "\n",
    "So, we need to prove that the MGF of $\\mathbf v$ has the form  \n",
    "\n",
    "$M_{\\mathbf v}\\big(\\mathbf t\\big) =  e^{\\mathbf t^T (A\\mathbf \\mu + \\mathbf b) + \\frac{1}{2}\\mathbf t^TA\\Sigma A^T\\mathbf t} $  \n",
    "\n",
    "- - - - \n",
    "**proof:**    \n",
    "\n",
    "For any real valued $A$ where the product $A \\mathbf x$ is defined, consider  \n",
    "$C:= AQ $  \n",
    "\n",
    "in blocked form:  \n",
    "$C = \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf c_1 & \\mathbf c_2 &\\cdots & \\mathbf c_{n}\n",
    "\\end{array}\\bigg]\n",
    "$   \n",
    "\n",
    "This is equivalent to   \n",
    "$A = CQ^T$  \n",
    " \n",
    "using this we see   \n",
    "$M_{\\mathbf v}\\big(\\mathbf t\\big) $  \n",
    "$= E_{\\mathbf v}\\big[e^{\\mathbf t^T \\mathbf v}\\big]$  (definition of MGF)  \n",
    "$= E_{\\mathbf x}\\big[e^{\\mathbf t^T \\mathbf v}\\big] $  (LOTUS)  \n",
    "$= E_{\\mathbf x}\\big[e^{\\mathbf t^T (A\\mathbf x + \\mathbf b)}\\big] $  \n",
    "$= E_{\\mathbf x}\\big[e^{\\mathbf t^T (A)\\mathbf x + \\mathbf t^T\\mathbf b}\\big] $  \n",
    "$= E_{\\mathbf x}\\big[e^{\\mathbf t^T (C Q^T)\\mathbf x + \\mathbf t^T\\mathbf b}\\big] $  \n",
    "$= E_{\\mathbf x}\\big[e^{\\mathbf t^T C (Q^T\\mathbf x) + \\mathbf t^T\\mathbf b}\\big] $  \n",
    "$= E_{\\mathbf x}\\big[e^{\\mathbf t^T C \\mathbf y + \\mathbf t^T\\mathbf b}\\big] $  \n",
    "$= E_{\\mathbf y}\\big[e^{\\mathbf t^T C \\mathbf y + \\mathbf t^T\\mathbf b}\\big] $  (LOTUS)  \n",
    "$= E_{\\mathbf y}\\big[e^{\\mathbf t^T C \\mathbf y} e^{ \\mathbf t^T\\mathbf b}\\big] $   \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot E_{\\mathbf y}\\big[e^{\\mathbf t^T (C \\mathbf y)}\\big] $  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot E_{\\mathbf y}\\big[e^{\\mathbf t^T (\\sum_{k=1}^n Y_k \\mathbf c_k})\\big] $  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot E_{\\mathbf y}\\big[e^{\\sum_{k=1}^n Y_k (\\mathbf t^T\\mathbf c_k)}\\big] $  (linearity, recall $Y_k$ are scalar valued mutually independent gaussians)  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot E_{\\mathbf y}\\big[\\prod_{k=1}^n e^{ Y_k (\\mathbf t^T\\mathbf c_k)}\\big] $  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot \\prod_{k=1}^n E_{\\mathbf y}\\big[ e^{ Y_k (\\mathbf t^T\\mathbf c_k)}\\big] $  (by independence)  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot \\prod_{k=1}^n e^{ (\\mathbf t^T\\mathbf c_k) \\mu_{Y_k} + \\frac{1}{2}(\\mathbf t^T\\mathbf c_k)^2 \\sigma_{Y_k}^2} $  (mgf for scalar Gaussians $Y_k$ with $s := \\mathbf t^T\\mathbf c_k$)  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot e^{\\sum_{k=1}^n \\{ (\\mathbf t^T\\mathbf c_k) \\mu_{Y_k} + \\frac{1}{2}(\\mathbf t^T\\mathbf c_k)^2 \\sigma_{Y_k}^2\\}} $  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot e^{\\mathbf t^T C (\\mathbf \\mu_{\\mathbf y}) + \\frac{1}{2}\\mathbf t^T C D C^T \\mathbf t} $ (matrix algebra, recall that $d_{k,k} = \\sigma_{Y_k}^2$, note $\\mathbf \\mu_{\\mathbf y}\\in \\mathbf R^n$ with means for $\\mathbf y$)    \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot e^{\\mathbf t^T C  (Q^T \\mathbf \\mu) + \\frac{1}{2}\\mathbf t^T (C) D (C^T) \\mathbf t} $  (recall $\\mathbf \\mu \\in \\mathbb R^n$ with means for $\\mathbf x$)  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot e^{\\mathbf t^T C  Q^T \\mathbf \\mu + \\frac{1}{2}\\mathbf t^T (AQ) D (Q^T A^T) \\mathbf t} $  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot e^{\\mathbf t^T (C  Q^T) \\mathbf \\mu + \\frac{1}{2}\\mathbf t^T A(Q D Q^T) A^T \\mathbf t} $  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot e^{\\mathbf t^T A \\mathbf \\mu + \\frac{1}{2}\\mathbf t^T A \\Sigma A^T \\mathbf t} $  \n",
    "$=  e^{\\mathbf t^T\\mathbf b + \\mathbf t^T A \\mathbf \\mu + \\frac{1}{2}\\mathbf t^T A \\Sigma A^T \\mathbf t} $  \n",
    "$=  e^{\\mathbf t^T( A \\mathbf \\mu + \\mathbf b) + \\frac{1}{2}\\mathbf t^T A \\Sigma A^T \\mathbf t} $  \n",
    "as required  \n",
    "\n",
    "This proves that $\\mathbf v$ is a random vector for a multivariate gaussian.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an easy extension is if we have a multivariate n dimensional gaussian then the distribution of a $d$ dimensional random vector conditioned on the (WLOG the first) $n-d$ components is also multivariate Gaussian.  \n",
    "\n",
    "That is, \n",
    "with $ A \\in \\mathbb R^{(n-d) x n}$  \n",
    "$A = \\bigg[\\begin{array}{c|c|c|c}\n",
    "I_{n-d} & \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "we have \n",
    "\n",
    "$\\mathbf v = A\\mathbf x$  \n",
    "and $\\mathbf z$ is constructed to be the random variable conditioned on $\\mathbf v$.  We now prove $\\mathbf z$ has the multivariate normal distribution as well.  \n",
    "\n",
    "By our earlier results we see the MGFs of $\\mathbf x  $  and $\\mathbf v$ given by  \n",
    "\n",
    "$M_{\\mathbf x}\\big(\\mathbf t\\big) = e^{\\mathbf t^T \\mu + \\frac{1}{2}\\mathbf t^T \\Sigma \\mathbf t}$  \n",
    "$M_{\\mathbf v}\\big(\\mathbf t\\big) =  e^{\\mathbf t^T A \\mathbf \\mu  + \\frac{1}{2}\\mathbf t^T A \\Sigma A^T \\mathbf t}$    \n",
    "\n",
    "As a reminder, by definition of conditional independence for densities we have (with somewhat awkward notation)    \n",
    "$f_{\\mathbf v}(\\tilde{\\mathbf v}) f_{\\mathbf z}(\\tilde{\\mathbf z}) =  f_{\\mathbf x}(\\tilde{\\mathbf x})  $  \n",
    "and that same independence implies the product of their MGFs is given by   \n",
    "\n",
    "\n",
    "$M_{\\mathbf v}\\big(\\mathbf t\\big)M_{\\mathbf z}\\big(\\mathbf t\\big) =  M_{\\mathbf x}\\big(\\mathbf t\\big)$  \n",
    "\n",
    "so  \n",
    "$M_{\\mathbf z}\\big(\\mathbf t\\big) =  M_{\\mathbf x}\\big(\\mathbf t\\big)\\cdot M_{\\mathbf v}\\big(\\mathbf t\\big)^{-1}$  \n",
    "*but there are dimensional issues!*    \n",
    "$\\mathbf  t\\in \\mathbb R^{d}$ vs $\\mathbf  t\\in \\mathbb R^{n}$  and $\\mathbf t\\in \\mathbb R^{n-d}$  \n",
    "\n",
    "there are a few approaches forward.  First we show the case of passing through $\\mathbf y$ the independent case, and working through scalar gaussian mgfs.  An alternative approach is to use projectors and matrix factorizations, which is shown (in a somewhat ugly manner) as the second approach.  \n",
    "- - - -   \n",
    "\n",
    "*first  solution -- passing through independence*   \n",
    "\n",
    "as before  \n",
    "$\\mathbf y = Q^T\\mathbf x$  or  $Q\\mathbf y = \\mathbf x$  \n",
    "$\\mathbf v = A\\mathbf x  = AQ\\mathbf y = C\\mathbf y$    \n",
    "note with our definition, \n",
    "$(AQ)(AQ)^T = CC^T = I_{n-d}$   \n",
    "and of course \n",
    "$(AQ)^T (AQ) =C^TC = P_n$   \n",
    "where $P=P^2 = P^T$ (i.e. $P$ is a projector with rank n-d)  \n",
    "\n",
    "\n",
    "so  \n",
    "$M_{\\mathbf z}\\big(\\mathbf t\\big) $  \n",
    "$=  M_{\\mathbf x}\\big(\\mathbf t\\big)\\cdot M_{\\mathbf v}\\big(\\mathbf t\\big)^{-1}$  \n",
    "$?\\cdot \\big(\\prod_{k=1}^n E_{\\mathbf y}\\big[ e^{ Y_k (\\mathbf t^T\\mathbf c_k)}\\big]\\big)^{-1}$ (see 'by independence' in above final derivation)   \n",
    "# first solution approach abandoned as nothing clean came to mind...  \n",
    "- - - -   \n",
    "*second solution -- projectors*  \n",
    "a solution is to use   \n",
    "$A^* =A^TA = \\begin{bmatrix}\n",
    "I_{n-d} & \\mathbf 0_{n-d}\\mathbf 0_d^T \\\\ \n",
    "\\mathbf 0_d\\mathbf 0_{n-d}^T & (\\mathbf {00}^T)_d\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "$ A^*$ is symmetric and idempotent i.e. $A^* A^* = A^*$ \n",
    "so it is a projector \n",
    "\n",
    "noting that the complement is given by  \n",
    "$I_n - A^* = \\begin{bmatrix}\n",
    "(\\mathbf {00}^T)_d & \\mathbf 0_d\\mathbf 0_{n-d}^T\\\\ \n",
    "\\mathbf 0_{n-d}\\mathbf 0_d^T & I_{n-d} \n",
    "\\end{bmatrix}$  \n",
    "\n",
    "\n",
    "and observe\n",
    "\n",
    "$M_{\\mathbf v\\text{  old}}\\big(\\mathbf t\\big) = M_{\\mathbf v}\\big(\\mathbf t^*\\big)= e^{\\mathbf t^{*T} A^* \\mathbf \\mu  + \\frac{1}{2}\\mathbf t^{*T} A^* \\Sigma A^{*T} \\mathbf t^*} = e^{\\mathbf t^{*T} \\mathbf \\mu  + \\frac{1}{2}\\mathbf t^{*T}  \\Sigma  \\mathbf t^*}$     \n",
    "where nominally  \n",
    "$\\mathbf t^* \\in \\mathbb R^{n}$ but  \n",
    "$\\mathbf t^* = \\sum_{i=1}^{n-d} \\alpha_i \\mathbf e_i$    \n",
    "(it is constrained to be written as a linear combination of the 1st n-d standard basis vectors)  \n",
    "and the $\\mathbf t$ used in $M_{\\mathbf x}\\big(\\mathbf t\\big)$ is given by  \n",
    "$\\mathbf t  = \\mathbf t^* + \\sum_{i= n-d +1}^{n} \\alpha_i \\mathbf e_i$    \n",
    "note $\\mathbf A^* \\mathbf t = \\mathbf t^*$  and $\\mathbf A^* \\mathbf t^* = \\mathbf t^*$  \n",
    "\n",
    "\n",
    "$M_{\\mathbf z}\\big(\\mathbf s\\big) $  where nominally $\\mathbf s \\in \\mathbb R^n$  but it lives in a d dimensional subspace (shown at the end)    \n",
    "$= M_{\\mathbf x}\\big(\\mathbf t\\big)\\cdot M_{\\mathbf v}\\big(\\mathbf t^*\\big)^{-1} $  \n",
    "$= e^{\\mathbf t^T \\mu  + \\frac{1}{2}\\mathbf t^T \\Sigma \\mathbf t}e^{-\\mathbf t^{*T}  \\mathbf \\mu  - \\frac{1}{2}\\mathbf t^{*T} \\Sigma  \\mathbf t^*}$  \n",
    "$= e^{\\mathbf t^T \\mu - \\mathbf t^{*T} \\mathbf \\mu  + \\frac{1}{2}\\mathbf t^T \\Sigma \\mathbf t - \\frac{1}{2}\\mathbf t^{*T} \\Sigma \\mathbf t^*}  $  \n",
    "$= e^{\\mathbf t^T\\big(I_n - A^* + A^*\\big) \\mu - \\mathbf t^{*T} \\mathbf \\mu  + \\frac{1}{2}\\mathbf t^T\\big(I_n - A^* + A^*\\big)  \\Sigma \\big(I_n - A^* + A^*\\big) \\mathbf t - \\frac{1}{2}\\mathbf t^{*T} \\Sigma \\mathbf t^*}  $  \n",
    "$= e^{\\mathbf t^T\\big(I_n - A^*\\big)\\mu + \\big\\{\\mathbf t^T A^* \\mu - \\mathbf t^{*T}  \\mathbf \\mu \\big\\} + \\frac{1}{2}\\mathbf t^T\\big(I_n - A^* \\big)  \\Sigma \\big(I_n - A^* \\big) \\mathbf t + \\big\\{\\frac{1}{2}\\mathbf t^T\\big(A^* \\big)  \\Sigma \\big(A^* \\big) \\mathbf t - \\frac{1}{2}\\mathbf t^{*T} \\Sigma \\mathbf t^*\\big\\}} $  $ \\text{    note:  }\\big \\Vert \\big(I-A^* \\big)  \\Sigma \\big(A^* \\big) \\big \\Vert_F =0$  \n",
    "$= e^{\\mathbf t^T\\big(I_n - A^*\\big)\\mu + \\frac{1}{2}\\mathbf t^T\\big(I_n - A^* \\big)  \\Sigma \\big(I_n - A^* \\big) \\mathbf t }$  \n",
    "$= e^{\\mathbf s^T \\mu + \\frac{1}{2}\\mathbf s^T\\Gamma \\mathbf s }$    \n",
    "\n",
    "with  \n",
    "$\\mathbf s^T: = \\mathbf t^T \\big(I_n - A^*\\big)= \\mathbf t^T - \\mathbf t^{*T}  = \\sum_{i=n-d+1}^{n} \\alpha_i \\mathbf e_i^T $ \n",
    "\n",
    "where \n",
    "$\\Gamma \\in \\mathbb R^{d x d}$  \n",
    "$\\Gamma =  B \\Sigma B^T$  \n",
    "\n",
    "$B =  \\bigg[\\begin{array}{c|c|c|c} \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0 & I_{d}\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c} \\mathbf 0_{d}\\mathbf 0_{n-d}^T & I_{d}\\end{array}\\bigg] $   \n",
    "(n-d zero vectors, then the d dimensional identity matrix )    \n",
    "\n",
    "or we can have $\\mathbf s_\\text{smallr} \\in \\mathbb R^d$ and grab the bottom left square of $\\Sigma$, call this $\\Gamma_{\\text{small}}$ , do the analogous procedure with $\\mu_{\\text{small}}$ \n",
    "\n",
    "where  \n",
    "$G = \\begin{bmatrix}\\mathbf 0_{n-d}\\mathbf 0_{d}^T \\\\ I_{d}\\end{bmatrix}$  \n",
    "\n",
    "$G\\cdot \\mathbf s_{\\text{small}} = \\mathbf s$   \n",
    "\n",
    "\n",
    "and $G$ has full column rank, so for every $\\mathbf s_\\text{smaller}\\in \\mathbb R^d$ there is exacly one $\\mathbf s$  \n",
    "\n",
    "\n",
    "$M_{\\mathbf z}\\big(\\mathbf s_{\\text{small}}\\big) = e^{\\mathbf s_{\\text{small}}^T \\mu_{\\text{small}} + \\frac{1}{2}\\mathbf s_{\\text{small}}^T\\Gamma_{\\text{small}} \\mathbf s_{\\text{small}} }$      \n",
    "\n",
    "\n",
    "which shows that $\\mathbf z$ is a multivariate guassian, independent of $\\mathbf v$   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
