{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOTUS Notes**  \n",
    "*via conditional expectations*   \n",
    "\n",
    "*This is shown for scalar valued random variables though the result follows almost immediately when considered over n dimensional vector spaces for random vectors via essentially the same argument*   \n",
    "\n",
    "the conditional expectations approach is, where  \n",
    "$X = g(Y)$  \n",
    "\n",
    "$E_Y\\Big[E\\big[X\\big \\vert Y=y\\big] \\Big]$  \n",
    "$= E\\Big[E\\big[X\\big \\vert Y=y\\big] \\Big]$  \n",
    "$ = E\\Big[X\\Big] $  \n",
    "$E_X\\Big[X\\Big]$  \n",
    "\n",
    "in integral form, we have  \n",
    "$g(Y=y) = E\\big[X\\big \\vert Y=y\\big]= \\int_X x\\cdot p\\big(X=x\\vert Y= y\\big)dx$   \n",
    "\n",
    "so iterating the expectation we have  \n",
    "$E_Y\\Big[E\\big[X\\big \\vert Y=y\\big] \\Big]$    \n",
    "$= E\\Big[E\\big[X\\big \\vert Y=y\\big] \\Big]$  \n",
    "$= \\int_Y p(Y=y)\\big(\\int_X x\\cdot p\\big(X=x\\vert Y= y\\big)dx\\big)dy   $  \n",
    "$= \\int_Y \\int_X x\\cdot p\\big(X=x\\vert Y= y\\big)p(Y=y)dxdy   $  \n",
    "$= \\int_Y \\int_X x\\cdot p\\big(X=x, Y=y\\big)dxdy   $  \n",
    "$= \\int_X \\int_Y x\\cdot p\\big(X=x, Y=y\\big)dydx   $  (Fubini)  \n",
    "$= \\int_X x dx \\int_Y  p\\big(X=x, Y=y\\big)dy $  \n",
    "$= \\int_X x dx \\int_Y p\\big(X=x\\big) p\\big(Y=y \\big \\vert X=x \\big)dy$  \n",
    "$= \\int_X  x\\cdot p\\big(X=x\\big) dx \\int_Y  p\\big(Y=y \\big \\vert X=x \\big)dy$  \n",
    "$= \\int_X x \\cdot p\\big(X=x\\big) dx \\Big(\\int_Y  p\\big(Y=y \\big \\vert X=x \\big)dy\\Big)$  \n",
    "$= \\int_X x \\cdot p\\big(X=x\\big) dx \\Big(1\\Big)$  \n",
    "$= \\int_X x \\cdot p\\big(X=x\\big) dx$  \n",
    "$= E_X\\Big[X\\Big] $  \n",
    "$= E\\Big[X\\Big] $  \n",
    "\n",
    "recalling that an expectation is said to exist *iff* the convergence occurs absolutely (so for convergence issues we may assume WLOG that all values are real non-negative in the above integrals)    \n",
    "- - - -   \n",
    "\n",
    "now for LOTUS in particular, making use of $X = g(Y)$, we have  \n",
    "$E_Y\\Big[X\\Big]$  \n",
    "$=E_Y\\Big[g(Y)\\Big]$  \n",
    "$=\\int_Y  p\\big(Y=y\\big)\\cdot g(y) dy $  \n",
    "$=E_Y\\Big[E\\big[g(y)\\big \\vert Y=y\\big] \\Big]$  \n",
    "$=E_Y\\Big[E\\big[g(Y)\\big \\vert Y=y\\big] \\Big]$  \n",
    "= $E_Y\\Big[E\\big[X\\big \\vert Y=y\\big] \\Big]$  \n",
    "$= E_X\\Big[X\\Big]$  (by above result)  \n",
    "\n",
    "so  \n",
    "$E_Y\\Big[X\\Big] = E_X\\Big[X\\Big]$  \n",
    "because $X = g(Y)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fact 1**  \n",
    "The moment generating function, if it exists in some open interval around zero --i.e. $(-a,a)$ for $a\\gt 0$, uniquely characterizes the underlying random variable's CDF at all points of continuity (i.e. almost everywhere).  Typically proven via complex analysis.  Feller volume 2 has a proof of this using SLLN (with the aid of some poisson distribution embedding) for real-nonnegative random variables -- e.g. gamma distributions.  In the case of random vectors, the MGF is said to exist if it exists in some open interval around zero for each component; there are finitely many components, so we take the minimum amongs components and say it exists over $(-a_{min},a_{min})$ for the random vector.  The gaussian has super exponential decay in its pdf and hence exists over $(-\\infty, \\infty)$.     \n",
    "\n",
    "\n",
    "**Fact 2**  \n",
    "a multivariate gaussian random vector  \n",
    "$\\mathbf x =  \\begin{bmatrix} \n",
    "X_{1} \\\\ \n",
    "X_{2}\\\\ \n",
    "\\vdots \\\\ \n",
    "X_{n} \n",
    "\\end{bmatrix}$  \n",
    "has mutually independent components **iff** the covariance matrix is diagonal. \n",
    "\n",
    "Proofs:  \n",
    "i) there is a slick proof of this using information theory plus the Hadarmard determinant inequality.  \n",
    "ii)The result also follows because arbitrary independent random variables with a second moment have zero covariance, and if we compute the MGF for a multivariate gaussian with independent components then its covariance matrix shows up in the mgf via a quadratic form with a diagonal matrix but because the MGF uniquely characterizes a distribution, any gaussian with diagonal covariance matrix must have independent components.  Note: as proven in the \"fun with trace\" notebook in the Linear Algebra folder, two n x n real symmetric matrices are equivalent *iff* their quadratic forms are equivalent over all selections of $\\mathbf t \\in \\mathbb R^n$ )  \n",
    "\n",
    "\n",
    "**Fact 3**  \n",
    "The multivariate mgf is defined as  \n",
    "$M_{\\mathbf x}\\big(\\mathbf t\\big) = E\\big[e^{\\mathbf t^T \\mathbf x}\\big] = E_{\\mathbf x}\\big[e^{\\mathbf t^T \\mathbf x}\\big] = E_{\\mathbf x}\\big[f\\big(\\mathbf x\\big)\\big] = E\\big[f\\big(\\mathbf x\\big)\\big]$    \n",
    "where we can think of $f$ as the mapping that has $\\mathbf x$ in its domain and it's image points are given by \n",
    "$\\exp\\big(\\sum_{i=1}^n t_i X_i\\big)$.  \n",
    "$f\\big(\\mathbf x\\big)$ is a new random variable but the expectation may be taken with respect to $\\mathbf x$ itself via LOTUS.  \n",
    "\n",
    "For the multivariate gaussian in particular the MGF is given by  \n",
    "$M_{\\mathbf x}\\big(\\mathbf t\\big) = E\\big[e^{\\mathbf t^T \\mathbf \\mu + \\frac{1}{2}\\mathbf t^T \\Sigma \\mathbf t} \\big]$    \n",
    "\n",
    "with  \n",
    "$\\mathbf \\mu = E\\big[\\mathbf x\\big]$   \n",
    "$\\Sigma = \\text{var}\\big(\\mathbf x\\big)$  \n",
    "(defined below in further detail)  \n",
    "\n",
    "**Fact 4**  \n",
    "The mean for a random vector $\\mathbf x$ is given by $E\\big[\\mathbf x\\big]$  \n",
    "where the expectation is taken component wise \n",
    "\n",
    "The (co)variance for such a random vector is given by  \n",
    "$\\text{var}\\big(\\mathbf x\\big) = E\\big[\\mathbf {xx}^T\\big] - E\\big[\\mathbf x\\big]E\\big[\\mathbf x\\big]^T$   \n",
    "again with the expectation being taken component wise.  In the case of zero mean random variable it is more succinctly given by $E\\big[\\mathbf {xx}^T\\big]$, which in general is the second moment.  \n",
    "\n",
    "if we have some linear mapping of our random vector to another random vector given by a matrix $\\mathbf A$ so   \n",
    "$\\mathbf v = A\\mathbf {x}$   \n",
    "via application of linearity of expectations the mean for $\\mathbf v$ is given by  \n",
    "$E\\big[\\mathbf v\\big] = E\\big[A\\mathbf x\\big] = A \\cdot E\\big[\\mathbf x\\big]$  \n",
    "\n",
    "The (co)variance of $\\mathbf v$ is given by first computing the second moment, so   \n",
    "$E\\big[\\mathbf {vv}^T\\big] = E\\big[A\\mathbf {xx}^T A^T\\big]$  \n",
    "\n",
    "however, making use of associativity and then linearity, we have  \n",
    "$E\\big[\\mathbf {vv}^T\\big]$  \n",
    "$=E\\Big[A\\mathbf {xx}^TA^T\\Big]$  \n",
    "$= E\\Big[\\big(A\\mathbf {xx}^T\\big)A^T\\Big]$  \n",
    "$= E\\Big[A\\mathbf {xx}^T\\Big]\\cdot A^T $  \n",
    "$= E\\Big[A\\big(\\mathbf {xx}^T\\big)\\Big]\\cdot A^T $  \n",
    "$= A \\cdot E\\Big[\\mathbf {xx}^T\\Big]\\cdot A^T $  \n",
    "\n",
    "- - - -  \n",
    "an *alternative proof* \n",
    "\n",
    "This can be done via a quadatric form argument and note that for *any* $\\mathbf u \\in \\mathbb R^n$  where   \n",
    "$\\mathbf w := A^T\\mathbf u$  \n",
    "we have again due to linearity of expectations  \n",
    "\n",
    "$\\mathbf u^T E\\big[\\mathbf {vv}^T\\big]\\mathbf u$  \n",
    "$= \\sum_{i=1}^n\\sum_{j=1}^n u_i u_j E[V_i V_j]$  \n",
    "$= \\sum_{i=1}^n\\sum_{j=1}^n  E[u_i V_i  u_jV_j]$  \n",
    "$= E\\big[ \\sum_{i=1}^n \\sum_{j=1}^n  u_i V_i  u_jV_j\\big]$  \n",
    "$= E\\big[ \\sum_{i=1}^n u_i V_i \\sum_{j=1}^n  u_jV_j\\big]$  \n",
    "$= E\\big[ \\big(\\sum_{i=1}^n u_i V_i\\big)\\big( \\sum_{j=1}^n  u_jV_j\\big)\\big]$  \n",
    "$= E\\big[ \\big(\\sum_{i=1}^n u_i V_i\\big)^2\\big]$  \n",
    "$= E\\big[\\big(\\mathbf {u}^T\\mathbf {v}\\big)^2\\big]$  \n",
    "\n",
    "- - - - -  \n",
    "*interlude:*  \n",
    "if we stopped the argument here, and had instead run this on the centered random vector   \n",
    "$\\mathbf v^* := \\mathbf v - E\\big[\\mathbf v\\big]$   \n",
    "we'd have   \n",
    "$\\mathbf u^T \\Sigma_{\\mathbf v^*} \\mathbf u = \\mathbf u^T E\\big[\\mathbf {v^*v^*}^T\\big]\\mathbf u = E\\big[\\big(\\mathbf {u}^T\\mathbf {v}^*\\big)^2\\big]\\geq 0$  \n",
    "\n",
    "where the selection of $\\mathbf u$ was arbitrary.  This proves that the covariance matrix $\\Sigma$ for $\\mathbf v$ is positive (semi)definite.   \n",
    "\n",
    "*end interlude*    \n",
    "Returning to where we left off \n",
    "- - - - -    \n",
    "$= E\\big[\\big(\\mathbf {u}^T\\mathbf {v}\\big)^2\\big]$  \n",
    "$= E\\big[\\big(\\mathbf {u}^T (A\\mathbf {x})\\big)^2\\big]$  \n",
    "$= E\\big[\\big((\\mathbf {u}^T A)\\mathbf {x}\\big)^2\\big]$  \n",
    "$= E\\big[\\big(\\mathbf {w}^T \\mathbf {x}\\big)^2\\big]$  \n",
    "$= E\\big[ \\big(\\sum_{i=1}^n w_i X_i\\big)^2\\big]$  \n",
    "$= E\\big[ \\big(\\sum_{i=1}^n w_i X_i\\big)\\big( \\sum_{j=1}^n  w_jX_j\\big)\\big]$   \n",
    "$= E\\big[ \\sum_{i=1}^n w_i X_i \\sum_{j=1}^n  w_j X_j\\big]$   \n",
    "$= E\\big[ \\sum_{i=1}^n  \\sum_{j=1}^n  w_i X_i w_jX_j\\big]$    \n",
    "$= \\sum_{i=1}^n  \\sum_{j=1}^n E\\big[w_i X_i w_jX_j\\big]$  \n",
    "$= \\sum_{i=1}^n  \\sum_{j=1}^n w_i w_j E\\big[ X_i X_j\\big]$  \n",
    "$ =\\mathbf w^T E\\big[\\mathbf {xx}^T\\big]\\mathbf w$  \n",
    "$ =\\mathbf u^TA\\cdot E\\big[\\mathbf {xx}^T\\big]\\cdot A^T\\mathbf u$  \n",
    "\n",
    "\n",
    "hence   \n",
    "$A\\cdot E\\big[ \\mathbf {xx}^T\\big]\\cdot A^T= E\\big[A\\mathbf {xx}^TA^T\\big]  = E\\big[\\mathbf {vv}^T\\big] $  \n",
    "where this last equality comes from the fact that 2 real symmetric nxn matrices are equal *iff* they agree over all quadratic forms.  This is proven in the Linear Algebra folder in the notebook \"Fun_with_Trace_and_Quadratic_Forms_and_CauchySchwarz.ipynb\" about half the way down (do a find / 'ctrl + f' on the words 'Quadratic form equivalence between Hermitian matrices' to skip to it, though it uses the slightly more general hermitian matrices for scalars in $\\mathbb C$ and uses $^H$ to denote the conjugate transpose )   \n",
    "\n",
    "\n",
    "\n",
    "- - - -  \n",
    "\n",
    "so we have \n",
    "\n",
    "$=\\text{var}\\big(A\\mathbf x\\big) $  \n",
    "$=\\text{var}\\big(\\mathbf v\\big) $  \n",
    "$= E\\big[\\mathbf {vv}^T\\big] - E\\big[\\mathbf {v}\\big]E\\big[\\mathbf {v}\\big]^T$  \n",
    "$= A \\cdot E\\big[\\mathbf {xx}^T\\big]\\cdot A^T -A\\cdot E\\big[\\mathbf {x}\\big]E\\big[\\mathbf {x}\\big]^T\\cdot A^T$  \n",
    "$= A \\cdot \\Big(E\\big[\\mathbf {xx}^T\\big]- E\\big[\\mathbf {x}\\big]E\\big[\\mathbf {x}\\big]^T\\Big)\\cdot A^T$  \n",
    "$= A \\cdot \\text{var}\\big(\\mathbf x\\big) \\cdot A^T$  \n",
    "\n",
    "- - - - -  \n",
    "note: in the slightly more general case of for some $\\mathbf b \\in \\mathbb R^n$  \n",
    "$\\mathbf v := A\\mathbf x + \\mathbf b$    \n",
    "we have   \n",
    "$E\\big[\\mathbf v\\big] = A\\cdot E\\big[\\mathbf x\\big] + E\\big[\\mathbf b\\big] = A \\cdot \\mathbf \\mu + \\mathbf b$    \n",
    "(where $\\mu \\in \\mathbb R^n$  though the bold doesn't show up that well)  \n",
    "\n",
    "so  \n",
    "$E\\big[\\mathbf v\\big]E\\big[\\mathbf v\\big]^T = A\\cdot E\\big[\\mathbf x\\big]E\\big[\\mathbf x\\big]^T A^T + A \\cdot E\\big[\\mathbf x\\big]E\\big[\\mathbf b\\big]^T + E\\big[\\mathbf b\\big]E\\big[\\mathbf x\\big]^T A^T + E\\big[\\mathbf b\\big]E\\big[\\mathbf b\\big]^T$  \n",
    "$= A\\cdot E\\big[\\mathbf x\\big]E\\big[\\mathbf x\\big]^T A^T +\\big\\{ A \\cdot E\\big[\\mathbf x\\big]\\mathbf b^T + \\mathbf b\\cdot E\\big[\\mathbf x\\big]^T A^T + \\mathbf b\\mathbf b^T\\big\\}$    \n",
    "\n",
    "and  \n",
    "$E\\big[\\mathbf {vv}^T\\big] = E\\Big[A\\mathbf {xx}^TA^T + A\\mathbf x\\mathbf b^T + \\mathbf b\\mathbf x^T A^T +\\mathbf b\\mathbf b^T  \\Big] $    \n",
    "$= E\\Big[A\\mathbf {xx}^TA^T\\Big] + E\\Big[\\big(A\\mathbf x\\big)\\mathbf b^T\\Big] + E\\Big[\\mathbf b\\big(\\mathbf x^T A^T\\big) \\Big] + E\\Big[ \\mathbf b\\mathbf b^T  \\Big] $    \n",
    "$= A\\cdot E\\Big[\\mathbf {xx}^T\\Big]\\cdot A^T + E\\Big[\\big(A\\mathbf x\\big)\\Big]\\mathbf b^T + \\mathbf b E\\Big[\\big(\\mathbf x^T A^T\\big) \\Big] + \\mathbf b\\mathbf b^T   $    \n",
    "$= A\\cdot E\\Big[\\mathbf {xx}^T\\Big]\\cdot A^T + A \\cdot E\\Big[\\mathbf x\\Big]\\mathbf b^T + \\mathbf b \\cdot E\\Big[\\mathbf x^T \\Big]\\cdot A^T + \\mathbf b\\mathbf b^T   $    \n",
    "$= A\\cdot E\\Big[\\mathbf {xx}^T\\Big]\\cdot A^T + \\big\\{A \\cdot E\\Big[\\mathbf x\\Big]\\mathbf b^T + \\mathbf b \\cdot E\\Big[\\mathbf x^T \\Big]\\cdot A^T + \\mathbf b\\mathbf b^T  \\big\\} $    \n",
    "\n",
    "hence  \n",
    "$\\text{var}\\big(\\mathbf v\\big) = E\\big[\\mathbf {vv}^T\\big] - E\\big[\\mathbf v\\big]E\\big[\\mathbf v\\big]^T = A\\cdot E\\Big[\\mathbf {xx}^T\\Big]\\cdot A^T - A\\cdot E\\big[\\mathbf x\\big]E\\big[\\mathbf x\\big]^T A^T = A\\Sigma A^T$  \n",
    "as we'd expect, because affine translations change the mean but not the variance of random variables  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Claim  \n",
    "\n",
    "suppose we have a random vector $\\mathbf x$ that is multivariate normal.  We want to prove that \n",
    "\n",
    "$\\mathbf v := A\\mathbf x + \\mathbf b$    \n",
    "must be multivariate normal as well\n",
    "\n",
    "**proof:**  \n",
    "since $\\mathbf x$ is mutlivariate normal we know it has an MGF given by  \n",
    "\n",
    "$M_{\\mathbf x}\\big(\\mathbf t\\big)=E_{\\mathbf x}\\big[e^{\\mathbf t^T \\mathbf x}\\big] = e^{\\mathbf t^T \\mathbf \\mu + \\frac{1}{2}\\mathbf t^T \\Sigma \\mathbf t} $    \n",
    "\n",
    "where $\\mathbf t \\in \\mathbf R^n$ (i.e. there are no radius of convergence issues for gaussians)\n",
    "\n",
    "we can also write this as  \n",
    "$M_{\\mathbf x}\\big(\\mathbf s\\big)=E_{\\mathbf x}\\big[e^{\\mathbf s^T \\mathbf x}\\big] =e^{\\mathbf s^T \\mathbf \\mu + \\frac{1}{2}\\mathbf s^T \\Sigma \\mathbf s} $  \n",
    "for any $\\mathbf s \\in \\mathbb R^n$  \n",
    "\n",
    "to avoid degenerate cases (e.g. we may want to require the existence of a valid normalizing constant for our gaussian) we know that  \n",
    "$\\Sigma $ must be nonsingular, so it is real symmetric positive definite.    \n",
    "\n",
    "we diagonalize it as  \n",
    "$\\Sigma = QDQ^T$  \n",
    "with orthogonal matrix $Q$ and consider the random vector \n",
    "\n",
    "$\\mathbf y = Q^T \\mathbf x$  \n",
    "this has mean   \n",
    "$E\\big[ \\mathbf y\\big] = Q^T E\\big[ \\mathbf x\\big] = Q^T \\mathbf \\mu$  \n",
    "$\\text{var}\\big(\\mathbf y\\big) = Q^T \\text{var}\\big(\\mathbf x\\big) Q = Q^T \\Sigma Q = D$  \n",
    "\n",
    "we conjecture that this random vector is multivariate guassian with mutually independent components.  \n",
    "\n",
    "To prove this, consider the inverbile mapping in $\\mathbb R^n$  \n",
    "where  \n",
    "$\\mathbf t = \\mathbf Q \\mathbf s$  \n",
    "then  for *any* $\\mathbf t \\in \\mathbb R^n$  \n",
    "\n",
    "$M_{\\mathbf x}\\big(\\mathbf t\\big)$  \n",
    "$= e^{\\mathbf t^T \\mathbf \\mu + \\frac{1}{2}\\mathbf t^T \\Sigma \\mathbf t}$  \n",
    "$= e^{\\mathbf (s^T Q^T) \\mathbf \\mu + \\frac{1}{2}(\\mathbf s^T Q^T) \\Sigma (Q \\mathbf s)} $  \n",
    "$= e^{\\mathbf s^T Q^T \\mathbf \\mu + \\frac{1}{2}\\mathbf s^T (Q^T \\Sigma Q) \\mathbf s} $  \n",
    "$= e^{\\mathbf s^T Q^T \\mathbf \\mu + \\frac{1}{2}\\mathbf s^T D \\mathbf s} $  (desired form)  \n",
    "$= E_{\\mathbf x}\\big[e^{\\mathbf t^T \\mathbf x}\\big] $  \n",
    "$= E_{\\mathbf x}\\big[e^{(\\mathbf s^T Q^T) \\mathbf x}\\big] $  \n",
    "$= E_{\\mathbf x}\\big[e^{\\mathbf s^T (Q^T \\mathbf x})\\big] $  \n",
    "$= E_{\\mathbf x}\\big[e^{\\mathbf s^T \\mathbf y}\\big]$  \n",
    "$= E_{\\mathbf y}\\big[e^{\\mathbf s^T \\mathbf y}\\big]$  (LOTUS)  \n",
    "$=M_{\\mathbf y}\\big(\\mathbf s\\big) $    \n",
    "for *any* $\\mathbf s \\in \\mathbb R^n$  \n",
    "\n",
    "hence we know $\\mathbf y$ is multivariable gaussian because its MGF is in the required form involving its means and variance and this uniquely specifies the distribution.    \n",
    "\n",
    "by *Fact 2*, since the covariance matrix for $\\mathbf y$ is diagonal $D$ we know its components are mutually independent.  \n",
    "\n",
    "*commentary*  \n",
    "The above structure is extremely close to a working argument for $\\mathbf v = A\\mathbf x + \\mathbf b$ however the matrix $A$ here may not be square and hence not invertible, so a bit more care is needed. With some cleverness we *could* resurrect the above approach to get the more general proof. \n",
    "\n",
    "However the above approach of mapping a multivariate gaussian to its independent 'equivalent' random vector is of particular interest in its own right *and* this approach allows us to use the independent case as a fundamental bridge between our starting case and any arbitrary transformation $\\mathbf x \\to \\mathbf v$ given by $A\\mathbf x + \\mathbf b$.  I.e we proceed by considering:  $\\mathbf x \\to \\mathbf y \\to \\mathbf v$.  \n",
    "- - - - \n",
    "**the close**  \n",
    "so consider  \n",
    "$\\mathbf v := A\\mathbf x + \\mathbf b$  \n",
    "by our earlier work we know  \n",
    "$E\\big[\\mathbf v \\big] =  A \\cdot E\\big[\\mathbf x \\big] + \\mathbf b = A \\cdot \\mathbf \\mu + \\mathbf b$  \n",
    "(again with $\\mathbf \\mu \\in \\mathbb R^n$ and constant vector $\\mathbf b \\in \\mathbb R^n$) \n",
    "and  \n",
    "\n",
    "$\\text{var}\\big(\\mathbf v\\big) = A\\Sigma A^T$   \n",
    "if we are concerned about degeneracies we may insist on $\\det\\big(A\\Sigma A^T\\big)\\gt 0$ which is the same as insisting on $A^T$ having full column rank, or equivalently $\\det\\big(A A^T\\big)\\gt 0 $, as we've already insisted that $\\Sigma$ is nonsingular.  \n",
    "\n",
    "So, we need to prove that the MGF of $\\mathbf v$ has the form  \n",
    "\n",
    "$M_{\\mathbf v}\\big(\\mathbf t\\big) =  e^{\\mathbf t^T (A\\mathbf \\mu + \\mathbf b) + \\mathbf t^TA\\Sigma A^T\\mathbf t} $  \n",
    "\n",
    "- - - - \n",
    "**proof:**    \n",
    "\n",
    "For any real valued $A$ where the product $A \\mathbf x$ is defined, consider  \n",
    "$C:= AQ $  \n",
    "\n",
    "in blocked form:  \n",
    "$C = \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf c_1 & \\mathbf c_2 &\\cdots & \\mathbf c_{n}\n",
    "\\end{array}\\bigg]\n",
    "$   \n",
    "\n",
    "This is equivalent to   \n",
    "$A = CQ^T$  \n",
    " \n",
    "using this we see   \n",
    "$M_{\\mathbf v}\\big(\\mathbf t\\big) $  \n",
    "$= E_{\\mathbf v}\\big[e^{\\mathbf t^T \\mathbf v}\\big]$  (definition of MGF)  \n",
    "$= E_{\\mathbf x}\\big[e^{\\mathbf t^T \\mathbf v}\\big] $  (LOTUS)  \n",
    "$= E_{\\mathbf x}\\big[e^{\\mathbf t^T (A\\mathbf x + \\mathbf b)}\\big] $  \n",
    "$= E_{\\mathbf x}\\big[e^{\\mathbf t^T (A)\\mathbf x + \\mathbf t^T\\mathbf b}\\big] $  \n",
    "$= E_{\\mathbf x}\\big[e^{\\mathbf t^T (C Q^T)\\mathbf x + \\mathbf t^T\\mathbf b}\\big] $  \n",
    "$= E_{\\mathbf x}\\big[e^{\\mathbf t^T C (Q^T\\mathbf x) + \\mathbf t^T\\mathbf b}\\big] $  \n",
    "$= E_{\\mathbf x}\\big[e^{\\mathbf t^T C \\mathbf y + \\mathbf t^T\\mathbf b}\\big] $  \n",
    "$= E_{\\mathbf y}\\big[e^{\\mathbf t^T C \\mathbf y + \\mathbf t^T\\mathbf b}\\big] $  (LOTUS)  \n",
    "$= E_{\\mathbf y}\\big[e^{\\mathbf t^T C \\mathbf y} e^{ \\mathbf t^T\\mathbf b}\\big] $   \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot E_{\\mathbf y}\\big[e^{\\mathbf t^T (C \\mathbf y)}\\big] $  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot E_{\\mathbf y}\\big[e^{\\mathbf t^T (\\sum_{k=1}^n Y_k \\mathbf c_k})\\big] $  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot E_{\\mathbf y}\\big[e^{\\sum_{k=1}^n Y_k (\\mathbf t^T\\mathbf c_k)}\\big] $  (linearity, recall $Y_k$ are scalar valued mutually independent gaussians)  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot E_{\\mathbf y}\\big[\\prod_{k=1}^n e^{ Y_k (\\mathbf t^T\\mathbf c_k)}\\big] $  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot \\prod_{k=1}^n E_{\\mathbf y}\\big[ e^{ Y_k (\\mathbf t^T\\mathbf c_k)}\\big] $  (by independence)  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot \\prod_{k=1}^n e^{ (\\mathbf t^T\\mathbf c_k) \\mu_{Y_k} + \\frac{1}{2}(\\mathbf t^T\\mathbf c_k)^2 \\sigma_{Y_k}^2} $  (mgf for scalar guassians $Y_k$ with $s := \\mathbf t^T\\mathbf c_k$)  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot e^{\\sum_{k=1}^n \\{ (\\mathbf t^T\\mathbf c_k) \\mu_{Y_k} + \\frac{1}{2}(\\mathbf t^T\\mathbf c_k)^2 \\sigma_{Y_k}^2\\}} $  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot e^{\\mathbf t^T C (\\mathbf \\mu_{\\mathbf y}) + \\frac{1}{2}\\mathbf t^T C D C^T \\mathbf t} $ (matrix algebra, recall that $d_{k,k} = \\sigma_{Y_k}^2$, note $\\mathbf \\mu_{\\mathbf y}\\in \\mathbf R^n$ with means for $\\mathbf y$)    \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot e^{\\mathbf t^T C  (Q^T \\mathbf \\mu) + \\frac{1}{2}\\mathbf t^T (C) D (C^T) \\mathbf t} $  (recall $\\mathbf \\mu \\in \\mathbb R^n$ with means for $\\mathbf x$)  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot e^{\\mathbf t^T C  Q^T \\mathbf \\mu + \\frac{1}{2}\\mathbf t^T (AQ) D (Q^T A^T) \\mathbf t} $  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot e^{\\mathbf t^T (C  Q^T) \\mathbf \\mu + \\frac{1}{2}\\mathbf t^T A(Q D Q^T) A^T \\mathbf t} $  \n",
    "$=  e^{ \\mathbf t^T\\mathbf b} \\cdot e^{\\mathbf t^T A \\mathbf \\mu + \\frac{1}{2}\\mathbf t^T A \\Sigma A^T \\mathbf t} $  \n",
    "$=  e^{\\mathbf t^T\\mathbf b + \\mathbf t^T A \\mathbf \\mu + \\frac{1}{2}\\mathbf t^T A \\Sigma A^T \\mathbf t} $  \n",
    "$=  e^{\\mathbf t^T( A \\mathbf \\mu + \\mathbf b) + \\frac{1}{2}\\mathbf t^T A \\Sigma A^T \\mathbf t} $  \n",
    "as required  \n",
    "\n",
    "This proves that $\\mathbf v$ is a random vector with a multivariate gaussian as well.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
