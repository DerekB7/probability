{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) \n",
    "For a non-negative random variable $X$, show that \n",
    "\n",
    "$\\Big(E\\big[X^n\\big]\\Big)^\\frac{1}{n}$  \n",
    "\n",
    "is non-decreasing in $n$  \n",
    "\n",
    "**remark:**  this should remind us quite a bit of Chapter 8 of Cauchy Schwarz Masterclass re: Ladder of Power Means.  \n",
    "\n",
    "since $n$ is a natural number, it suffices to prove \n",
    "\n",
    "$\\Big(E\\big[X^n\\big]\\Big)^\\frac{1}{n} \\leq \\Big(E\\big[X^{n+1}\\big]\\Big)^\\frac{1}{n+1}$  \n",
    "\n",
    "or \n",
    "\n",
    "$E\\big[X^n\\big]^\\frac{n+1}{n} \\leq E\\big[X^{n+1}\\big]$  \n",
    "\n",
    "where $Z:=X^{n+1}$, we have    \n",
    "\n",
    "\n",
    "$E\\big[X^n\\big]^\\frac{n+1}{n} = E\\big[Z^\\frac{n}{n+1}\\big]^\\frac{n+1}{n} = f\\Big(E\\big[Z^\\frac{n}{n+1}\\big]\\Big) \\leq E\\Big[f\\big(Z^\\frac{n}{n+1}\\big)\\Big] =\\Big(E\\big[Z\\big]\\Big)=E\\big[X^{n+1}\\big]  $  \n",
    "\n",
    "\n",
    "which holds by Jensen's Inequality.  For avoidance of doubt, we may confirm that $f$ is the *convex* function given by $f(u) = u^\\frac{n+1}{n}$, which is easily confirmed to be convex by examining the second derivative which gives us \n",
    "\n",
    "for all $u \\geq 0$  \n",
    "\n",
    "$f''(u) = \\frac{(n+1)u^{\\frac{1}{n}-1}}{n^2} \\geq 0$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) X is a standard normal r.v. with density f(x)    \n",
    "*remark:  this is an interesting, and very long, problem that works through importance sampling as a technique for the normal distribution -- which does not have an elementary closed form CDF and of course is a ubiquitous distribution so the length of the exercise seems justifiable.*  \n",
    "\n",
    "Let $g(x) = x e^{-\\frac{x^2}{2}} = x\\cdot f(x)\\cdot \\sqrt{2 \\pi}$   for $x \\in(0, \\infty)$  \n",
    "\n",
    "(we can check that this integrates to one, and each term is real non-negative... note how this is the 'right half' of the standard normal, and in effect the first moment of said distribution -- if it did not integrate to one, we could put whatever value it did integrate to in the denominator as a constant. This is a general recurring theme in that importance sampling of distribution against something related to its $t$ th moment is useful -- in the Chernoff bound case we use MGF in here, but it's a variation on a very common theme.)  \n",
    "- - - - \n",
    "*edit:*  $g(x)$ aparently is called the Rayleigh distribution.  Reference e.g. problem 54 on page 239 of Blitzstein and Hwang (chp 5).  \n",
    "- - - - \n",
    "**for $c \\gt 0$ **  \n",
    "\n",
    "**(a)**    \n",
    "$P(X\\gt c)= P_f(X\\gt c)= E_g\\big[\\frac{f(X)}{g(X)}\\big \\vert X \\gt c\\big] P_g(X\\gt c)$   \n",
    "$= E_g\\big[\\frac{f(X)}{X\\cdot f(X)\\cdot \\sqrt{2 \\pi}}\\big \\vert X \\gt c\\big] P_g(X\\gt c)$  \n",
    "$= \\frac{1}{\\sqrt{2 \\pi}} E_g\\big[\\frac{1}{X}\\big \\vert X \\gt c\\big] P_g(X\\gt c)$  \n",
    "$= \\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{c^2}{2}} E_g\\big[\\frac{1}{X}\\big \\vert X \\gt c\\big]$   \n",
    "\n",
    "where we note that $P_g(X\\gt c) = \\int_{c}^\\infty x e^{-\\frac{x^2}{2}}dx = e^{-\\frac{c^2}{2}}$  \n",
    "\n",
    "**(b)**  \n",
    "for any $W \\gt 0$ we have \n",
    "\n",
    "*claim:*    \n",
    "$E\\big[\\frac{1}{W} \\big \\vert W \\gt c\\big] \\leq E\\big[\\frac{1}{W}\\big]$   \n",
    "\n",
    "to make this meaningul, as this is a more general statement, we assume that $E\\big[\\frac{1}{W}\\big] \\lt \\infty$   \n",
    "\n",
    "*proof:*   \n",
    "let $U :=\\frac{1}{W}$  and some constant $\\alpha \\gt 0$ which we'll select at a later time \n",
    "\n",
    "applying total expectation we have   \n",
    "$E\\big[U\\big] = P\\big(U \\lt \\alpha\\big) \\cdot E\\big[U \\big \\vert U \\lt \\alpha\\big] + P\\big(U \\geq \\alpha\\big) \\cdot E\\big[U \\big \\vert U \\geq \\alpha\\big] $   \n",
    "\n",
    "noting that an underlying point-wise bound gives us:  \n",
    "\n",
    "$E\\big[U \\big \\vert U \\lt \\alpha\\big] \\lt \\alpha \\leq E\\big[U \\big \\vert U \\geq \\alpha\\big]$  \n",
    "\n",
    "hence we have \n",
    "\n",
    "$E\\big[U \\big \\vert U \\lt \\alpha\\big]$  \n",
    "$= \\Big(1\\Big) \\cdot E\\big[U \\big \\vert U \\lt \\alpha\\big]$   \n",
    "$= \\Big(P\\big(U \\lt \\alpha\\big) +P\\big(U \\geq \\alpha\\big)\\Big) \\cdot E\\big[U \\big \\vert U \\lt \\alpha\\big]$   \n",
    "$= P\\big(U \\lt \\alpha\\big) \\cdot E\\big[U \\big \\vert U \\lt \\alpha\\big] + P\\big(U \\geq \\alpha\\big) \\cdot E\\big[U \\big \\vert U \\lt \\alpha\\big]$  \n",
    "$\\lt  P\\big(U \\lt \\alpha\\big) \\cdot E\\big[U \\big \\vert U \\lt \\alpha\\big] + P\\big(U \\geq \\alpha\\big) \\cdot \\alpha$  \n",
    "$\\leq  P\\big(U \\lt \\alpha\\big) \\cdot E\\big[U \\big \\vert U \\lt \\alpha\\big] + P\\big(U \\geq \\alpha\\big) \\cdot E\\big[U \\big \\vert U \\geq \\alpha\\big]$  \n",
    "$\\leq  E\\big[U\\big]$  \n",
    "\n",
    "now considering that the event \n",
    "\n",
    "$U \\lt \\alpha$  is equivalent to the event $\\frac{1}{W} \\lt \\alpha$ which is equivalent to $W \\gt \\frac{1}{\\alpha}$, we now have the inequality of \n",
    "\n",
    "\n",
    "$E\\big[\\frac{1}{W} \\big \\vert W \\gt \\alpha^{-1}\\big] \\leq  E\\big[\\frac{1}{W}\\big]$   \n",
    "\n",
    "setting $\\alpha := c^{-1}$ i.e. $c = \\alpha^{-1}$, gives us  \n",
    "\n",
    "$E\\big[\\frac{1}{W} \\big \\vert W \\gt c\\big] \\leq  E\\big[\\frac{1}{W}\\big]$   \n",
    "\n",
    "as desired  \n",
    "\n",
    "**(c)**  \n",
    "Prove   \n",
    "$P(X\\gt c) \\leq \\frac{1}{2}e^{\\frac{-c^2}{2}}$   \n",
    "\n",
    "where it is understood that $P(X\\gt c) = P_f(X\\gt c)$  \n",
    "\n",
    "using (a), then (b), we have \n",
    "\n",
    "$P(X\\gt c) $  \n",
    "$= \\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{c^2}{2}} E_g\\big[\\frac{1}{X}\\big \\vert X \\gt c\\big]$  \n",
    "$\\leq \\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{c^2}{2}} E_g\\big[\\frac{1}{X}\\big]$  \n",
    "$=\\Big(\\frac{1}{\\sqrt{2 \\pi}}E_g\\big[\\frac{1}{X}\\big]\\Big)e^{-\\frac{c^2}{2}} $  \n",
    "$= \\frac{1}{2}e^{-\\frac{c^2}{2}}$  \n",
    "\n",
    "as required.  Note that  $\\frac{1}{\\sqrt{2 \\pi}} E_g\\big[\\frac{1}{X}\\big] = \\frac{1}{\\sqrt{2 \\pi}}\\int_0^\\infty e^{-\\frac{x^2}{2}} = \\frac{1}{2}$  i.e. it is integral of the right half of the standard normal random variable's density.  \n",
    "- - - - \n",
    "*remark:*  \n",
    "if we used the Chernoff Bounding technique from problem 5, we'd have, for a standard normal random variable $X$  \n",
    "\n",
    "(note that since $X$ has a density, we can use $P(X\\gt c) = P(X\\geq c)$ and ignore any issues relating to strictness of the inequality)  \n",
    "\n",
    "$P(X\\gt c) \\leq  M(t)e^{-tc} = e^\\frac{t^2}{2}e^{-tc} = \\exp\\big(\\frac{t^2}{2}-tc\\big)$  \n",
    "\n",
    "selecting the minimizing $t$ for the RHS is given by recognizing that we have a convex function inside the exponential function (and the fact that the exponential map is monotone increasing) so we minimize $(\\frac{t^2}{2}-tc)$ by differentiating once and setting equal to zero, giving us  \n",
    "\n",
    "$0= t^*-c \\to t^* = c$    \n",
    "\n",
    "so plugging in optimal $t^*$, the Chernoff Bounding technique gives  \n",
    "\n",
    "$P(X\\gt c) \\leq  \\exp\\big(\\frac{t^2}{2}-tc\\big) = \\exp\\big(\\frac{c^2}{2}-c^2\\big) = \\exp\\big(-\\frac{c^2}{2}\\big) = e^\\frac{-c^2}{2}$  \n",
    "\n",
    "this upper bound is the same as the one above, except it is twice as large  \n",
    "\n",
    "**note while not assymptotically tight, both of the above results for (c) prove that the standard normal random variable is \"sub-gaussian\"**  \n",
    "\n",
    "\n",
    "- - - -  \n",
    "\n",
    "*begin interlude*  \n",
    "using the 'slip-in trick' from ex. 7.8 in *The Cauchy Schwarz Masterclass* we have   \n",
    "$P(X\\gt c) \\leq \\frac{1}{c\\cdot \\sqrt{2\\pi}}e^{\\frac{-c^2}{2}}$  \n",
    "which in general is a much tighter bound, except for small $c$ (we can fine tune it, but any $c\\geq 1$ is sufficient)  \n",
    "\n",
    "where the slip-in trick is \n",
    "\n",
    "$P_f(X\\gt c) = \\frac{1}{\\sqrt{2 \\pi}}\\int_c^\\infty e^\\frac{-x^2}{2} dx$  \n",
    "$= \\frac{1}{\\sqrt{2 \\pi}}\\int_c^\\infty \\frac{x}{x} e^\\frac{-x^2}{2} dx$  \n",
    "$\\leq \\frac{1}{\\sqrt{2 \\pi}}\\int_c^\\infty \\frac{x}{c} e^\\frac{-x^2}{2} dx$  \n",
    "$= \\frac{1}{c\\cdot\\sqrt{2 \\pi}}\\int_c^\\infty x e^\\frac{-x^2}{2} dx$  \n",
    "$= \\frac{1}{c\\cdot\\sqrt{2 \\pi}} P_g(X\\gt c)$  \n",
    "$= \\frac{1}{c\\cdot\\sqrt{2 \\pi}} e^{-\\frac{c^2}{2}}$ \n",
    "\n",
    "where in the last 3 lines we used the results from the end of part (a) which stated  \n",
    "$P_g(X\\gt c) = \\int_{c}^\\infty x e^{-\\frac{x^2}{2}}dx = e^{-\\frac{c^2}{2}}$  \n",
    "\n",
    "*end interlude*  \n",
    "- - - - \n",
    "\n",
    "**(d)**  \n",
    "show that  \n",
    "$E_g\\big[X \\big \\vert X \\gt c\\big] = c + e^{c^2}\\sqrt{2\\pi}\\cdot P(X\\gt c) $   \n",
    "equivalently, using (a), prove that \n",
    "\n",
    "$E_g\\big[X \\big \\vert X \\gt c\\big] = c +  E_g\\big[\\frac{1}{X} \\big \\vert X \\gt c\\big]$    \n",
    "\n",
    "*proof*  \n",
    "$E_g\\big[X \\big \\vert X \\gt c\\big]$  \n",
    "$= \\frac{1}{P_g(X\\gt c)}\\int_c^\\infty x^2 e^{\\frac{-x^2}{2}}$   \n",
    "$= \\frac{1}{P_g(X\\gt c)}\\Big(\\big(-x e^{-\\frac{x^2}{2}}\\Big|_c^\\infty\\big)  + \\int_c^\\infty e^{-\\frac{x^2}{2}}dx\\Big)$ (via integration by parts)   \n",
    "$= \\frac{1}{P_g(X\\gt c)}\\big(-x e^{-\\frac{x^2}{2}}\\Big|_c^\\infty\\big) + \\frac{1}{P_g(X\\gt c)}\\int_c^\\infty e^{-\\frac{x^2}{2}}dx$   \n",
    "$= \\frac{1}{P_g(X\\gt c)}\\big(-x e^{-\\frac{x^2}{2}}\\Big|_c^\\infty\\big)  + E_g\\big[\\frac{1}{X} \\big \\vert X \\gt c\\big]$   \n",
    "$= \\frac{1}{P_g(X\\gt c)}\\big(c e^{-\\frac{c^2}{2}}\\big)  + E_g\\big[\\frac{1}{X} \\big \\vert X \\gt c\\big]$   \n",
    "$= c\\cdot \\big(\\frac{e^{-\\frac{c^2}{2}}}{P_g(X\\gt c)}\\big)  + E_g\\big[\\frac{1}{X} \\big \\vert X \\gt c\\big]$   \n",
    "$= c  + E_g\\big[\\frac{1}{X} \\big \\vert X \\gt c\\big]$   \n",
    "\n",
    "- - - - \n",
    "where on the 2nd to last line we used the fact that was stated at the end of (a), i.e. that \n",
    "\n",
    "$P_g(X\\gt c) = \\int_{c}^\\infty x e^{-\\frac{x^2}{2}}dx = e^{-\\frac{c^2}{2}}$  \n",
    "- - - - \n",
    "**(e)**  \n",
    "The book says to use Jensen's Inequality here, though what we really want is Cauchy-Schwarz.  I.e. Cauchy-Schwarz tells us \n",
    "\n",
    "*lemma*  \n",
    "$1 = 1^2 = \\Big(\\frac{1}{P_g(X\\gt c)} \\int_c^\\infty 1 \\cdot g(x)dx\\Big)^2 = \\Big(\\int_c^\\infty \\big(\\frac{1}{x}\\frac{g(x)}{P_g(X\\gt c)}\\big)^\\frac{1}{2}\\cdot \\big(x\\frac{g(x)}{P_g(X\\gt c)}\\big)^\\frac{1}{2} dx\\Big)^2 $  \n",
    "$\\leq \\Big(\\int_c^\\infty \\frac{1}{x}\\frac{g(x)}{P_g(X\\gt c)}dx\\Big) \\Big(\\int_c^\\infty x\\frac{g(x)}{P_g(X\\gt c)}dx\\Big) $  \n",
    "$= \\Big(\\frac{1}{P_g(X\\gt c)} \\int_c^\\infty \\frac{1}{x}\\cdot g(x)dx\\Big) \\Big(\\frac{1}{P_g(X\\gt c)} \\int_c^\\infty x\\cdot g(x)dx\\Big) $  \n",
    "$= E_g\\big[\\frac{1}{X}\\big \\vert X \\gt c\\big]\\cdot E_g\\big[X\\big \\vert X \\gt c\\big]$  \n",
    "\n",
    "we've thus proven for positive random variables and $c \\gt 0$  \n",
    "$1 \\leq  E_g\\big[\\frac{1}{X}\\big \\vert X \\gt c\\big]\\cdot E_g\\big[X\\big \\vert X \\gt c\\big]$   \n",
    "\n",
    "by Cauchy Schwarz  \n",
    "\n",
    "- - - - \n",
    "*begin interlude:*  \n",
    "if we prefer, we can use the Gram Matrix approach (see the chapter 7 notes in the Cauchy Schwarz Masterclass folder) and see:  \n",
    "\n",
    "$\\mathbf G := \\begin{bmatrix}\n",
    "E[Y_1^2] & E[Y_1 Y_2] \\\\\n",
    " E[Y_2 Y_1] &  E[Y_2^2]\\\\  \n",
    "\\end{bmatrix}$  \n",
    "\n",
    "where $\\mathbf G \\succeq 0$ and we select \n",
    "\n",
    "$Y_1 : =_d \\big(\\frac{1}{X}\\big)^\\frac{1}{2}\\big \\vert X \\gt c$  \n",
    "$Y_2 : =_d \\big(X\\big)^\\frac{1}{2}\\big \\vert X \\gt c$  \n",
    "\n",
    "thus  \n",
    "\n",
    "$\\mathbf G =  \\begin{bmatrix}\n",
    "E\\big[\\frac{1}{X}\\big \\vert X \\gt c \\big] & 1 \\\\\n",
    " 1 &  E\\big[X\\big \\vert X \\gt c\\big]\\\\  \n",
    "\\end{bmatrix}$  \n",
    "\n",
    "hence by Cauchy-Schwarz (or because we know $\\det\\big(\\mathbf G\\big) \\geq 0$) we have  \n",
    "$1 \\leq E\\big[\\frac{1}{X}\\big \\vert X \\gt c \\big]\\cdot E\\big[X\\big \\vert X \\gt c\\big]$  \n",
    "\n",
    "*end interlude*   \n",
    "- - - - \n",
    "- - - - \n",
    "*begin subsequent interlude:*  \n",
    "it seems that the Jensen's Inequality hint was getting at:  \n",
    "assign \n",
    "\n",
    "$Z:=_d [X\\big \\vert X \\gt c]$    \n",
    "(again for some $c \\gt 0$ -- this technique is similar to that used in Theorem 4.21 of this chapter)    \n",
    "\n",
    "so we know \n",
    "$\\mu = E\\big[Z\\big] = E[X\\big \\vert X \\gt c]$  \n",
    "but by Jensen's Inequality, we know \n",
    "$\\mu^{-1} \\leq E\\big[Z^{-1}\\big] = E[X^{-1}\\big \\vert X \\gt c]$  \n",
    "\n",
    "taking advantage of positivity and multiplying these two inequalities, we get:   \n",
    "$1  = \\mu^{-1}\\mu \\leq E[X\\big \\vert X \\gt c]\\cdot E[X^{-1}\\big \\vert X \\gt c]$  \n",
    "\n",
    "Which seems to be what the exercise hint was getting at... however Cauchy-Schwarz seems more fundamentally relevant to your author here.  \n",
    "\n",
    "\n",
    "*end subsequent interlude*   \n",
    "- - - - \n",
    "now using the results from (d) \n",
    "\n",
    "$E_g\\big[X \\big \\vert X \\gt c\\big] = c + e^{c^2}\\sqrt{2\\pi}\\cdot P(X\\gt c) $   \n",
    "\n",
    "and multiplying each side by $P(X\\gt c)$, we have  \n",
    "\n",
    "$P(X\\gt c)\\cdot E_g\\big[X \\big \\vert X \\gt c\\big] = P(X\\gt c)\\cdot c + e^{c^2}\\sqrt{2\\pi}\\cdot P(X\\gt c)^2 $   \n",
    "\n",
    "subsitituing in the results for part(a) on the LHS, and application of the lemma, gives us \n",
    "\n",
    "$\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{c^2}{2}} \\leq \\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{c^2}{2}} \\cdot E_g\\big[\\frac{1}{X}\\big \\vert X \\gt c\\big]\\cdot E_g\\big[X \\big \\vert X \\gt c\\big] = c\\cdot P(X\\gt c) c + e^{c^2}\\sqrt{2\\pi}\\cdot P(X\\gt c)^2 $   \n",
    "\n",
    "or more simply   \n",
    "$\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{c^2}{2}} \\leq c \\cdot P(X\\gt c) + e^{c^2}\\sqrt{2\\pi}\\cdot P(X\\gt c)^2 $   \n",
    "\n",
    "**(f)**  \n",
    "using the prior inequality we can see the outlines of a quadratic polynomial, i.e. \n",
    "\n",
    "$e^{c^2}\\sqrt{2\\pi}\\cdot P(X\\gt c)^2 + c\\cdot P(X\\gt c)  - \\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{c^2}{2}} \\geq 0 $  \n",
    "\n",
    "if we ignore the inequality momentarily, we can see the polynomial \n",
    "\n",
    "$p(z) = e^{c^2}\\sqrt{2\\pi}\\cdot z^2 + c\\cdot z - \\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{c^2}{2}}$  \n",
    "\n",
    "note that the coefficient for dominant $z^2$ term is positive hence hence the function is convex (aka it looks like an 'upward cup')  \n",
    "\n",
    "what we want to show is that this polynomial has two distinct roots, $\\lambda_1 \\gt 0$ and $\\lambda_2 \\lt 0$.  This means that for our domain including $z$  we know that $p(z) \\geq p(\\lambda_1) =0$  which implies that $z \\geq \\lambda_1$ -- specifically if we have $z^* \\in(0,\\lambda_1)$ then our inequality would tell us that $0 \\leq p(z^*) \\lt 0$  which is a contradiction.  \n",
    "\n",
    "\n",
    "**(g)**  \n",
    "via use of wolframlpha  \n",
    "https://www.wolframalpha.com/input/?i=roots+of+cz+%2B+exp(c%5E2%2F2)+*+sqrt(2+pi)+z%5E2+-+1%2Fsqrt(2pi)exp(-c%5E2%2F2)\n",
    "\n",
    "we see $\\lambda_2 \\lt 0$ and $\\lambda_1 \\gt 0$, with  \n",
    "\n",
    "$\\lambda_1 = \\frac{e^{-c^2}\\cdot \\big(\\sqrt{(c^2 + 4)e^{c^2}} - c e^{\\frac{c^2}{2}}\\big)}{2\\sqrt{2\\pi}} $  \n",
    "$ = \\frac{1}{{2\\sqrt{2\\pi}}}\\big(e^{-c^2}e^{\\frac{c^2}{2}}\\sqrt{c^2 + 4} - c e^{\\frac{-c^2}{2}}\\big)$  \n",
    "$ = \\frac{1}{{2\\sqrt{2\\pi}}}\\big(e^{\\frac{-c^2}{2}}\\sqrt{c^2 + 4} - c e^{\\frac{-c^2}{2}}\\big)$  \n",
    "$ = \\frac{1}{{2\\sqrt{2\\pi}}}\\big(\\sqrt{c^2 + 4} - c\\big)e^{-\\frac{c^2}{2}}$  \n",
    "\n",
    "hence \n",
    "\n",
    "$P(X\\gt c) \\geq \\lambda_1 \\geq  \\frac{1}{{2\\sqrt{2\\pi}}}\\big(\\sqrt{c^2 + 4} - c\\big)e^{-\\frac{c^2}{2}}$  \n",
    "\n",
    "or more succinctly:  \n",
    "$P(X\\gt c) \\geq \\frac{1}{{2\\sqrt{2\\pi}}}\\big(\\sqrt{c^2 + 4} - c\\big)e^{-\\frac{c^2}{2}}$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**extra note:**   if we want to lower bound this, we can say \n",
    "\n",
    "$P(X\\gt c) \\geq \\frac{1}{{2\\sqrt{2\\pi}}}\\big(\\sqrt{c^2 + 4} - c\\big)e^{-\\frac{c^2}{2}}$  \n",
    "$= \\frac{e^{-\\frac{c^2}{2}}}{{2\\sqrt{2\\pi}}}c\\big(\\sqrt{1 + \\frac{4}{c^2}} - 1\\big)  $  \n",
    "$= \\frac{e^{-\\frac{c^2}{2}}}{{2\\sqrt{2\\pi}}}c\\big(( 1+ \\frac{4}{c^2})^\\frac{1}{2}\\cdot1^\\frac{1}{2} - 1\\big)$    \n",
    "$\\geq \\frac{e^{-\\frac{c^2}{2}}}{{2\\sqrt{2\\pi}}}c\\big(\\frac{2}{\\frac{1}{( 1+ \\frac{4}{c^2})} + \\frac{1}{1}} - 1\\big)$  \n",
    "$= \\frac{e^{-\\frac{c^2}{2}}}{{2\\sqrt{2\\pi}}}c\\big(\\frac{2}{\\frac{c^2}{c^2+ 4}+\\frac{c^2+ 4}{c^2+ 4}} - 1\\big) $  \n",
    "$= \\frac{e^{-\\frac{c^2}{2}}}{{2\\sqrt{2\\pi}}}c\\big(\\frac{2(c^2+ 4)}{2c^2 + 4} - 1\\big) $  \n",
    "$= \\frac{e^{-\\frac{c^2}{2}}}{{2\\sqrt{2\\pi}}}c\\big(\\frac{(2c^2+ 4) + 4}{2c^2 + 4} - 1\\big) $  \n",
    "$= \\frac{e^{-\\frac{c^2}{2}}}{{2\\sqrt{2\\pi}}}c\\big(1 + \\frac{2}{c^2 + 2} - 1\\big) $  \n",
    "$=\\frac{e^{-\\frac{c^2}{2}}}{{2\\sqrt{2\\pi}}}\\frac{2c}{c^2 + 2}  $    \n",
    "$=\\frac{e^{-\\frac{c^2}{2}}}{{\\sqrt{2\\pi}}}\\frac{c}{c^2 + 2}  $  \n",
    "$=\\frac{e^{-\\frac{c^2}{2}}}{{\\sqrt{2\\pi}}}\\frac{1}{c + \\frac{2}{c}} $    \n",
    "\n",
    "via $\\text{GM} \\geq \\text{HM}$  \n",
    "\n",
    "\n",
    "hence this result in combination with the slip-in trick (see above interlude) gives us  \n",
    "\n",
    "$\\frac{1}{{\\sqrt{2\\pi}}}\\cdot e^{-\\frac{c^2}{2}}\\cdot\\frac{1}{c + \\frac{2}{c}}= \\text{looser lower bound}\\leq \\frac{1}{{2\\sqrt{2\\pi}}}\\big(\\sqrt{c^2 + 4} - c\\big)e^{-\\frac{c^2}{2}} \\leq P(X\\gt c) \\leq  \\text{upper bound} = \\frac{1}{\\sqrt{2 \\pi}}\\cdot e^{-\\frac{c^2}{2}}\\cdot \\frac{1}{c}$  \n",
    "\n",
    "where we note that\n",
    "\n",
    "$\\frac{\\text{upper bound}}{\\text{looser lower bound}} = \\frac{ \\frac{1}{c}}{\\frac{1}{c + \\frac{2}{c}}} =  \\frac{ \\frac{1}{c}}{\\frac{c}{c^2 + 2}}=   \\frac{c^2 + 2}{c^2}= \\frac{c^2}{c^2} +  \\frac{2}{c^2} = 1 +  \\frac{2}{c^2}$  \n",
    "\n",
    "which tends to 1 as $c$ grows large -- i.e. this tells us that  \n",
    "\n",
    "$P(X\\gt c) \\sim \\frac{1}{c\\cdot\\sqrt{2 \\pi}} e^{-\\frac{c^2}{2}}$  \n",
    "\n",
    "(note that since this relies on an even looser lower bound courtesy of $\\text{GM} \\geq \\text{HM}$, the actual rate of convergence/tightening between stated lower and upper bounds is a bit faster than the decay rate which is quadratic in $c$ in the above)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Chernoff Bound on the tail of Poisson distribution  \n",
    "Let X be Poisson with mean $\\lambda$.  Show that for $n\\geq \\lambda$ (i.e. natural number $n$ that is 'to the right' of the mean), the Chernoff bound gives us \n",
    "\n",
    "**claim:**    \n",
    "$P\\big(X\\geq n\\big) \\leq \\frac{e^{-\\lambda} (\\lambda e )^n}{n^n}$  \n",
    "\n",
    "**remark:**    \n",
    "this looks awfully similar to the PMF value (not the complementary CDF) of the Poisson at $n$ except we have $(\\frac{n}{e})^n$ in the denominator instead of $n!$.  There are ideas of the stirling approximation and also truncation of Taylor series lurking here... \n",
    "\n",
    "It may be useful to compare this to a simplistic geometric series bound on the residual of the exponential series as well as some of the better used more sophisticated bounds \n",
    "\n",
    "**proof:**  \n",
    "The Chernoff Bound gives us, for $t \\gt 0$   \n",
    "\n",
    "$P\\big(X\\geq n\\big) \\leq \\inf M(t) e^{-tn} \\leq   M(t) e^{-tn} \\leq \\exp\\big(\\lambda (e^t - 1)\\big) e^{-tn} = \\exp\\big(\\lambda e^t - \\lambda -tn\\big) = e^{-\\lambda} \\exp\\big(\\lambda e^t  -tn\\big) = e^{-\\lambda} \\frac{\\exp (\\lambda e^t)}{\\exp(t)^n}$ \n",
    "\n",
    "the clue is that $\\lambda \\leq n$ and we want to simplify and cancel many terms, so suppose we select \n",
    "\n",
    "$e^t := \\frac{n}{\\lambda} \\geq 1$ \n",
    "which implies that $t\\geq 0$.  \n",
    "- - - - - - \n",
    "*justification for such a choice*  \n",
    "With respect to minimization, we are selecting an optimal $t$ and we consider  \n",
    "$e^{-\\lambda} \\exp\\big(\\lambda e^t  -tn\\big) $  by minimizing  \n",
    "$\\exp\\big(\\lambda e^t  -tn\\big) $  \n",
    "which we can take the log of and minimize in logspace instead, which gives  \n",
    "$\\lambda e^t  -tn $   \n",
    "or taking advtange of positivity, we can seek to minimize  \n",
    "$g(t)= e^t  -t\\big(\\frac{n}{\\lambda}\\big) = e^t  -t \\cdot c $   \n",
    "differentiating once gives  \n",
    "$g'(t) = e^t  -c := 0 $ \n",
    "and of course this is convex \n",
    "$g''(t) = e^t \\gt 0 $ \n",
    "so we have a minimum  when \n",
    "$ e^t = c = \\frac{n}{\\lambda} $ \n",
    "\n",
    "- - - - -  \n",
    "\n",
    "(*Technical nit:* we really should insist on $t \\gt 0$ and hence $n \\gt \\lambda$ though the inequality in the stated question in the book is not strict for some reason.)  \n",
    "\n",
    "Thus we have \n",
    "\n",
    "$P\\big(X\\geq n\\big) \\leq e^{-\\lambda} \\frac{\\exp (\\lambda e^t)}{\\exp(t)^n} = e^{-\\lambda} \\frac{\\exp (\\lambda \\frac{n}{\\lambda})}{(\\frac{n}{\\lambda})^n}= e^{-\\lambda} \\frac{\\exp (n)}{(\\frac{n}{\\lambda})^n} = e^{-\\lambda} \\frac{(e\\lambda)^n }{n^n} = e^{-\\lambda} \\big(\\frac{e\\lambda}{n}\\big)^n$  \n",
    "\n",
    "as desired.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Moment Bound via Importance Sampling  \n",
    "\n",
    "referencing Corollary 4.4. on page 115,  \n",
    "for some $c \\gt 0$ we have   \n",
    "\n",
    "$P_f(X\\gt c)= E_g\\big[\\frac{f(X)}{g(X)}\\big \\vert X \\gt c\\big] P_g(X\\gt c)$  \n",
    "\n",
    "where $f$ is the density (or probability mass function) for $X$ and $g$ is a different mass function. \n",
    "\n",
    "in this case for natural number $t$, any $X$ may be used so long as $x^t \\geq 0$ and $x$ isn't zero WP1-- in this case we can use: \n",
    "\n",
    "**key insights on positivity here...**  \n",
    "\n",
    "$g(x) = \\frac{f(x)x^t}{E[X^t]} = \\frac{f(x)x^t}{E_f[X^t]}= \\frac{f(x)x^t}{\\mu_t}$  \n",
    "\n",
    "where for avoidance of doubt the t'th moment of X is with respect to $f$  \n",
    "\n",
    "noting that by construction this is real non-negative and integrates to one.  Note that this is a general technique similar in spirit to the construction of $g$ in problem 2 and extremely similar to construction of $g$ on page 119 for the Chernoff Bound.  \n",
    "\n",
    "now plugging this into our importance sampling identity gives us \n",
    "\n",
    "**main argument:**   \n",
    "\n",
    "$P_f(X\\gt c)= E_g\\big[\\frac{f(X)}{\\frac{f(X)X^t}{E_f[X^t]}}\\big \\vert X \\gt c\\big] P_g(X\\gt c)$  \n",
    "$= E_f[X^t]\\cdot E_g\\big[\\frac{1}{X^t}\\big \\vert X \\gt c\\big] P_g(X\\gt c)$  \n",
    "$\\leq E_f[X^t]\\cdot E_g\\big[\\frac{1}{X^t}\\big \\vert X \\gt c\\big]$  (probabilities are in $[0,1]$)   \n",
    "$\\lt E_f[X^t]\\cdot E_g\\big[\\frac{1}{c^t}\\big \\vert X \\gt c\\big]$   \n",
    "$= E_f[X^t] c^{-t} \\cdot E_g\\big[1 \\big \\vert X \\gt c\\big]$  \n",
    "$= E_f[X^t] c^{-t} \\cdot 1$  \n",
    "$= E_f[X^t] c^{-t}$   \n",
    "$= E[X^t] c^{-t}$    \n",
    "\n",
    "where we note that the second inequality is justified, because, conditioned on $X\\gt c $   \n",
    "we have $X^t \\gt c^t$   and $\\frac{1}{X^t} \\lt \\frac{1}{c^t}$, and observing this point-wise bound, we may take the expectation.  \n",
    "\n",
    "**commentary:**  \n",
    "note that if $X\\geq 0$ we can get the same result via Markov's Inequality, in particular, observe the point-wise bound:  \n",
    "\n",
    "$\\mathbb I_{X\\gt c }\\cdot c \\leq X$  \n",
    "raise to the $t$th power, recalling that indicators are idempotent, giving us  \n",
    "$\\mathbb I_{X\\gt c }\\cdot c^t \\leq X^t$  \n",
    "taking expectations gives  \n",
    "$Pr\\{X \\gt c\\}\\cdot c^t = E\\big[\\mathbb I_{X\\gt c }\\big]\\cdot c^t \\leq E\\big[X^t\\big]$  \n",
    "and dividing by $c^t$ gives us  \n",
    "$Pr\\{X \\gt c\\}\\leq E\\big[X^t\\big] c^{-t}$   \n",
    "- - - - \n",
    "note that the Markov Inequality approach is quicker but requires $X\\geq 0$ which is not required by the importance sampling approach.  However the more flexible approach, naively applied may lead to weaker results, e.g. when $t$ is even.  Consider some random variable with all moments that is symmetric about zero.  If we select say $t=2$ we are setting up for Chevbyshev's Inequality.  The Importance Sampling approach immediately gives us   \n",
    "\n",
    "$P_f(X\\gt c) \\lt E[X^2] c^{-2}$  \n",
    "\n",
    "However the Markov Inequality setup can be written as \n",
    "\n",
    "$\\mathbb I_{X^2\\gt c^2 }\\cdot c^2 \\leq X^2$  \n",
    "taking expectations and simplifying, we have \n",
    "$Pr\\{X^2 \\gt c^2\\}\\leq E\\big[X^2\\big] c^{-2}$   or  \n",
    "$Pr\\{\\big \\vert X\\big \\vert \\gt c\\}\\leq E\\big[X^2\\big] c^{-2}$   \n",
    "\n",
    "which is a much stronger claim (in fact about twice as strong)  \n",
    "\n",
    "So additional care is needed in dealing with and using negative numbers.  However, this problem does nicely illustrate usage of a technique to 'back into' an identity you want via importance sampling.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5)  Chernoff Bounds\n",
    "Fill in the details of the proof that for independent bernouli random variables, $X_1, ..., X_n$ and some $c\\gt 0$   \n",
    "**claim: **  \n",
    "$P\\Big(S_n - E\\big[S_n\\big] \\leq -c\\Big) \\leq e^{-\\frac{2c^2}{n}}$  \n",
    "\n",
    "*remark: *  \n",
    "there are in essence two approaches here \n",
    "\n",
    "The one being suggested, and from this chapter comes from Importance Sampling.  The alternative comes from a Martingale setup and is worked through via Azuma-Hoeffding on pages 100 and 101 of the text.  \n",
    "\n",
    "**proof:**  \n",
    "letting $Y: = S_n - E\\big[S_n\\big]$  \n",
    "where $Y$ is a centered random variable\n",
    "and $Z:= - Y$ \n",
    "is also a centered random variable, we have \n",
    "\n",
    "$P\\big(Y \\leq c\\big) = P\\big(Z \\geq c\\big)$  \n",
    "$= P\\big(e^{tZ} \\geq e^{tc}\\big)$    \n",
    "$\\leq e^{-tc}E\\big[e^{t(Z)}\\big]$  (Markov Inequality)  \n",
    "$= e^{-tc}E\\Big[\\exp\\big(\\sum_{i=1}^{n} t(\\bar{X_i} - X_i)\\big)\\Big]$    \n",
    "$= e^{-tc}E\\Big[\\prod_{i=1}^n \\exp\\big(t(\\bar{X_i} - X_i)\\big)\\Big]$  (summation in exponential domain is product)  \n",
    "$= e^{-tc}\\prod_{i=1}^nE\\Big[ \\exp\\big(t(\\bar{X_i} - X_i)\\big)\\Big]$ (by independence)  \n",
    "$\\leq e^{-tc}e^{\\frac{n t^2}{8}}$   \n",
    "\n",
    "\n",
    "Because we take advantage of positivity and observe a point-wise bound where  \n",
    "$Z_i:=(\\bar{X_i} - X_i)$  is a bounded and centered random variable so we know (see reference mentioned in problem 7)  \n",
    "\n",
    "$E\\Big[e^{t(\\bar{X_i} - X)}\\big)\\Big] = E\\Big[e^{tZ_i}\\Big] \\leq e^{\\frac{t^2}{8}}$  \n",
    "\n",
    "\n",
    "setting $t:= \\frac{4c}{n}$ gives us \n",
    "\n",
    "$e^{-tc}e^{\\frac{n t^2}{8}} =e^{\\frac{n t^2 - 8tc}{8}} = e^{\\frac{n (\\frac{4c}{n})^2 - 8 (\\frac{4c}{n})c}{8}} = e^{\\frac{ \\frac{16c^2}{n} - \\frac{32c^2}{n}}{8}}= e^{\\frac{ \\frac{-16c^2}{n}}{8}}= e^\\frac{-2c^2}{n}  $   \n",
    "\n",
    "which completes the exercise  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alternative derivation of Chernoff Bounds for Bernoulis:**  \n",
    "the derivation of Chernoff bounds is a bit odd in 4.4 as it starts developing it via importance sampling but then switches to a more 'traditional' chernoff bound technique which seems less motivated and does not tie in as nicely with the importance sampling. To develop this, consider $Z= -Y$ our centered random variable that is bounded in $[a,A]$.  Note that being bounded is not needed in general for Chernoff Bounds -- though we do need for the MGF to exist in an open interval around zero-- (e.g. reference problem 3 where we bound the tail of a Poisson) however we do need to be careful and recognize that we can only apply Hoeffding's Lemma in the manner of this problem, for a bounded random variable.  \n",
    "\n",
    "starting with the derivation in the text:   \n",
    "\n",
    "$g(z) = \\frac{e^{tz}f(z)}{M(t)}$  \n",
    "i.e. $Z$ has pmf / density of $f$ and $g$ is the titled distribution of $Z$  and of course \n",
    "$M(t) = E_f\\big[e^{tZ}\\big]$  \n",
    "is the moment generating function of $Z$  \n",
    "\n",
    "Now via importance sampling we have, for any $c\\gt 0$ \n",
    "\n",
    "$P_f\\big(Z\\geq c\\big) $  \n",
    "$= E_g\\Big[\\frac{f(Z)}{g(Z)}\\big \\vert Z \\geq c\\Big]P_g\\big(Z\\geq c\\big)$   \n",
    "$= E_g\\Big[\\frac{f(Z)}{\\frac{e^{tZ}f(Z)}{M(t)}}\\big \\vert Z \\geq c\\Big]P_g\\big(Z\\geq c\\big)$   \n",
    "$= E_g\\Big[M(t)\\exp(-tZ)\\big \\vert Z \\geq c\\Big]P_g\\big(Z\\geq c\\big)$   \n",
    "$\\leq  E_g\\Big[M(t)\\exp(-tZ)\\big \\vert Z \\geq c\\Big]$   (by positivitiy and probabilities being bounded in [0,1])  \n",
    "$= M(t)\\cdot E_g\\Big[\\exp(-tZ)\\big \\vert Z \\geq c\\Big]$  \n",
    "$\\leq  M(t)e^{-tc}$    \n",
    "- - - - \n",
    "*justfication:*  \n",
    "conditioning on $Z \\geq c$ we have --whether using $f$ or $g$, *in this conditional world* we have the point-wise bound:  \n",
    "$Z \\geq c$  \n",
    "$-tZ \\leq -tc$  (negative sign flips the inequality and $t \\gt 0$)  \n",
    "$e^{-tZ} \\leq e^{-tc}$   \n",
    "$E_g \\Big[e^{-tZ}\\big\\vert Z \\geq c \\Big] \\leq E_g\\Big[e^{-tc}\\big\\vert Z\\geq c\\Big]= e^{-tc} $   \n",
    "i.e. taking expectations with respect to $g$, in this conditional world, and noticing that this upper bound does not depend on distribution $g$ or $f$ \n",
    "- - - - \n",
    "the above holds for all $t\\gt 0$ so we may select the minimizing value of $t$   \n",
    "$P_f\\big(Z\\geq c\\big) \\leq \\frac{\\text{inf}}{t\\gt 0} M(t)e^{-tc}$   \n",
    "\n",
    "**remark:** this is a slightly longer form of what is shown directly in the book.  \n",
    "Now in the case of sums of iid centered bernoulis (whether working with Y or its negative Z), we have \n",
    "\n",
    "$Z := Z_1 + Z_2 + ... + Z_n$  \n",
    "\n",
    "using the fact that MGF's convert convolutions/sums to products, we have, with $M(t)$ the MGF of $Z$   \n",
    "\n",
    "$M(t) = E\\Big[\\exp\\big(t(Z_1 + Z_2 + ... + Z_n)\\big)\\Big]$  \n",
    "$=E\\Big[\\exp\\big(tZ_1\\big)\\exp\\big(tZ_2\\big)...\\exp\\big(tZ_n\\big)\\Big]$  \n",
    "$=E\\Big[\\exp\\big(tZ_1\\big)\\Big]E\\Big[\\exp\\big(tZ_2\\big)\\Big]...E\\Big[\\exp\\big(tZ_n\\big)\\Big] $  \n",
    "$=  \\big(M_{Z_i}(t)\\big)^n$  \n",
    "(where the second to last equality follows by zero covariance for independent random variables)  \n",
    "\n",
    "\n",
    "$P\\big(Z \\geq c \\big) \\leq \\frac{\\text{inf}}{t\\gt 0} M(t)e^{-tc} = \\frac{\\text{inf}}{t\\gt 0} \\big(M_{Z_i}(t)\\big)^ne^{-tc}   \\leq \\big(e^{\\frac{t^2}{8}}\\big)^ne^{-tc} = e^{\\frac{nt^2 - 8tc}{8}}$  \n",
    "by Hoeffdings lemma, with A-a = 1, as it is for Bernoulis.  See \"martingales.ipynb\" for more information.  \n",
    "\n",
    "the graceful finish comes from selecting $t:= \\frac{4c}{n}$\n",
    "- - - -  \n",
    "note: this finish is nice because is simplifies the expression.  It *also* is nice because it is the minimizing $t$, in particular, since the exponential function is montone increasing, we merely want to minimize \n",
    " \n",
    "$\\frac{nt^2}{8} - tc$  \n",
    "\n",
    "with respect to $t$.  The function is quadratice and convex, so it has a single minimum that is the global minimum.  \n",
    "\n",
    "differentiating with respect to $t$ and setting equal to zero gives  \n",
    "\n",
    "$\\frac{1}{4}n t^* - c := 0$  \n",
    "$t^* = \\frac{4c}{n}$  \n",
    "\n",
    "- - - -  \n",
    "\n",
    "which gives \n",
    "\n",
    "$P\\big(Z \\geq c \\big) \\leq e^{\\frac{nt^2 - 8tc}{8}} =\\exp\\big(\\frac{n \\frac{16c^2}{n^2} - 8\\frac{4c}{n}c}{8}\\big)  = e^{\\frac{2c^2 - 4c^2}{n}} = e^{\\frac{-2c^2}{n}}$   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Detour into Feller: Bounding Bernouli's  \n",
    "problem 17, page 142 of Feller vol 1, 3rd edition  \n",
    "\n",
    "with independent events $A_1, A_2, ..., A_n$ being *independent* and $P(A_k) = p_k$ , prove that the probability of at least $k$ events occuring is bounded above by $\\frac{(p_1 + p_2 + ... + p_n)^k}{k!}$   \n",
    "\n",
    "letting   \n",
    "$N := \\mathbb I_{A_1} + \\mathbb I_{A_2}  + ... + \\mathbb I_{A_n}$  \n",
    "$\\lambda :=  p_1 + p_2 + ... + p_n$  \n",
    "\n",
    "we see that \n",
    "\n",
    "$E\\big[N\\big] = \\lambda$  \n",
    "\n",
    "Thus   \n",
    "$Pr\\{N \\geq 1  \\} = Pr\\big(A_1\\cup A_2 \\cup ... \\cup A_n\\big) \\leq \\frac{\\lambda^1}{1} = \\lambda$  \n",
    "which holds by the Union Bound (or equivalently, by the Markov Inequality)  \n",
    "\n",
    "The official problem statement then references Bonferonni Inequalities (from page 110 in particular) to solve the problem.  Your author prefers to approach this with the help of the Union Bound and elementary symmetric functions. So for $k \\geq 2$ we introduct elementary symmetric functions.  \n",
    "\n",
    "let  \n",
    "$\\mathbf v :=  \\begin{bmatrix} \n",
    "\\mathbb I_{A_1} \\\\ \n",
    "\\mathbb I_{A_2}\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbb I_{A_n} \n",
    "\\end{bmatrix}$  \n",
    "\n",
    "$\\mathbf p :=  \\begin{bmatrix} \n",
    "p_1 \\\\ \n",
    "p_2\\\\ \n",
    "\\vdots \\\\ \n",
    "p_n\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "so to make use of the union bound, we define \n",
    "\n",
    "$B_k :=\\text{compound event that at least k simple events occurred simultaneously}$  \n",
    "To get this, consider the intersection of $k$ distinct events  \n",
    "$A_{i_1}\\cap A_{i_2}\\cap...\\cap A_{i_k} $  and we take the union over all $\\binom{n}{k}$ combinations\n",
    "\n",
    "\n",
    "$Pr\\{N \\geq k \\} = Pr\\{B_k\\} = Pr\\{\\bigcup_{1\\leq i_1\\lt i_2 \\lt...\\lt i_k \\leq n} A_{i_1}\\cap A_{i_2}\\cap...\\cap A_{i_k} \\} \\leq \\sum_{1\\leq i_1\\lt i_2 \\lt...\\lt i_k \\leq n} Pr\\{A_{i_1}\\cap A_{i_2}\\cap...\\cap A_{i_k} \\}   = E\\Big[e_k\\big(\\mathbf v\\big)\\Big]$  \n",
    "\n",
    "where the inequality holds due to the Union Bound.  Now, since the simple events are independent, their probabilities multiply in the case of an intersection (or equivalently in terms of the indicator random variables-- the expectation of a product is equal to the product of the expectations due to mutual independence), so we can simplify the above to  \n",
    "\n",
    "$Pr\\{N \\geq k \\} \\leq  E\\Big[e_k\\big(\\mathbf v\\big)\\Big] = e_k\\Big(E\\big[\\mathbf v\\big]\\Big) = e_k\\Big(\\mathbf p\\Big) $  \n",
    "from here we may proceed by Maclaurin's Inequalities or Majorization   \n",
    "(see chapter 12 notes from *Cauchy Schwarz Masterclass*)   \n",
    "\n",
    "The majorization inspired approach is to compare our problem with an easier one -- the homogenous case, where \n",
    "\n",
    "$\\tilde{p_i} := \\frac{\\lambda}{n}$    \n",
    "of course $\\tilde{p_i} \\gt 0$, and $\\sum_{i=1}^n \\tilde{p_i} =  n \\cdot \\frac{\\lambda}{n} = \\lambda$    \n",
    "\n",
    "so we have \n",
    "\n",
    "$\\frac{e_k(\\mathbf p)}{\\binom{n}{k}} \\leq \\frac{e_k(\\tilde{p_i} \\mathbf 1)}{\\binom{n}{k}} = \\big(\\tilde{p_i}\\big)^k = \\big(\\frac{\\lambda}{n}\\big)^k $  \n",
    "\n",
    "where the inequality holds by Schur Concavitiy of elementary symmetric functions\n",
    "\n",
    "- - - -  \n",
    "if we instead applied Maclaurin's Inequality, we'd have  \n",
    "$0 \\lt \\Big(\\frac{e_k(\\mathbf p)}{\\binom{n}{k}}\\Big)^\\frac{1}{k} \\leq \\frac{e_1(\\mathbf p)}{n} = \\big(\\frac{\\lambda}{n}\\big) $  \n",
    "with equality **iff** $p_1 = p_2 = ... = p_n$   \n",
    "\n",
    "raising each side to the kth power gives  \n",
    "$\\frac{e_k(\\mathbf p)}{\\binom{n}{k}} \\leq \\big(\\frac{\\lambda}{n}\\big)^k $  \n",
    "- - - -  \n",
    "\n",
    "\n",
    "This simplifies to  \n",
    "\n",
    "$e_k(\\mathbf p) \\leq \\big(\\frac{\\lambda}{n}\\big)^k \\binom{n}{k}= \\frac{\\lambda^k}{n^k} \\frac{n(n-1)...(n-(k-1))}{k!} =  \\big(\\frac{\\lambda^k}{k!}\\big) \\prod_{j=0}^{k-1}(\\frac{n-j}{n})= \\big(\\frac{\\lambda^k}{k!}\\big) \\prod_{j=0}^{k-1}(1-\\frac{j}{n})\\leq \\big(\\frac{\\lambda^k}{k!}\\big)e^{\\sum_{j=0}^{k-1}(-\\frac{j}{n})}= \\big(\\frac{\\lambda^k}{k!}\\big) e^{-\\frac{(k^2 -k)}{2n} } \\leq \\frac{\\lambda^k}{k!}$    \n",
    "\n",
    "where we use that all important exponential function inequality $1+a \\leq e^a$ that is strict for $a \\neq 0$ by strict convexity of the exponential function and  $-\\frac{(k^2 -k)}{2n} \\lt 0$, hence taking advantage of the monotone increasing nature of the exponential function, we have  $0\\lt e^{-\\frac{(k^2 -k)}{2n}} \\lt e^0 = 1$   \n",
    "\n",
    "and putting this all together, we have   \n",
    "$Pr\\{N \\geq k \\} \\leq e_k(\\mathbf p) \\leq \\big(\\frac{\\lambda^k}{k!}\\big) e^{-\\frac{(k^2 -k)}{2n} } \\leq \\frac{\\lambda^k}{k!}$  \n",
    "where the first inequality is due to the union bound, and then concavity bounds on elementary symmetric functions, with convexity of the exponential function thrown in.  We can more simply say:  \n",
    " \n",
    "$Pr\\{N \\geq k \\} \\leq \\frac{\\lambda^k}{k!} = \\frac{(p_1 + p_2 + ... + p_n)^k}{k!}$  \n",
    "as required by the exercise.    \n",
    "\n",
    "- - - - \n",
    "The below numerics / simulation show that the above bound can be quite sharp, even better than Chernoff bounds, in certain cases of rare events -- i.e. $k$ is relatively large but the $p_i$ are rather small (having small $p_i$ is key for tightness in the union bound, and compresses them in terms of the majorization relation with the homogenous case/ binomial)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.3460088514 \n",
      " \n",
      "0.000149440686103 \n",
      "\n",
      "1.94869910196e-05 refined feller bound \n",
      "\n",
      "2.23516370761e-05 upper feller bound with extra factor  0.149568619223 \n",
      "\n",
      "0.0136394475217 regular chernoff\n",
      "trying chernoff on poisson approx\n",
      "tvd bound\n",
      "0.0706422028376\n",
      "8.019384745436574e-06\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "pr_vec = np.random.random(n)/10\n",
    "# pr_vec = np.ones(n) / 10\n",
    "pr_vec\n",
    "\n",
    "lam = np.sum(pr_vec)\n",
    "k = 20\n",
    "assert k <= n\n",
    "\n",
    "###\n",
    "additional_factor_running_prod = 1\n",
    "for j in range(0, k):\n",
    "    additional_factor_running_prod *= (n-j)/n\n",
    "\n",
    "\n",
    "print(lam, \"\\n \")\n",
    "# classical_bound = lam**k / factorial(k)\n",
    "# in logspace for extra care\n",
    "classical_bound_logspace = k*np.log(lam)\n",
    "for kdx in range(1, k+1):\n",
    "    classical_bound_logspace -= np.log(kdx)\n",
    "# print(classical_bound)\n",
    "print(np.exp(classical_bound_logspace), \"\\n\")\n",
    "print(np.exp(classical_bound_logspace)*additional_factor_running_prod, \"refined feller bound \\n\")\n",
    "print(np.exp(classical_bound_logspace)*np.exp(-(k**2-k)/(2*n)), \"upper feller bound with extra factor \", np.exp(-(k**2-k)/(2*n)), \"\\n\")\n",
    "\n",
    "# c + lam = k\n",
    "c = k - lam\n",
    "assert(c > 0)\n",
    "chernoff_bound = np.exp(-2*c**2/n)\n",
    "print(chernoff_bound, \"regular chernoff\")\n",
    "print(\"trying chernoff on poisson approx\")\n",
    "print(\"tvd bound\")\n",
    "print((min(1, 1/lam))*pr_vec@pr_vec)\n",
    "print(np.exp(-lam)*np.exp(k)*lam**k/(k**k))\n",
    "\n",
    "# print(\"the below is a binomial estimate -- remember that it is stochastically more variable\")  \n",
    "# print(1-binom.cdf(k-1, n, lam/n))\n",
    "# this has been commented out... there seems to be numeric stability problems with using the binomial.  \n",
    "# a binomial estimate (though homogenous case is stochastically more variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# End Detour "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6)  \n",
    "this problem is essentially an immediate result from applying the (5) and the associated result proven in the chapter. For some $c \\gt 0$:    \n",
    "\n",
    "$P\\Big( \\big \\vert X - np\\big \\vert \\geq c \\Big) = P\\Big(  X - np \\geq c \\Big) + P\\Big(  X - np \\leq -c \\Big) \\leq e^{-\\frac{2c^2}{n}} + e^{-\\frac{2c^2}{n}} = 2e^{-\\frac{2c^2}{n}}$  \n",
    "\n",
    "Where we have \n",
    "\n",
    "$P\\Big( \\big \\vert X - np\\big \\vert \\geq c \\Big) = P\\Big(A_1 \\cup A_2\\Big) =  P\\Big(A_1\\Big) + P\\Big(A_2\\big) = P\\Big(  X - np \\geq c \\Big) + P\\Big(  X - np \\leq -c \\Big)$   \n",
    "i.e. the equality case  of Boole's Inequality because the underlying events are mutually exclusive  \n",
    "\n",
    "(b) choosing $c = \\alpha np$ gives the result here using standard Chernoff bound.  However it seems worth pointing out that $\\alpha$ should be non-negative, though the text does not specify this for some reason.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7)   \n",
    "*Problem:* Give details of the proof of lemma 4.7 (i.e. Hoeffding's Lemma)  \n",
    "*Solution:* see \"Hoeffding's Lemma\" under 'martingales' notebook for an original take at the proof    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8)  \n",
    "prove that for convex function $f$ and random variables $X,Y$ we have the below refinement of Jensen's Inequality  \n",
    "*claim:*  \n",
    "$f\\Big(E\\big[X\\big]\\Big) \\leq  E\\Big[f\\Big(E\\big[X \\big \\vert Y \\big]\\Big)\\Big]\\leq E\\Big[f\\big(X\\big)\\Big]$  \n",
    "\n",
    "*proof:*  \n",
    "the left hand side is immediate because $Z:=E\\big[X \\big \\vert Y \\big]$ is a random variable, and by Jensen's Inequality  \n",
    "\n",
    "$f\\Big(E\\Big[X\\Big]\\Big) = f\\Big(E\\Big[E\\big[X \\big \\vert Y \\big]\\Big]\\Big)  = f\\Big(E\\Big[Z\\Big]\\Big) \\leq  E\\Big[f\\Big(Z\\Big)\\Big]$  \n",
    "\n",
    "the right hand side takes a little more care, but may be proven as \n",
    "\n",
    "$f\\Big(E\\big[X \\big \\vert Y \\big]\\Big)\\leq E\\big[f\\big(X\\big)\\big \\vert Y \\big]$   \n",
    "\n",
    "note that exercise 4 of chapter 3 was to prove the slightly more general form of the above inequality:  \n",
    "$f\\Big(E\\big[X \\big \\vert \\mathscr F \\big]\\Big)\\leq E\\big[f\\big(X\\big)\\big \\vert \\mathscr F \\big]$    \n",
    " \n",
    "if  $Y \\in \\mathscr F$ (and the only element of said sigmal field) then the above immediately follows.  \n",
    "\n",
    "- - - - \n",
    "if we sacrifice a touch of formalism, and reason over individual sample paths, we see that \n",
    "$f\\Big(E\\big[X \\big \\vert Y =y \\big]\\Big)\\leq E\\big[f\\big(X\\big)\\big \\vert Y=y \\big]$  which follows from the familiar fact of Jensen's Inequality / linear lower bound property of 'tangent' lines for convex functions, or equivalently, for each $Y(\\omega) = y$ we may create a random variable $Z$ which the distribution of $X \\big \\vert Y =y$, and we then recognize that   \n",
    "\n",
    "$f\\Big(E\\big[X \\big \\vert Y =y \\big]\\Big)= f\\Big(E\\big[Z\\big]\\Big)\\leq E\\big[f\\big(Z\\big)\\big] = E\\big[f\\big(X\\big)\\big \\vert Y=y \\big]$   \n",
    "\n",
    "where the inequality is Jensen's, and $E\\big[f\\big(Z\\big)\\big] = E\\big[f\\big(X\\big)\\big \\vert Y=y \\big]$ is a nice instance of using law of the unconcious statistician (LOTUS). \n",
    "\n",
    "- - - - \n",
    "finally, taking expectatons gives \n",
    "\n",
    "$E\\Big[f\\Big(E\\big[X \\big \\vert Y \\big]\\Big)\\Big]\\leq E\\Big[ E\\big[f\\big(X\\big)\\big \\vert Y \\big]\\Big]=E\\Big[f\\big(X\\big)\\Big]$   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9)  \n",
    "\n",
    "\n",
    "$X = X_1 + X_2 + ... + X_n$   \n",
    "$E\\big[X\\big] = \\bar{X} = E\\big[X_1\\big] +E\\big[X_2\\big]  + ... + E\\big[X_n\\big] = p_1 + p_2 + ... + p_n$    \n",
    "\n",
    "where each $p_i \\gt 0$  \n",
    "\n",
    "each $X_i$ is an indicator r.v. but not necessarily independent.  \n",
    "\n",
    "Let $I$ uniformly at random take on values $1, 2, ..., n$ and be independent of $X_i$ (**tbc: is this neeeded?)**) and let $R$ be any random variable independent of $I$ (though possibly dependent on $X_i$).  \n",
    "\n",
    "*remark:*  $X_I$ can be interpretted as drawing one of the $X_i$ uniformly at random from an urn  \n",
    "\n",
    "(a) show $P\\big(I =i \\big \\vert X_I = 1\\big) = \\frac{p_i}{\\bar{X}}$  \n",
    "gut check:  $\\sum_{i=1}^n P\\big(I =i \\big \\vert X_I = 1\\big) = \\sum_{i=1}^n \\frac{p_i}{\\bar{X}} = \\frac{1}{\\bar{X}}\\big(\\sum_{i=1}^n p_i\\big) = \\frac{1}{\\bar{X}} \\big(\\bar{X}\\big) = 1$    \n",
    "\n",
    "proof:  \n",
    "$P\\big(I =i \\big \\vert X_I = 1\\big) P\\big( X_I = 1\\big) = P\\big(I =i , X_I = 1\\big)= P\\big(X_I = 1 \\big \\vert I = i \\big) P\\big( I = i\\big)= \\frac{P\\big(X_I = 1 \\big \\vert I = i \\big)}{n} = \\frac{P\\big(X_i = 1 \\big \\vert I = i \\big)}{n}= \\frac{p_i}{n}$   \n",
    "dividing by $P\\big( X_I = 1\\big)$  \n",
    "$P\\big(I =i \\big \\vert X_I = 1\\big) = \\frac{p_i}{n\\cdot P( X_I = 1)}= \\frac{p_i}{n\\cdot ( \\frac{1}{n}\\sum_{i=1}^n p_i )} = \\frac{p_i}{\\sum_{i=1}^n p_i } = \\frac{p_i}{\\bar{X}}$  \n",
    "as desired \n",
    "\n",
    "(b) show $E\\big[XR\\big] =E\\big[X\\big]E\\big[R\\big \\vert X_I = 1\\big] $  \n",
    "part a gives us the identity that  \n",
    "$\\bar{X}\\cdot P\\big(I =i \\big \\vert X_I = 1\\big) = p_i$  \n",
    "*remark:*  using the results from page 123, which really is basic conditioning, the above identity allows us to homogenize what we are conditioning on and squeeze out a new result  \n",
    "\n",
    "so from page 123, via basic application of total expectation and linearity of expectations, we have   \n",
    "$E\\big[XR\\big] = \\sum_{i=1}^n p_i \\cdot E\\big[R\\big \\vert X_i = 1\\big] $  \n",
    "$= \\sum_{i=1}^n \\Big(\\bar{X}\\cdot P\\big(I =i \\big \\vert X_I = 1\\big) \\Big)\\cdot E\\big[R\\big \\vert X_i = 1\\big]$   \n",
    "$= \\bar{X} \\cdot \\sum_{i=1}^n   P\\big(I =i \\big \\vert X_I = 1\\big) \\cdot E\\big[R\\big \\vert X_i = 1\\big]$   \n",
    "$= \\bar{X} \\cdot E\\big[R\\big \\vert X_I = 1\\big]$   \n",
    "\n",
    "where we note that via Law of Total Expectation we have  \n",
    "$E\\big[R\\big \\vert X_I = 1\\big] = \\sum_{i=1}^n   P\\big(I =i \\big \\vert X_I = 1\\big) \\cdot E\\big[R\\big \\vert X_i = 1\\big]$  \n",
    "\n",
    "\n",
    "(c)  show $P\\big(X\\gt 0\\big) = E\\big[X\\big] E\\big[\\frac{1}{X}\\big \\vert X_I = 1\\big]$  \n",
    "\n",
    "using results from page 124, we have   \n",
    "$R := \\frac{I_{X\\gt 0}}{X}$  \n",
    "\n",
    "this gives \n",
    "\n",
    "i.)  $P\\big(X\\gt 0\\big) = E\\big[XR\\big] $   \n",
    "ii.) $E\\big[R\\big \\vert X_i = 1]= E\\big[\\frac{1}{X} \\big \\vert X_i = 1\\big]$  \n",
    "(because conditioning on *any* $X_i = 1$ or as we'll see $X_I=1$  implies $I_{X\\gt 0}=1$ \n",
    "\n",
    "but using results from (b) we now have  \n",
    "\n",
    "$P\\big(X\\gt 0\\big) = E\\big[XR\\big]  = \\bar{X} \\cdot E\\big[R\\big \\vert X_I = 1\\big] = \\bar{X} \\cdot E\\big[\\frac{1}{X}\\big \\vert X_I = 1\\big]   $  \n",
    "\n",
    "\n",
    "as desired  \n",
    "\n",
    "**commentary:**  There are probably some deeper ideas lurking underneath this problem that may come out after further contemplation.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10)  \n",
    "\n",
    "This is a rather tricky problem designed to use ex 8 and 9 to show that for non-negative random variable $X$ (that isn't identically zero) comprised of a sum of indicator random variables, $X_i$'s, the conditional expectations inequality is sharper than the second moment method (reference pages 123, 124).  \n",
    "\n",
    "consider:  \n",
    "the convex function $f$  \n",
    "$f: u \\to \\frac{1}{u}$  for $u \\in (0, \\infty)$, \n",
    "\n",
    "\n",
    "$\\frac{P\\big(X\\gt 0\\big)}{\\bar{X}} $  \n",
    "$= \\frac{1}{\\bar{X}}\\sum_{i=1}^n p_i\\cdot E\\big[\\frac{1}{X}\\big \\vert X_i = 1\\big]$   \n",
    "$= \\frac{1}{\\bar{X}}\\sum_{i=1}^n p_i\\cdot E\\big[f(X)\\big \\vert X_i = 1\\big]$  \n",
    "$\\geq \\frac{1}{\\bar{X}}\\sum_{i=1}^n p_i\\cdot f\\Big( E\\big[X\\big \\vert X_i = 1\\big]\\Big)$  \n",
    "$=  \\sum_{i=1}^n \\frac{p_i}{\\bar{X}}\\cdot f\\Big(E\\big[X\\big \\vert X_i = 1\\big]\\Big)$  \n",
    "$=  \\sum_{i=1}^n P\\big(I =i \\big \\vert X_I = 1\\big) \\cdot f\\Big(E\\big[X\\big \\vert X_i = 1\\big]\\Big)$  \n",
    "$=  E\\Big[ f\\Big(E\\big[X\\big \\vert X_i = 1\\big]\\Big)\\Big]$  \n",
    "$\\geq  f\\Big(E\\Big[  E\\big[X\\big \\vert X_i = 1\\big]\\Big]\\Big)$  \n",
    "$=  f\\Big( E\\big[X\\big \\vert X_I=1\\big]\\Big)$  \n",
    "$=\\frac{1}{ E\\big[X\\big \\vert X_I=1\\big]}$  \n",
    "$=\\frac{\\bar{X}}{ \\bar{X} \\cdot E\\big[X\\big \\vert X_I=1\\big]}$  \n",
    "$=\\frac{\\bar{X}}{ E\\big[X^2\\big] } $   \n",
    "\n",
    "where the final line used 9(b)  \n",
    "$E\\big[XR\\big] = \\bar{X} \\cdot E\\big[R\\big \\vert X_I = 1\\big]$, setting $R := X$ gives   \n",
    "$E\\big[X^2\\big] = \\bar{X} \\cdot E\\big[X\\big \\vert X_I = 1\\big]$  \n",
    "\n",
    "\n",
    "thus we have \n",
    "\n",
    "$\\frac{P\\big(X\\gt 0\\big)}{\\bar{X}} \\geq \\frac{1}{\\bar{X}}\\sum_{i=1}^n p_i\\cdot f\\Big( E\\big[X\\big \\vert X_i = 1\\big]\\Big)   = \\frac{1}{\\bar{X}}\\sum_{i=1}^n p_i\\cdot \\frac{1}{E\\big[X\\big \\vert X_i = 1\\big]} \\geq \\frac{\\bar{X}}{ E\\big[X^2\\big] } $     \n",
    "\n",
    "rescaling by $\\bar{X}$  gives  \n",
    "\n",
    "$P\\big(X\\gt 0\\big)\\geq \\sum_{i=1}^n p_i\\cdot \\frac{1}{E\\big[X\\big \\vert X_i = 1\\big]} \\geq \\frac{\\bar{X^2}}{ E\\big[X^2\\big] } $     \n",
    "\n",
    "as desired  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11)   \n",
    "Let $X_i$ be exponential with means $8 + 2i$ for $i = 1, 2, 3$.  Obtain an upper bound on \n",
    "$E\\big[\\max X_i\\big]$   \n",
    "(pages 126 - 128) \n",
    "\n",
    "and compute the exact result when the $X_i$ are independent  \n",
    "\n",
    "solution:  \n",
    "we have   \n",
    "$1=\\sum_{i=1}^3 P\\big(X_i \\gt c^*\\big) = \\exp\\big(-\\frac{c^*}{10}\\big) + \\exp\\big(-\\frac{c^*}{12}\\big) + \\exp\\big(-\\frac{c^*}{14}\\big)$  \n",
    "\n",
    "https://www.wolframalpha.com/input/?i=exp(-c%2F10)+%2B+exp(-c%2F12)+%2B+exp(-c%2F14)+%3D+1\n",
    "\n",
    "yields \n",
    "$c^* \\approx 13.0733$, (recalling that the results hold for any $c\\geq 0$ but $c^*$ gives optimal tightness.  Hence using results from page 127, we have  \n",
    "\n",
    "$E\\big[\\max X_i\\big] \\leq c^* + \\sum_{i=1}^3 \\int_{c^*}^\\infty Pr\\{X_i\\gt y\\}dy =c^* + \\sum_{i=1}^3 \\int_{c^*}^\\infty \\exp\\big(-\\lambda_i y\\big)dy = c^* + \\sum_{i=1}^3 \\lambda_i^{-1}\\cdot \\exp\\big(-\\lambda_i c^*\\big)$   \n",
    "$E\\big[\\max X_i\\big] \\leq 13.0733 + \\Big(10 \\cdot \\exp\\big(-\\frac{13.0733}{10}\\big) + 12 \\cdot \\exp\\big(-\\frac{13.0733}{12}\\big) + 14 \\cdot \\exp\\big(-\\frac{13.0733}{14}\\big) \\Big) \\approx 25.3183$  \n",
    "\n",
    "\n",
    "*exact computations for independent case:*    \n",
    "$\\lambda_1^{-1} = 10$  \n",
    "$\\lambda_2^{-1} = 12$  \n",
    "$\\lambda_3^{-1} = 14$   \n",
    "\n",
    "hence   \n",
    "$\\lambda_1 = \\frac{1}{10}$  \n",
    "$\\lambda_2 = \\frac{1}{12}$  \n",
    "$\\lambda_3 = \\frac{1}{14}$   \n",
    " \n",
    "*approach one:*   \n",
    "(order statistics)  \n",
    "compute the CDF of  \n",
    "$Pr\\{\\max X_i\\leq x\\} = Pr\\{X_1 \\leq x\\}Pr\\{X_2 \\leq x\\}Pr\\{X_3 \\leq x\\} = \\big(1- \\exp(-\\lambda_1x\\big) \\big(1- \\exp(-\\lambda_2x\\big) \\big(1- \\exp(-\\lambda_3x\\big)$  \n",
    "\n",
    "*rationale:* the last 'arrival' has occurred at time $x$ if and only if all arrivals have occurred at that time, which each respective arrival probability is given by the respective CDF, and by stochastic independence we may multiply them.  \n",
    "\n",
    "then integrate the complementary cdf  \n",
    "\n",
    "$E\\big[\\max X_i\\big] =\\int_0^\\infty \\Big(1- \\big(1- \\exp(-\\lambda_1x\\big) \\big(1- \\exp(-\\lambda_2x\\big) \\big(1- \\exp(-\\lambda_3x\\big)\\Big)dx  \\approx 22.176$  \n",
    "\n",
    "https://www.wolframalpha.com/input/?i=integral+from+0+to+infinity+(1-(1-exp(-+x%2F10))(1-exp(-+x%2F12))(1-exp(-+x%2F14)+)++)dx\n",
    "\n",
    "*approach two:*  \n",
    "use conditional expectations \n",
    "\n",
    "define event $A_i = X_i \\text{  is last arrival }$  \n",
    "$\\max X_i = \\mathbb I_{A_1}X_1 +  \\mathbb I_{A_2}X_2 + \\mathbb I_{A_3}X_3  = \\sum_{k=1}^3 \\mathbb I_{A_k}X_k$   \n",
    "$E\\big[\\max X_i\\big]= E\\big[\\sum_{k=1}^3 \\mathbb I_{A_k}X_k\\big] = \\sum_{k=1}^3 E\\big[\\mathbb I_{A_k}X_k\\big]$   \n",
    "\n",
    "where, using the tower property of conditional expectations we see \n",
    "\n",
    "$E\\Big[\\mathbb I_{A_k}X_k\\Big] = E\\Big[E\\big[\\mathbb I_{A_k}X_k\\big \\vert X_k \\big]\\Big] = E\\Big[X_k \\cdot E\\big[\\mathbb I_{A_k}\\big \\vert X_k \\big]\\Big]= E\\Big[X_k \\cdot E\\big[\\mathbb I_{A_k}\\big \\vert X_k = x \\big]\\Big]$  \n",
    "\n",
    "taking a closer look at \n",
    "$E\\big[\\mathbb I_{A_k}\\big \\vert X_k = x \\big] $    \n",
    "in word this reads the probability that event $A_k$ occurs, given $X_k$'s arrival time is x, which is precisely given by the product of the CDFs of the other two arrivals.  E.g. if $k=1$ we have  \n",
    "$E\\big[\\mathbb I_{A_1}\\big \\vert X_1 = x \\big] = Pr\\{X_2 \\leq x\\}Pr\\{X_3 \\leq x\\} = \\big(1- \\exp(-\\lambda_2x\\big) \\big(1- \\exp(-\\lambda_3x\\big)\\Big)$    \n",
    "\n",
    "\n",
    "hence  \n",
    "\n",
    "$E\\Big[\\mathbb I_{A_1}X_1\\Big] = E\\Big[X_1 \\cdot E\\big[\\mathbb I_{A_k}\\big \\vert X_k \\big]\\Big]= E\\Big[X_k \\cdot E\\big[\\mathbb I_{A_k}\\big \\vert X_k = x \\big]\\Big]= E\\Big[X_k \\cdot\\big(1- \\exp(-\\lambda_2x\\big) \\big(1- \\exp(-\\lambda_3x\\big)\\Big)\\Big]$  \n",
    "$=\\int_0^\\infty x\\cdot\\lambda_1 \\exp\\big(-\\lambda_1 x\\big)\\big(1- \\exp(-\\lambda_2x\\big) \\big(1- \\exp(-\\lambda_3x\\big)  dx$    \n",
    "\n",
    "\n",
    "Thus returning to our expected value, we have  \n",
    "$E\\big[\\max X_i\\big]= \\sum_{k=1}^3 E\\big[\\mathbb I_{A_k}X_k\\big] = \\sum_{k=1}^3 \\int_0^\\infty x\\lambda_k \\exp\\big(-\\lambda_k x\\big)\\Big(\\prod_{j\\neq k}\\big(1- \\exp(-\\lambda_j x\\big) \\Big) dx$  \n",
    "$ =  \\int_0^\\infty \\sum_{k=1}^3 x\\lambda_k \\exp\\big(-\\lambda_k x\\big)\\Big(\\prod_{j\\neq k}\\big(1- \\exp(-\\lambda_j x\\big) \\Big) dx \\approx 22.176$   \n",
    "\n",
    "if we add up the three integrals below  \n",
    "https://www.wolframalpha.com/input/?i=integral+from+0+to+infinity+x%2F10+exp(-x%2F10+)(1-exp(-x%2F12))(1-exp(-x%2F14))+dx  \n",
    "https://www.wolframalpha.com/input/?i=integral+from+0+to+infinity+x%2F12+exp(-x%2F12+)(1-exp(-x%2F10))(1-exp(-x%2F14))+dx\n",
    "https://www.wolframalpha.com/input/?i=integral+from+0+to+infinity+x%2F14+exp(-x%2F14+)(1-exp(-x%2F12))(1-exp(-x%2F10))+dx\n",
    "\n",
    "\n",
    "*remark:*  \n",
    "working directly with the CDF is much more expedient, however carefully working through conditional expectations and the associated events gives some useful insights and another way to check our work   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12)   \n",
    "similar to exercise 11, except we now have $n$ uiform random varaibles $X_i \\in (0,1)$.  \n",
    "$E\\big[\\max X_i\\big]$   \n",
    "\n",
    "solution:  \n",
    "we have   \n",
    "$1=\\sum_{i=1}^n P\\big(X_i \\gt c^*\\big) = n\\cdot (1-c^*)$  \n",
    "$ (1-c^*) = \\frac{1}{n}$  \n",
    "$c^* = \\frac{n-1}{n}$  \n",
    "\n",
    "\n",
    "$E\\big[\\max X_i\\big] \\leq c^* + \\sum_{i=1}^n \\int_{c^*}^\\infty Pr\\{X_i \\gt y\\}dy = c^* + \\sum_{i=1}^n \\int_{c^*}^1 (1-y)\\cdot dy = c^* + \\sum_{i=1}^n \\frac{1}{2}\\big(1-c^*\\big)^2= \\frac{n-1}{n} + \\frac{n}{2}\\big(1-\\frac{n-1}{n}\\big)^2$    \n",
    "\n",
    "$E\\big[\\max X_i\\big] \\leq \\frac{n-1}{n} + \\frac{n}{2}\\big(\\frac{n}{n}-\\frac{n-1}{n}\\big)^2 = \\frac{n-1}{n} + \\frac{n}{2}\\big(\\frac{1}{n}\\big)^2 = \\frac{n-1}{n} + \\frac{n}{2}\\cdot\\frac{1}{n^2}= \\frac{2n-2}{2n} + \\frac{1}{2n} = \\frac{2n-1}{2n} = 1 - \\frac{1}{2n}$    \n",
    "\n",
    "\n",
    "*exact answer in the independent case*  \n",
    "given the homogenization we can easily apply the conditional expectations approach and see  \n",
    "$E\\big[\\max X_i\\big]= E\\big[\\sum_{k=1}^n \\mathbb I_{A_k}X_k\\big]= \\sum_{k=1}^n E\\big[\\mathbb I_{A_k}X_k\\big] = n \\cdot E\\big[\\mathbb I_{A_k}X_k\\big]= n\\cdot E\\Big[X_k \\cdot E\\big[\\mathbb I_{A_k}\\big \\vert X_k = x \\big]\\Big]$  \n",
    "\n",
    "$E\\big[\\max X_i\\big] = n\\cdot E\\Big[X_k \\cdot x^{n-1}\\Big]= n\\cdot \\int_0^1 (1x)\\cdot x^{n-1}dx= n\\cdot \\int_0^1  x^{n}dx = \\frac{n}{n+1} = 1 -\\frac{1}{n+1}$  \n",
    "\n",
    "all in all the upper bound seems to be rather close to the exact amount *and* of course covers non-independent cases as well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13)   \n",
    "an extension of ex 12:  \n",
    "\n",
    "consider two $X_i \\sim U(0,1)$ upper bound the maximum and show that it is obtained when $U_2 := 1 - U_1$.  \n",
    "\n",
    "(Note that a more interesting and much stronger statement would be that such a thing is obtained *iff* we have $X_2$ almost surely have that distribution -- however such a thing is not the question at hand.)  \n",
    "\n",
    "*solution:*  \n",
    "using results from the prior problem we have \n",
    "$E\\big[\\max X_i\\big] \\leq  1 - \\frac{1}{2n}$  with $n=2$ giving us \n",
    "$E\\big[\\max X_i\\big] \\leq  1 - \\frac{1}{4}= \\frac{3}{4}$   \n",
    "\n",
    "\n",
    "we also have, using conditional expectations for this specialized dependent case:   \n",
    "\n",
    "$E\\big[\\max X_i\\big]= n\\cdot E\\Big[X_k \\cdot E\\big[\\mathbb I_{A_k}\\big \\vert X_k = x \\big]\\Big] = 2\\cdot E\\Big[X_k \\cdot E\\big[\\mathbb I_{A_k}\\big \\vert X_k = x \\big]\\Big]$  \n",
    "\n",
    "and with some care we can see  \n",
    "$E\\big[\\mathbb I_{A_k}\\big \\vert X_k = x \\big] = 1 \\text{  if x } \\geq \\frac{1}{2} \\text{ and otherwise} =0 $\n",
    "\n",
    "which gives us \n",
    "\n",
    "$2 \\cdot E\\Big[X_k \\cdot E\\big[\\mathbb I_{A_k}\\big \\vert X_k = x \\big]\\Big] = 2\\cdot\\Big(\\big(\\int_0^\\frac{1}{2}(1x)\\cdot 0 dx\\big)+\\big(\\int_\\frac{1}{2}^1(1x)\\cdot 1dx\\big)  \\Big)= 2 \\cdot\\Big(0 + \\frac{3}{8}\\Big) = \\frac{3}{4}$    \n",
    "\n",
    "*remark:*   \n",
    "This problem likely could have been approached via the CDF method (i.e. with order statistics), but it would be somewhat clunky in order to deal with the dependencies.  One of the chief benefits of the conditional expectations approach is that it is quite flexible to accomodate numerous different events and conditioning setups.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 15)   \n",
    "is an open problem of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
