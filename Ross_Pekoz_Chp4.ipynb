{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) \n",
    "For a non-negative random variable $X$, show that \n",
    "\n",
    "$\\Big(E\\big[X^n\\big]\\Big)^\\frac{1}{n}$  \n",
    "\n",
    "is non-decreasing in $n$  \n",
    "\n",
    "remark:  this should remind us quite a bit of Chapter 8 of Cauchy Schwarz Masterclass re: Ladder of Power Means.  \n",
    "\n",
    "since $n$ is a natural number, it suffices to prove \n",
    "\n",
    "$\\Big(E\\big[X^n\\big]\\Big)^\\frac{1}{n} \\leq \\Big(E\\big[X^{n+1}\\big]\\Big)^\\frac{1}{n+1}$  \n",
    "\n",
    "or \n",
    "\n",
    "$E\\big[X^n\\big]^\\frac{n+1}{n} \\leq E\\big[X^{n+1}\\big]$  \n",
    "\n",
    "where $Z:=X^{n+1}$, we have    \n",
    "\n",
    "\n",
    "$E\\big[X^n\\big]^\\frac{n+1}{n} = E\\big[Z^\\frac{n}{n+1}\\big]^\\frac{n+1}{n} = f\\Big(E\\big[Z^\\frac{n}{n+1}\\big]\\Big) \\leq E\\Big[f\\big(Z^\\frac{n}{n+1}\\big)\\Big] =\\Big(E\\big[Z\\big]\\Big)=E\\big[X^{n+1}\\big]  $  \n",
    "\n",
    "\n",
    "which holds by Jensen's Inequality.  For avoidance of doubt, we may confirm that $f$ is the *convex* function given by $f(u) = u^\\frac{n+1}{n}$, which is easily confirmed to be convex by examining the second derivative which gives us \n",
    "\n",
    "for all $u \\geq 0$  \n",
    "\n",
    "$f''(u) = \\frac{(n+1)u^{\\frac{1}{n}-1}}{n^2} \\geq 0$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) X is a standard normal r.v. with density f(x).  \n",
    "*remark:  this is an interesting, and very long, problem that works through importance sampling as a technique for the normal distribution -- which does not have an elementary closed form CDF and of course is a ubiquitous distribution so the length of the exercise seems justifiable.*  \n",
    "\n",
    "Let $g(x) = x e^{-\\frac{x^2}{2}} = x\\cdot f(x)\\cdot \\sqrt{2 \\pi}$   for $x \\in(0, \\infty)$  \n",
    "\n",
    "(we can check that this integrates to one, and each term is real non-negative... note how this is the 'right half' of the standard normal, and in effect the first moment of said distribution -- if it did not integrate to one, we could put whatever value it did integrate to in the denominator as a constant. This is a general recurring theme in that importance sampling of distribution against something related to its $t$ th moment is useful -- in the Chernoff bound case we use MGF in here, but it's a variation on a very common theme.)  \n",
    "\n",
    "for $c \\gt 0$  \n",
    "**(a)**    \n",
    "$P(X\\gt c)= P_f(X\\gt c)= E_g\\big[\\frac{f(X)}{g(X)}\\big \\vert X \\gt c\\big] P_g(X\\gt c)$   \n",
    "$= E_g\\big[\\frac{f(X)}{X\\cdot f(X)\\cdot \\sqrt{2 \\pi}}\\big \\vert X \\gt c\\big] P_g(X\\gt c)$  \n",
    "$= \\frac{1}{\\sqrt{2 \\pi}} E_g\\big[\\frac{1}{X}\\big \\vert X \\gt c\\big] P_g(X\\gt c)$  \n",
    "$= \\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{c^2}{2}} E_g\\big[\\frac{1}{X}\\big \\vert X \\gt c\\big]$   \n",
    "\n",
    "where we note that $P_g(X\\gt c) = \\int_{c}^\\infty x e^{-\\frac{x^2}{2}} = e^{-\\frac{c^2}{2}}$  \n",
    "\n",
    "**(b)**  \n",
    "for any $W \\gt 0$ we have \n",
    "\n",
    "*claim:*    \n",
    "$E\\big[\\frac{1}{W} \\big \\vert W \\gt c\\big] \\leq E\\big[\\frac{1}{W}\\big]$   \n",
    "\n",
    "to make this meaningul, as this is a more general statement, we assume that $E\\big[\\frac{1}{W}\\big] \\lt \\infty$   \n",
    "\n",
    "*proof:*   \n",
    "let $U :=\\frac{1}{W}$  and some constant $\\alpha \\gt 0$ which we'll select at a later time \n",
    "\n",
    "applying total expectation we have   \n",
    "$E\\big[U\\big] = P\\big(U \\lt \\alpha\\big) \\cdot E\\big[U \\big \\vert U \\lt \\alpha\\big] + P\\big(U \\geq \\alpha\\big) \\cdot E\\big[U \\big \\vert U \\geq \\alpha\\big] $   \n",
    "\n",
    "noting that an underlying point-wise bound gives us:  \n",
    "\n",
    "$E\\big[U \\big \\vert U \\lt \\alpha\\big] \\lt \\alpha \\leq E\\big[U \\big \\vert U \\geq \\alpha\\big]$  \n",
    "\n",
    "hence we have \n",
    "\n",
    "$E\\big[U \\big \\vert U \\lt \\alpha\\big]$  \n",
    "$= P\\big(U \\lt \\alpha\\big) \\cdot E\\big[U \\big \\vert U \\lt \\alpha\\big] + P\\big(U \\geq \\alpha\\big) \\cdot E\\big[U \\big \\vert U \\lt \\alpha\\big]$  \n",
    "$\\lt  P\\big(U \\lt \\alpha\\big) \\cdot E\\big[U \\big \\vert U \\lt \\alpha\\big] + P\\big(U \\geq \\alpha\\big) \\cdot \\alpha$  \n",
    "$\\leq  P\\big(U \\lt \\alpha\\big) \\cdot E\\big[U \\big \\vert U \\lt \\alpha\\big] + P\\big(U \\geq \\alpha\\big) \\cdot E\\big[U \\big \\vert U \\geq \\alpha\\big]$  \n",
    "$\\leq  E\\big[U\\big]$  \n",
    "\n",
    "now considering that the event \n",
    "\n",
    "$U \\lt \\alpha$  is equivalent to the event $\\frac{1}{W} \\lt \\alpha$ which is equivalent to $W \\gt \\frac{1}{\\alpha}$, we now have the inequality of \n",
    "\n",
    "\n",
    "$E\\big[\\frac{1}{W} \\big \\vert W \\gt \\alpha^{-1}\\big] \\leq  E\\big[\\frac{1}{W}\\big]$   \n",
    "\n",
    "setting $\\alpha := c^{-1}$ i.e. $c = \\alpha^{-1}$, gives us  \n",
    "\n",
    "$E\\big[\\frac{1}{W} \\big \\vert W \\gt c\\big] \\leq  E\\big[\\frac{1}{W}\\big]$   \n",
    "\n",
    "as desired  \n",
    "\n",
    "**(c)**  \n",
    "Prove   \n",
    "$P(X\\gt c) \\leq \\frac{1}{2}e^{\\frac{-c^2}{2}}$   \n",
    "\n",
    "where it is understood that $P(X\\gt c) = P_f(X\\gt c)$  \n",
    "\n",
    "using (a), then (b), we have \n",
    "\n",
    "$P(X\\gt c) $  \n",
    "$= \\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{c^2}{2}} E_g\\big[\\frac{1}{X}\\big \\vert X \\gt c\\big]$  \n",
    "$\\leq \\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{c^2}{2}} E_g\\big[\\frac{1}{X}\\big]$  \n",
    "$=\\Big(\\frac{1}{\\sqrt{2 \\pi}}E_g\\big[\\frac{1}{X}\\big]\\Big)e^{-\\frac{c^2}{2}} $  \n",
    "$= \\frac{1}{2}e^{-\\frac{c^2}{2}}$  \n",
    "\n",
    "as required.  Note that  $\\frac{1}{2\\pi} E_g\\big[\\frac{1}{X}\\big] = \\frac{1}{2\\pi}\\int_0^\\infty e^{-\\frac{x^2}{2}} = \\frac{1}{2}$  i.e. it is the right half of the standard normal random variables' density.  \n",
    "- - - - \n",
    "*begin interlude*  \n",
    "using the 'slip-in trick' from ex. 7.8 in *The Cauchy Schwarz Masterclass* we have   \n",
    "$P(X\\gt c) \\leq \\frac{1}{c\\cdot \\sqrt{2\\pi}}e^{\\frac{-c^2}{2}}$  \n",
    "which in general is a much tighter bound, except for small $c$ (we can fine tune it, but any $c\\geq 1$ is sufficient)  \n",
    "\n",
    "where the slip-in trick is \n",
    "\n",
    "$P_f(X\\gt c) = \\frac{1}{\\sqrt{2 \\pi}}\\int_c^\\infty e^\\frac{-x^2}{2} dx$  \n",
    "$= \\frac{1}{\\sqrt{2 \\pi}}\\int_c^\\infty \\frac{x}{x} e^\\frac{-x^2}{2} dx$  \n",
    "$\\leq \\frac{1}{\\sqrt{2 \\pi}}\\int_c^\\infty \\frac{x}{c} e^\\frac{-x^2}{2} dx$  \n",
    "$= \\frac{1}{c\\cdot\\sqrt{2 \\pi}}\\int_c^\\infty x e^\\frac{-x^2}{2} dx$  \n",
    "$= \\frac{1}{c\\cdot\\sqrt{2 \\pi}} P_g(X\\gt c)$  \n",
    "$= \\frac{1}{c\\cdot\\sqrt{2 \\pi}} e^{-\\frac{c^2}{2}}$ \n",
    "\n",
    "where in the last 3 lines we used the results from the end of part (a) which stated  \n",
    "$P_g(X\\gt c) = \\int_{c}^\\infty x e^{-\\frac{x^2}{2}} = e^{-\\frac{c^2}{2}}$  \n",
    "\n",
    "*end interlude*  \n",
    "- - - - \n",
    "\n",
    "**(d)**  \n",
    "show that  \n",
    "$E_g\\big[X \\big \\vert X \\gt c\\big] = c + e^{c^2}\\sqrt{2\\pi}\\cdot P(X\\gt c) $   \n",
    "equivalently, using (a), prove that \n",
    "\n",
    "$E_g\\big[X \\big \\vert X \\gt c\\big] = c +  E_g\\big[\\frac{1}{X} \\big \\vert X \\gt c\\big]$    \n",
    "\n",
    "*proof*  \n",
    "$E_g\\big[X \\big \\vert X \\gt c\\big]$  \n",
    "$= \\frac{1}{P_g(X\\gt c)}\\int_c^\\infty x^2 e^{\\frac{-x^2}{2}}$   \n",
    "$= \\frac{1}{P_g(X\\gt c)}\\Big(\\big(-x e^{-\\frac{x^2}{2}}\\Big|_c^\\infty\\big)  + \\int_c^\\infty e^{-\\frac{x^2}{2}}dx\\Big)$ (via integration by parts)   \n",
    "$= \\frac{1}{P_g(X\\gt c)}\\big(-x e^{-\\frac{x^2}{2}}\\Big|_c^\\infty\\big) + \\frac{1}{P_g(X\\gt c)}\\int_c^\\infty e^{-\\frac{x^2}{2}}dx$   \n",
    "$= \\frac{1}{P_g(X\\gt c)}\\big(-x e^{-\\frac{x^2}{2}}\\Big|_c^\\infty\\big)  + E_g\\big[\\frac{1}{X} \\big \\vert X \\gt c\\big]$   \n",
    "$= \\frac{1}{P_g(X\\gt c)}\\big(c e^{-\\frac{c^2}{2}}\\big)  + E_g\\big[\\frac{1}{X} \\big \\vert X \\gt c\\big]$   \n",
    "$= c\\cdot \\big(\\frac{e^{-\\frac{c^2}{2}}}{P_g(X\\gt c)}\\big)  + E_g\\big[\\frac{1}{X} \\big \\vert X \\gt c\\big]$   \n",
    "$= c  + E_g\\big[\\frac{1}{X} \\big \\vert X \\gt c\\big]$   \n",
    "\n",
    "- - - - \n",
    "where on the 2nd to last line we used the fact that was stated at the end of (a), i.e. that \n",
    "\n",
    "$P_g(X\\gt c) = \\int_{c}^\\infty x e^{-\\frac{x^2}{2}} = e^{-\\frac{c^2}{2}}$  \n",
    "- - - - \n",
    "**(e)**  \n",
    "The book says to use Jensen's Inequality here, though what we really want is Cauchy Schwarz.  I.e. Cauchy Schwarz tells us \n",
    "\n",
    "*lemma*  \n",
    "$1 = 1^2 = \\Big(\\frac{1}{P_g(X\\gt c)} \\int_c^\\infty 1 \\cdot g(x)dx\\Big)^2 = \\Big(\\int_c^\\infty \\big(\\frac{1}{x}\\frac{g(x)}{P_g(X\\gt c)}\\big)^\\frac{1}{2}\\cdot \\big(x\\frac{g(x)}{P_g(X\\gt c)}\\big)^\\frac{1}{2} dx\\Big)^2 $  \n",
    "$\\leq \\Big(\\int_c^\\infty \\frac{1}{x}\\frac{g(x)}{P_g(X\\gt c)}dx\\Big) \\Big(\\int_c^\\infty x\\frac{g(x)}{P_g(X\\gt c)}dx\\Big) $  \n",
    "$= \\Big(\\frac{1}{P_g(X\\gt c)} \\int_c^\\infty \\frac{1}{x}\\cdot g(x)dx\\Big) \\Big(\\frac{1}{P_g(X\\gt c)} \\int_c^\\infty x\\cdot g(x)dx\\Big) $  \n",
    "$= E_g\\big[\\frac{1}{X}\\big \\vert X \\gt c\\big]\\cdot E_g\\big[X\\big \\vert X \\gt c\\big]$  \n",
    "\n",
    "we've thus proven for positive random variables and $c \\gt 0$  \n",
    "$1 \\leq  E_g\\big[\\frac{1}{X}\\big \\vert X \\gt c\\big]\\cdot E_g\\big[X\\big \\vert X \\gt c\\big]$   \n",
    "\n",
    "by Cauchy Schwarz  \n",
    "\n",
    "- - - \n",
    "now using the results from (d) \n",
    "\n",
    "$E_g\\big[X \\big \\vert X \\gt c\\big] = c + e^{c^2}\\sqrt{2\\pi}\\cdot P(X\\gt c) $   \n",
    "\n",
    "and multiplying each side by $P(X\\gt c)$, we have  \n",
    "\n",
    "$P(X\\gt c)\\cdot E_g\\big[X \\big \\vert X \\gt c\\big] = P(X\\gt c)\\cdot c + e^{c^2}\\sqrt{2\\pi}\\cdot P(X\\gt c)^2 $   \n",
    "\n",
    "subsitituing in the results for part(a) on the LHS, and application of the lemma, gives us \n",
    "\n",
    "$\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{c^2}{2}} \\leq \\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{c^2}{2}} \\cdot E_g\\big[\\frac{1}{X}\\big \\vert X \\gt c\\big]\\cdot E_g\\big[X \\big \\vert X \\gt c\\big] = c\\cdot P(X\\gt c) c + e^{c^2}\\sqrt{2\\pi}\\cdot P(X\\gt c)^2 $   \n",
    "\n",
    "or more simply   \n",
    "$\\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{c^2}{2}} \\leq c \\cdot P(X\\gt c) + e^{c^2}\\sqrt{2\\pi}\\cdot P(X\\gt c)^2 $   \n",
    "\n",
    "**(f)**  \n",
    "using the prior inequality we can see the outlines of a quadratic polynomial, i.e. \n",
    "\n",
    "$e^{c^2}\\sqrt{2\\pi}\\cdot P(X\\gt c)^2 + c\\cdot P(X\\gt c)  - \\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{c^2}{2}} \\geq 0 $  \n",
    "\n",
    "if we ignore the inequality momentarily, we can see the polynomial \n",
    "\n",
    "$p(z) = e^{c^2}\\sqrt{2\\pi}\\cdot z^2 + c\\cdot z - \\frac{1}{\\sqrt{2 \\pi}}e^{-\\frac{c^2}{2}}$  \n",
    "\n",
    "what we want to show is that this polynomial has two distinct roots, $\\lambda_1 \\gt 0$ and $\\lambda_2 \\lt 0$.  This means that our domain for $z$  we know that $p(z) \\geq p(\\lambda_1) =0$  which implies that $z \\geq \\lambda_1$ -- specifically if we have $z^* \\in(0\\lambda_1)$ then our inequality would tell us that $0 \\leq p(z^*) \\lt 0$  which is a contradiction.  \n",
    "\n",
    "\n",
    "**(g)**  \n",
    "via use of wolframlpha  \n",
    "https://www.wolframalpha.com/input/?i=roots+of+cz+%2B+exp(c%5E2%2F2)+*+sqrt(2+pi)+z%5E2+-+1%2Fsqrt(2pi)exp(-c%5E2%2F2)\n",
    "\n",
    "we see $\\lambda_2 \\lt 0$ and $\\lambda_1 \\gt 0$, with  \n",
    "\n",
    "$\\lambda_1 = \\frac{e^{-c^2}\\cdot \\big(\\sqrt{(c^2 + 4)e^{c^2}} - c e^{\\frac{c^2}{2}}\\big)}{2\\sqrt{2\\pi}} $  \n",
    "$ = \\frac{1}{{2\\sqrt{2\\pi}}}\\big(e^{-c^2}e^{\\frac{c^2}{2}}\\sqrt{c^2 + 4} - c e^{\\frac{-c^2}{2}}\\big)$  \n",
    "$ = \\frac{1}{{2\\sqrt{2\\pi}}}\\big(e^{\\frac{-c^2}{2}}\\sqrt{c^2 + 4} - c e^{\\frac{-c^2}{2}}\\big)$  \n",
    "$ = \\frac{1}{{2\\sqrt{2\\pi}}}\\big(\\sqrt{c^2 + 4} - c\\big)e^{-\\frac{c^2}{2}}$  \n",
    "\n",
    "hence \n",
    "\n",
    "$P(X\\gt c) \\geq \\lambda_1 \\geq  \\frac{1}{{2\\sqrt{2\\pi}}}\\big(\\sqrt{c^2 + 4} - c\\big)e^{-\\frac{c^2}{2}}$  \n",
    "\n",
    "or more succinctly:  \n",
    "$P(X\\gt c) \\geq \\frac{1}{{2\\sqrt{2\\pi}}}\\big(\\sqrt{c^2 + 4} - c\\big)e^{-\\frac{c^2}{2}}$   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Chernoff Bound on the tail of Poisson distribution  \n",
    "Let X be Poisson with mean $\\lambda$.  Show that for $n\\geq \\lambda$ (i.e. natural number $n$ that is 'to the right' of the mean), the Chernoff bound gives us \n",
    "\n",
    "**claim:**    \n",
    "$P\\big(X\\geq n\\big) \\leq \\frac{e^{-\\lambda} (\\lambda e )^n}{n^n}$  \n",
    "\n",
    "**remark:**    \n",
    "this looks awfully similar to the PMF value (not the complementary CDF) of the Poisson at $n$ except we have $(\\frac{n}{e})^n$ in the denominator instead of $n!$.  There are ideas of the stirling approximation and also truncation of Taylor series lurking here... \n",
    "\n",
    "It may be useful to compare this to a simplistic geometric series bound on the residual of the exponential series as well as some of the better used more sophisticated bounds \n",
    "\n",
    "**proof:**  \n",
    "The Chernoff Bound gives us, for $t \\gt 0$   \n",
    "\n",
    "$P\\big(X\\geq n\\big) \\leq \\inf M(t) e^{-tn} \\leq   M(t) e^{-tn} \\leq \\exp\\big(\\lambda (e^t - 1)\\big) e^{-tn} = \\exp\\big(\\lambda e^t - \\lambda -tn\\big) = e^{-\\lambda} \\exp\\big(\\lambda e^t  -tn\\big) = e^{-\\lambda} \\frac{\\exp (\\lambda e^t)}{\\exp(t)^n}$ \n",
    "\n",
    "the clue is that $\\lambda \\leq n$ and we want to simplify and cancel many terms, so suppose we select \n",
    "\n",
    "$e^t := \\frac{n}{\\lambda} \\geq 1$ \n",
    "which implies that $t\\geq 0$.  \n",
    "\n",
    "(*Technical nit:* we really should insist on $t \\gt 0$ and hence $n \\gt \\lambda$ though the inequality in the stated question in the book is not strict for some reason.)  \n",
    "\n",
    "Thus we have \n",
    "\n",
    "$P\\big(X\\geq n\\big) \\leq e^{-\\lambda} \\frac{\\exp (\\lambda e^t)}{\\exp(t)^n} = e^{-\\lambda} \\frac{\\exp (\\lambda \\frac{n}{\\lambda})}{(\\frac{n}{\\lambda})^n}= e^{-\\lambda} \\frac{\\exp (n)}{(\\frac{n}{\\lambda})^n} = e^{-\\lambda} \\frac{(e\\lambda)^n }{n^n}$  \n",
    "\n",
    "as desired.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Moment Bound via Importance Sampling  \n",
    "\n",
    "referencing Corollary 4.4. on page 115,  \n",
    "for some $c \\gt 0$ we have   \n",
    "\n",
    "$P_f(X\\gt c)= E_g\\big[\\frac{f(X)}{g(X)}\\big \\vert X \\gt c\\big] P_g(X\\gt c)$  \n",
    "\n",
    "where $f$ is the density (or probability mass function) for $X$ and $g$ is a different mass function. \n",
    "\n",
    "in this case for natural number $t$, any $X$ may be used so long as $x^t \\geq 0$ and $x$ isn't zero WP1-- in this case we can use: \n",
    "\n",
    "**key insights on positivity here...**  \n",
    "\n",
    "$g(x) = \\frac{f(x)x^t}{E[X^t]} = \\frac{f(x)x^t}{E_f[X^t]}= \\frac{f(x)x^t}{\\mu_t}$  \n",
    "\n",
    "where for avoidance of doubt the t'th moment of X is with respect to $f$  \n",
    "\n",
    "noting that by construction this is real non-negative and integrates to one.  Note that this is a general technique similar in spirit to the construction of $g$ in problem 2 and extremely similar to construction of $g$ on page 119 for the Chernoff Bound.  \n",
    "\n",
    "now plugging this into our importance sampling identity gives us \n",
    "\n",
    "**main argument:**   \n",
    "\n",
    "$P_f(X\\gt c)= E_g\\big[\\frac{f(X)}{\\frac{f(X)X^t}{E_f[X^t]}}\\big \\vert X \\gt c\\big] P_g(X\\gt c)$  \n",
    "$= E_f[X^t]\\cdot E_g\\big[\\frac{1}{X^t}\\big \\vert X \\gt c\\big] P_g(X\\gt c)$  \n",
    "$\\leq E_f[X^t]\\cdot E_g\\big[\\frac{1}{X^t}\\big \\vert X \\gt c\\big]$  (probabilities are in $[0,1]$)   \n",
    "$\\lt E_f[X^t]\\cdot E_g\\big[\\frac{1}{c^t}\\big \\vert X \\gt c\\big]$   \n",
    "$= E_f[X^t] c^{-t} \\cdot E_g\\big[1 \\big \\vert X \\gt c\\big]$  \n",
    "$= E_f[X^t] c^{-t} \\cdot 1$  \n",
    "$= E_f[X^t] c^{-t}$   \n",
    "$= E[X^t] c^{-t}$    \n",
    "\n",
    "where we note that the second inequality is justified, because, conditioned on $X\\gt c $   \n",
    "we have $X^t \\gt c^t$   and $\\frac{1}{X^t} \\lt \\frac{1}{c^t}$, and observing this point-wise bound, we may take the expectation.  \n",
    "\n",
    "**commentary:**  \n",
    "note that if $X\\geq 0$ we can get the same result via Markov's Inequality, in particular, observe the point-wise bound:  \n",
    "\n",
    "$\\mathbb I_{X\\gt c }\\cdot c \\leq X$  \n",
    "raise to the $t$th power, recalling that indicators are idempotent, giving us  \n",
    "$\\mathbb I_{X\\gt c }\\cdot c^t \\leq X^t$  \n",
    "taking expectations gives  \n",
    "$Pr\\{X \\gt c\\}\\cdot c^t = E\\big[\\mathbb I_{X\\gt c }\\big]\\cdot c^t \\leq E\\big[X^t\\big]$  \n",
    "and dividing by $c^t$ gives us  \n",
    "$Pr\\{X \\gt c\\}\\leq E\\big[X^t\\big] c^{-t}$   \n",
    "- - - - \n",
    "note that the Markov Inequality approach is quicker but requires $X\\geq 0$ which is not required by the importance sampling approach.  However the more flexible approach may lead to weaker results, e.g. when $t$ is even.  Consider some random variable will all moments that is symmetric about zero.  If we select say $t=2$ we are setting up for Chevbyshev's Inequality.  The Importance Sampling approach immediately gives us   \n",
    "\n",
    "$P_f(X\\gt c) \\lt E[X^2] c^{-2}$  \n",
    "\n",
    "However the Markov Inequality setup can be written as \n",
    "\n",
    "$\\mathbb I_{X^2\\gt c^2 }\\cdot c^2 \\leq X^2$  \n",
    "taking expectations and simplifying, we have \n",
    "$Pr\\{X^2 \\gt c^2\\}\\leq E\\big[X^2\\big] c^{-2}$   or  \n",
    "$Pr\\{\\big \\vert X\\big \\vert \\gt c\\}\\leq E\\big[X^2\\big] c^{-2}$   \n",
    "\n",
    "which is a much stronger claim (in fact about twice as strong)  \n",
    "\n",
    "So additional care is needed in dealing with and using negative numbers.  However, this problem does nicely illustrate usage of a technique to 'back into' an identity you want via importance sampling.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5)  \n",
    "Fill in the details of the proof that for independent bernouli random variables, $X_1, ..., X_n$ and some $c\\gt 0$\n",
    "**claim: **  \n",
    "$P\\Big(S_n - E\\big[S_n\\big] \\leq -c\\Big) \\leq e^{-\\frac{2c^2}{n}}$  \n",
    "\n",
    "*remark: *  \n",
    "there are in essence two approaches here \n",
    "\n",
    "The one being suggested, and from this chapter comes from Importance Sampling.  The alternative comes from a Martingale setup and is worked through via Azuma-Hoeffding on pages 100 and 101 of the text.  \n",
    "\n",
    "**proof:**  \n",
    "*via Importance Sampling*  \n",
    "letting $Y: = S_n - E\\big[S_n\\big]$  \n",
    "where $Y$ is a centered random variable\n",
    "and $Z:= - Y$ \n",
    "is also a centered random variable, we have \n",
    "\n",
    "$P\\big(Y \\leq c\\big) = P\\big(Z \\geq c\\big)$  \n",
    "$= P\\big(e^{tZ} \\geq e^{tc}\\big)$    \n",
    "$\\leq e^{-tc}E\\big[e^{t(Z)}\\big]$  (Markov Inequality)  \n",
    "$= e^{-tc}E\\Big[\\exp\\big(\\sum_{i=1}^{n} t(\\bar{X_i} - X)\\big)\\Big]$    \n",
    "$= e^{-tc}E\\Big[\\prod_{i=1}^n \\exp\\big(t(\\bar{X_i} - X)\\big)\\Big]$  (summation in exponential domain is product)  \n",
    "$= e^{-tc}\\prod_{i=1}^nE\\Big[ \\exp\\big(t(\\bar{X_i} - X)\\big)\\Big]$ (by independence)  \n",
    "$\\leq e^{-tc}e^{\\frac{n t^2}{8}}$   \n",
    "\n",
    "\n",
    "Because we take advantage of positivity and observe a point-wise bound where  \n",
    "$Z_i:=(\\bar{X_i} - X_i)$  is a bounded and centered random variable so we know (see reference mentioned in problem 7)  \n",
    "\n",
    "$E\\Big[e^{t(\\bar{X_i} - X)}\\big)\\Big] = E\\Big[e^{tZ_i}\\Big] \\leq e^{\\frac{t^2}{8}}$  \n",
    "\n",
    "\n",
    "setting $t:= \\frac{4c}{n}$ gives us \n",
    "\n",
    "$e^{-tc}e^{\\frac{n t^2}{8}} =e^{\\frac{n t^2 - 8tc}{8}} = e^{\\frac{n (\\frac{4c}{n})^2 - 8 (\\frac{4c}{n})c}{8}} = e^{\\frac{ \\frac{16c^2}{n} - \\frac{32c^2}{n}}{8}}= e^{\\frac{ \\frac{-16c^2}{n}}{8}}= e^\\frac{-2c^2}{n}  $   \n",
    "\n",
    "which completes the exercise  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alternative derivation of Chernoff Bounds for Bernoulis:**  \n",
    "the derivation of Chernoff bounds is a bit odd in 4.4 as it starts developing it via importance sampling but then switches to a more 'traditional' chernoff bound technique which seems less motivated and does not tie in as nicely with the importance sampling. To develop this, consider $Z= -Y$ our centered random variable that is bounded in $[a,A]$.  Note that being bounded is not needed in general for Chernoff Bounds -- though we do need for the MGF to exist in an open interval around zero-- (e.g. reference problem 3 where we bound the tail of a Poisson) however we do need to be careful and recognize that we can only apply Hoeffding's Lemma in the manner of this problem, for a bounded random variable.  \n",
    "\n",
    "starting with the derivation in the text:   \n",
    "\n",
    "$g(z) = \\frac{e^{tz}f(z)}{M(t)}$  \n",
    "i.e. $Z$ has pmf / density of $f$ and $g$ is the titled distribution of $Z$  and of course \n",
    "$M(t) = E_f\\big[e^{tZ}\\big]$  \n",
    "is the moment generating function of $Z$  \n",
    "\n",
    "Now via importance sampling we have, for any $c\\gt 0$ \n",
    "\n",
    "$P_f\\big(Z\\geq c\\big) $  \n",
    "$= E_g\\Big[\\frac{f(Z)}{g(Z)}\\big \\vert Z \\geq c\\Big]$   \n",
    "$= E_g\\Big[\\frac{f(Z)}{\\frac{e^{tZ}f(Z)}{M(t)}}\\big \\vert Z \\geq c\\Big]$   \n",
    "$= E_g\\Big[M(t)\\exp(-tZ)\\big \\vert Z \\geq c\\Big]P_g\\big(Z\\geq c\\big)$   \n",
    "$\\leq  E_g\\Big[M(t)\\exp(-tZ)\\big \\vert Z \\geq c\\Big]$   (by positivitiy and probabilities being bounded in [0,1])  \n",
    "$= M(t)\\cdot E_g\\Big[\\exp(-tZ)\\big \\vert Z \\geq c\\Big]$  \n",
    "$\\leq  M(t)e^{-tc}$    \n",
    "- - - - \n",
    "justfication:  \n",
    "conditioning on $Z \\geq c$ we have (whether using $f$ or $g$, *in this conditional world* we have the point-wise bound:  \n",
    "$Z \\geq c$  \n",
    "$-tZ \\leq -tc$  (negative sign flips the inequality and $t \\gt 0$  \n",
    "$e^{-tZ} \\leq e^{-tc}$   \n",
    "$E_g \\Big[e^{-tZ}\\big\\vert Z \\geq c \\Big] \\leq E_g\\Big[e^{-tc}\\big\\vert Z\\Big]= e^{-tc} $ \n",
    "i.e. taking expectations with respect to $g$, in this conditional world  \n",
    "- - - - \n",
    "the above holds for all $t\\gt 0$ so we may select the minimizing value of $t$   \n",
    "$P_f\\big(Z\\geq c\\big) \\leq \\frac{\\text{inf}}{t\\gt 0} M(t)e^{-tc}$   \n",
    "\n",
    "remark: this is a slightly longer form of what is shown directly in the book.  \n",
    "Now in the case of sums of centered bernoulis (whether working with Y or its negative Z), we have \n",
    "\n",
    "$Z := Z_1 + Z_2 + ... + Z_n$  \n",
    "\n",
    "using the fact that MGF's convert convolutions/sums to products, we have \n",
    "\n",
    "$M_(t) = M_{Z_i}(t)^n$  \n",
    "\n",
    "$P\\big(Z \\geq c \\big) \\leq \\frac{\\text{inf}}{t\\gt 0} M(t)e^{-tc} = \\frac{\\text{inf}}{t\\gt 0} M_{Z_i}(t)^ne^{-tc}   \\leq \\big(e^{\\frac{t^2}{8}}\\big)^ne^{-tc} = e^{\\frac{nt^2 - 8tc}{8}}$  \n",
    "by Hoeffdings lemma, with A-a = 1, as it is for Bernoulis.  See \"martingales.ipynb\" for more information.  \n",
    "\n",
    "the graceful finish comes from selecting $t:= \\frac{4c}{n}$ which gives \n",
    "\n",
    "$P\\big(Z \\geq c \\big) \\leq e^{\\frac{nt^2 - 8tc}{8}} =\\exp\\big(\\frac{n \\frac{16c^2}{n^2} - 8\\frac{4c}{n}c}{8}\\big)  = e^{\\frac{2c^2 - 4c^2}{n}} = e^{\\frac{-2c^2}{n}}$   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6)  \n",
    "this problem is essentially an immediate result from applying the (5) and the associated result proven in the chapter.  \n",
    "\n",
    "$P\\Big( \\big \\vert X - np\\big \\vert \\geq c \\Big) = P\\Big(  X - np \\geq c \\Big) + P\\Big(  X - np \\leq -c \\Big) \\leq e^{-\\frac{2c^2}{n}} + e^{-\\frac{2c^2}{n}} = 2e^{-\\frac{2c^2}{n}}$  \n",
    "\n",
    "(b) choosing $c = \\alpha np$ gives the result here using standard chernoof bound.  However it seems worth pointing out that $alpha$ should be non-negative, though the text does not specify this for some reason.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7)   \n",
    "*Problem:* Give detailes of the proof of lemma 4.7 (i.e. Hoeffding's lemma)  \n",
    "*Solution:* see \"Hoeffding's lemma\" under 'martingales' notebook for an original take at the proof    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8)  \n",
    "prove that for convex function $f$ and random variables $X,Y$ we have the below refinement of Jensen's Inequality  \n",
    "*claim:*  \n",
    "$f\\Big(E\\big[X\\big]\\Big) \\leq  E\\Big[f\\Big(E\\big[X \\big \\vert Y \\big]\\Big)\\Big]\\leq E\\Big[f\\big(X\\big)\\Big]$  \n",
    "\n",
    "*proof:*  \n",
    "the left hand side is immediate because $Z:=E\\big[X \\big \\vert Y \\big]$ is a random variable, and by Jensen's Inequality  \n",
    "\n",
    "$f\\Big(E\\Big[X\\Big]\\Big) = f\\Big(E\\Big[E\\big[X \\big \\vert Y \\big]\\Big]\\Big)  = f\\Big(E\\Big[Z\\Big]\\Big) \\leq  E\\Big[f\\Big(Z\\Big)\\Big]$  \n",
    "\n",
    "the right hand side takes a little more care, but may be proven as \n",
    "\n",
    "$f\\Big(E\\big[X \\big \\vert Y \\big]\\Big)\\leq E\\big[f\\big(X\\big)\\big \\vert Y \\big]$   \n",
    "\n",
    "note that exercise 4 of chapter 3 was to prove the slightly more general form of the above inequality:  \n",
    "$f\\Big(E\\big[X \\big \\vert \\mathscr F \\big]\\Big)\\leq E\\big[f\\big(X\\big)\\big \\vert \\mathscr F \\big]$    \n",
    " \n",
    "if  $Y \\in \\mathscr F$ (and the only element of said sigmal field) then the above immediately follows.  \n",
    "\n",
    "- - - - \n",
    "if we sacrifice a touch of formalism, and reason over individual sample paths, we see that \n",
    "$f\\Big(E\\big[X \\big \\vert Y =y \\big]\\Big)\\leq E\\big[f\\big(X\\big)\\big \\vert Y=y \\big]$  which follows from the familiar fact of Jensen's Inequality / linear lower bound property of 'tangent' lines for convex functions, or equivalently, for each $Y(\\omega) = y$ we may create a random variable $Z$ which the distribution of $X \\big \\vert Y =y$, and we then recognize that   \n",
    "\n",
    "$f\\Big(E\\big[X \\big \\vert Y =y \\big]\\Big)= f\\Big(E\\big[Z\\big]\\Big)\\leq E\\big[f\\big(Z\\big)\\big] = E\\big[f\\big(X\\big)\\big \\vert Y=y \\big]$   \n",
    "\n",
    "where the inequality is Jensen's, and $E\\big[f\\big(Z\\big)\\big] = E\\big[f\\big(X\\big)\\big \\vert Y=y \\big]$ is a nice instance of using law of the unconcious stastician (LOTUS). \n",
    "\n",
    "- - - - \n",
    "finally, taking expectatons gives \n",
    "\n",
    "$E\\Big[f\\Big(E\\big[X \\big \\vert Y \\big]\\Big)\\Big]\\leq E\\Big[ E\\big[f\\big(X\\big)\\big \\vert Y \\big]\\Big]=E\\Big[f\\big(X\\big)\\Big]$   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9)  \n",
    "\n",
    "** more details need to be filled in here**  \n",
    "\n",
    "$X = X_1 + X_2 + ... + X_n$   \n",
    "$E\\big[X\\big] = \\bar{X} = E\\big[X_1\\big] +E\\big[X_2\\big]  + ... + E\\big[X_n\\big] = p_1 + p_2 + ... + p_n$    \n",
    "\n",
    "where each $p_i \\gt 0$  \n",
    "\n",
    "each $X_i$ is an indicator r.v. but not necessarily independent.  \n",
    "\n",
    "Let $I$ uniformly at random take on values $1, 2, ..., n$ and be independent of $X_i$ (**tbc: is this neeeded?)**) and let $R$ be any random variable independent of $I$ (though possibly dependent on $X_i$).  \n",
    "\n",
    "*remark:*  $X_I$ can be interpretted as drawing one of the $X_i$ uniformly at random from an urn  \n",
    "\n",
    "(a) show $P\\big(I =i \\big \\vert X_I = 1\\big) = \\frac{p_i}{\\bar{X}}$  \n",
    "gut check:  $\\sum_{i=1}^n P\\big(I =i \\big \\vert X_I = 1\\big) = \\sum_{i=1}^n \\frac{p_i}{\\bar{X}} = \\frac{1}{\\bar{X}}\\big(\\sum_{i=1}^n p_i\\big) = \\frac{1}{\\bar{X}} \\big(\\bar{X}\\big) = 1$    \n",
    "\n",
    "proof:  \n",
    "$P\\big(I =i \\big \\vert X_I = 1\\big) P\\big( X_I = 1\\big) = P\\big(I =i , X_I = 1\\big)= P\\big(X_I = 1 \\big \\vert I = i \\big) P\\big( I = i\\big)= \\frac{P\\big(X_I = 1 \\big \\vert I = i \\big)}{n} = \\frac{P\\big(X_i = 1 \\big \\vert I = i \\big)}{n}= \\frac{p_i}{n}$   \n",
    "dividing by $P\\big( X_I = 1\\big)$  \n",
    "$P\\big(I =i \\big \\vert X_I = 1\\big) = \\frac{p_i}{n\\cdot P( X_I = 1)}= \\frac{p_i}{n\\cdot ( \\frac{1}{n}\\sum_{i=1}^n p_i )} = \\frac{p_i}{\\sum_{i=1}^n p_i } = \\frac{p_i}{\\bar{X}}$  \n",
    "as desired \n",
    "\n",
    "(b) show $E\\big[XR\\big] =E\\big[X\\big]E\\big[R\\big \\vert X_I = 1\\big] $  \n",
    "part a gives us the identity that  \n",
    "$\\bar{X}\\cdot P\\big(I =i \\big \\vert X_I = 1\\big) = p_i$  \n",
    "*remark:*  using the results from page 123, which really is basic conditioning, the above identity allows us to homogenize what we are conditioning on and squeeze out a new result  \n",
    "\n",
    "so from page 123, via basic application of total expectation and linearity of expectations, we have   \n",
    "$E\\big[XR\\big] = \\sum_{i=1}^n p_i \\cdot E\\big[R\\big \\vert X_i = 1\\big] $  \n",
    "$= \\sum_{i=1}^n \\Big(\\bar{X}\\cdot P\\big(I =i \\big \\vert X_I = 1\\big) \\Big)\\cdot E\\big[R\\big \\vert X_i = 1\\big]$   \n",
    "$= \\bar{X} \\cdot \\sum_{i=1}^n   P\\big(I =i \\big \\vert X_I = 1\\big) \\cdot E\\big[R\\big \\vert X_i = 1\\big]$   \n",
    "$= \\bar{X} \\cdot E\\big[R\\big \\vert X_I = 1\\big]$   \n",
    "\n",
    "where we note that via Law of Total Expectation we have  \n",
    "$E\\big[R\\big \\vert X_I = 1\\big] = \\sum_{i=1}^n   P\\big(I =i \\big \\vert X_I = 1\\big) \\cdot E\\big[R\\big \\vert X_i = 1\\big]$  \n",
    "\n",
    "\n",
    "(c)  show $P\\big(X\\gt 0\\big) = E\\big[X\\big] E\\big[\\frac{1}{X}\\big \\vert X_I = 1\\big]$  \n",
    "\n",
    "using results from page 124, we have   \n",
    "$R := \\frac{I_{X\\gt 0}}{X}$  \n",
    "\n",
    "this gives \n",
    "\n",
    "i.)  $P\\big(X\\gt 0\\big) = E\\big[XR\\big] $   \n",
    "ii.) $E\\big[R\\big \\vert X_i = 1]= E\\big[\\frac{1}{X} \\big \\vert X_i = 1\\big]$  \n",
    "(because conditioning on *any* $X_i = 1$ or as we'll see $X_I=1$  implies $I_{X\\gt 0}=1$ \n",
    "\n",
    "but using results from (b) we now have  \n",
    "\n",
    "$P\\big(X\\gt 0\\big) = E\\big[XR\\big]  = \\bar{X} \\cdot E\\big[R\\big \\vert X_I = 1\\big] = \\bar{X} \\cdot E\\big[\\frac{1}{X}\\big \\vert X_I = 1\\big]   $  \n",
    "\n",
    "\n",
    "as desired  \n",
    "\n",
    "**commentary:**  There are probably some deeper ideas lurking underneath this problem that will come out after further contemplation.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10)  \n",
    "\n",
    "This is a rather tricky problem designed to use ex 8 and 9 to show that for non-negative random variable $X$ (that isn't identically zero) comprised of a sum of indicator random variables, $X_i$'s, the conditional expectations inequality is sharper than the second moment method (reference pages 123, 124).  \n",
    "\n",
    "consider:  \n",
    "the convex function $f$  \n",
    "$f: u \\to \\frac{1}{u}$  for $u \\in (0, \\infty)$, \n",
    "\n",
    "\n",
    "$\\frac{P\\big(X\\gt 0\\big)}{\\bar{X}} $  \n",
    "$= \\frac{1}{\\bar{X}}\\sum_{i=1}^n p_i\\cdot E\\big[\\frac{1}{X}\\big \\vert X_i = 1\\big]$   \n",
    "$= \\frac{1}{\\bar{X}}\\sum_{i=1}^n p_i\\cdot E\\big[f(X)\\big \\vert X_i = 1\\big]$  \n",
    "$\\geq \\frac{1}{\\bar{X}}\\sum_{i=1}^n p_i\\cdot f\\Big( E\\big[X\\big \\vert X_i = 1\\big]\\Big)$  \n",
    "$=  \\sum_{i=1}^n \\frac{p_i}{\\bar{X}}\\cdot f\\Big(E\\big[X\\big \\vert X_i = 1\\big]\\Big)$  \n",
    "$=  \\sum_{i=1}^n P\\big(I =i \\big \\vert X_I = 1\\big) \\cdot f\\Big(E\\big[X\\big \\vert X_i = 1\\big]\\Big)$  \n",
    "$=  E\\Big[ f\\Big(E\\big[X\\big \\vert X_i = 1\\big]\\Big)\\Big]$  \n",
    "$\\geq  f\\Big(E\\Big[  E\\big[X\\big \\vert X_i = 1\\big]\\Big]\\Big)$  \n",
    "$=  f\\Big( E\\big[X\\big \\vert X_I=1\\big]\\Big)$  \n",
    "$=\\frac{1}{ E\\big[X\\big \\vert X_I=1\\big]}$  \n",
    "$=\\frac{\\bar{X}}{ \\bar{X} \\cdot E\\big[X\\big \\vert X_I=1\\big]}$  \n",
    "$=\\frac{\\bar{X}}{ E\\big[X^2\\big] } $   \n",
    "\n",
    "where the final line used 9(b)  \n",
    "$E\\big[XR\\big] = \\bar{X} \\cdot E\\big[R\\big \\vert X_I = 1\\big]$, setting $R := X$ gives   \n",
    "$E\\big[X^2\\big] = \\bar{X} \\cdot E\\big[X\\big \\vert X_I = 1\\big]$  \n",
    "\n",
    "\n",
    "thus we have \n",
    "\n",
    "$\\frac{P\\big(X\\gt 0\\big)}{\\bar{X}} \\geq \\frac{1}{\\bar{X}}\\sum_{i=1}^n p_i\\cdot f\\Big( E\\big[X\\big \\vert X_i = 1\\big]\\Big)   = \\frac{1}{\\bar{X}}\\sum_{i=1}^n p_i\\cdot \\frac{1}{E\\big[X\\big \\vert X_i = 1\\big]} \\geq \\frac{\\bar{X}}{ E\\big[X^2\\big] } $     \n",
    "\n",
    "rescaling by $\\bar{X}$  gives  \n",
    "\n",
    "$P\\big(X\\gt 0\\big)\\geq \\sum_{i=1}^n p_i\\cdot \\frac{1}{E\\big[X\\big \\vert X_i = 1\\big]} \\geq \\frac{\\bar{X^2}}{ E\\big[X^2\\big] } $     \n",
    "\n",
    "as desired  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11)   \n",
    "Let $X_i$ be exponential with means $8 + 2i$ for $i = 1, 2, 3$.  Obtain an upper bound on  \\\n",
    "$E\\big[\\max X_i\\big]$   \n",
    "(pages 126 - 128) \n",
    "\n",
    "and compute the exact result whne the $X_i$ are independent  \n",
    "\n",
    "solution:  \n",
    "we have   \n",
    "$1=\\sum_{i=1}^3 P\\big(X_i \\gt c^*\\big) = \\exp\\big(-\\frac{c^*}{10}\\big) + \\exp\\big(-\\frac{c^*}{12}\\big) + \\exp\\big(-\\frac{c^*}{14}\\big)$  \n",
    "\n",
    "https://www.wolframalpha.com/input/?i=exp(-c%2F10)+%2B+exp(-c%2F12)+%2B+exp(-c%2F14)+%3D+1\n",
    "\n",
    "yields \n",
    "$c^* \\approx 13.0733$, (recalling that the results hold for any $c\\geq 0$ but $c^*$ gives optimal tightness.  Hence using results from page 127, we have  \n",
    "\n",
    "$E\\big[\\max X_i\\big] \\leq c^* + \\sum_{i=1}^3 \\int_{c^*}^\\infty Pr\\{X_i\\gt y\\}dy =c^* + \\sum_{i=1}^3 \\int_{c^*}^\\infty \\exp\\big(-\\lambda_i y\\big)dy = c^* + \\sum_{i=1}^3 \\lambda_i^{-1}\\cdot \\exp\\big(-\\lambda_i c^*\\big)$   \n",
    "$E\\big[\\max X_i\\big] \\leq 13.0733 + \\Big(10 \\cdot \\exp\\big(-\\frac{13.0733}{10}\\big) + 12 \\cdot \\exp\\big(-\\frac{13.0733}{12}\\big) + 14 \\cdot \\exp\\big(-\\frac{13.0733}{14}\\big) \\Big) \\approx 25.3183$  \n",
    "\n",
    "\n",
    "*exact computations for independent case:*    \n",
    "$\\lambda_1^{-1} = 10$  \n",
    "$\\lambda_2^{-1} = 12$  \n",
    "$\\lambda_3^{-1} = 14$   \n",
    "\n",
    "hence   \n",
    "$\\lambda_1 = \\frac{1}{10}$  \n",
    "$\\lambda_2 = \\frac{1}{12}$  \n",
    "$\\lambda_3 = \\frac{1}{14}$   \n",
    " \n",
    "*approach one:*   \n",
    "compute the CDF of  \n",
    "$\\max X_i = Pr\\{X_1 \\leq x\\}Pr\\{X_2 \\leq x\\}Pr\\{X_3 \\leq x\\} = \\big(1- \\exp(-\\lambda_1x\\big) \\big(1- \\exp(-\\lambda_2x\\big) \\big(1- \\exp(-\\lambda_3x\\big)$  \n",
    "\n",
    "*rationale:* the last 'arrival' has occurred at time $x$ if and only if all arrivals have occurred at that time, which each respective arrival probability is given by the respective CDF, and by stochastic independence we may multiply them.  \n",
    "\n",
    "then integrate the complementary cdf  \n",
    "\n",
    "$E\\big[\\max X_i\\big] =\\int_0^\\infty \\Big(1- \\big(1- \\exp(-\\lambda_1x\\big) \\big(1- \\exp(-\\lambda_2x\\big) \\big(1- \\exp(-\\lambda_3x\\big)\\Big)dx  \\approx 22.176$  \n",
    "\n",
    "https://www.wolframalpha.com/input/?i=integral+from+0+to+infinity+(1-(1-exp(-+x%2F10))(1-exp(-+x%2F12))(1-exp(-+x%2F14)+)++)dx\n",
    "\n",
    "*approach two:*  \n",
    "use conditional expectations \n",
    "\n",
    "define event $A_i = X_i \\text{is last arrival }$  \n",
    "$\\max X_i = \\mathbb I_{A_1}X_1 +  \\mathbb I_{A_2}X_2 + \\mathbb I_{A_3}X_3  = \\sum_{k=1}^3 \\mathbb I_{A_k}X_k$   \n",
    "$E\\big[\\max X_i\\big]= E\\big[\\sum_{k=1}^3 \\mathbb I_{A_k}X_k\\big] = \\sum_{k=1}^3 E\\big[\\mathbb I_{A_k}X_k\\big]$   \n",
    "\n",
    "where, using the tower property of conditional expectations we see \n",
    "\n",
    "$E\\Big[\\mathbb I_{A_k}X_k\\Big] = E\\Big[E\\big[\\mathbb I_{A_k}X_k\\big \\vert X_k \\big]\\Big] = E\\Big[X_k \\cdot E\\big[\\mathbb I_{A_k}\\big \\vert X_k \\big]\\Big]= E\\Big[X_k \\cdot E\\big[\\mathbb I_{A_k}\\big \\vert X_k = x \\big]\\Big]$  \n",
    "\n",
    "taking a closer look at \n",
    "$E\\big[\\mathbb I_{A_k}\\big \\vert X_k = x \\big] $    \n",
    "in word this reads the probability that event $A_k$ occurs, given $X_k$'s arrival time is x, which is precisely given by the product of the CDFs of the other two arrivals.  E.g. if $k=1$ we have  \n",
    "$E\\big[\\mathbb I_{A_1}\\big \\vert X_1 = x \\big] = Pr\\{X_2 \\leq x\\}Pr\\{X_3 \\leq x\\} = \\big(1- \\exp(-\\lambda_2x\\big) \\big(1- \\exp(-\\lambda_3x\\big)\\Big)$    \n",
    "\n",
    "\n",
    "hence  \n",
    "\n",
    "$E\\Big[\\mathbb I_{A_1}X_1\\Big] = E\\Big[X_1 \\cdot E\\big[\\mathbb I_{A_k}\\big \\vert X_k \\big]\\Big]= E\\Big[X_k \\cdot E\\big[\\mathbb I_{A_k}\\big \\vert X_k = x \\big]\\Big]= E\\Big[X_k \\cdot\\big(1- \\exp(-\\lambda_2x\\big) \\big(1- \\exp(-\\lambda_3x\\big)\\Big)\\Big]$  \n",
    "$=\\int_0^\\infty x\\cdot\\lambda_1 \\exp\\big(-\\lambda_1 x\\big)\\big(1- \\exp(-\\lambda_2x\\big) \\big(1- \\exp(-\\lambda_3x\\big)  dx$    \n",
    "\n",
    "\n",
    "Thus returning to our expected value, we have  \n",
    "$E\\big[\\max X_i\\big]= \\sum_{k=1}^3 E\\big[\\mathbb I_{A_k}X_k\\big] = \\sum_{k=1}^3 \\int_0^\\infty x\\lambda_k \\exp\\big(-\\lambda_k x\\big)\\Big(\\prod_{j\\neq k}\\big(1- \\exp(-\\lambda_j x\\big) \\Big) dx$  \n",
    "$ =  \\int_0^\\infty \\sum_{k=1}^3 x\\lambda_k \\exp\\big(-\\lambda_k x\\big)\\Big(\\prod_{j\\neq k}\\big(1- \\exp(-\\lambda_j x\\big) \\Big) dx \\approx 22.176$   \n",
    "\n",
    "if we add up the three integrals below  \n",
    "https://www.wolframalpha.com/input/?i=integral+from+0+to+infinity+x%2F10+exp(-x%2F10+)(1-exp(-x%2F12))(1-exp(-x%2F14))+dx  \n",
    "https://www.wolframalpha.com/input/?i=integral+from+0+to+infinity+x%2F12+exp(-x%2F12+)(1-exp(-x%2F10))(1-exp(-x%2F14))+dx\n",
    "https://www.wolframalpha.com/input/?i=integral+from+0+to+infinity+x%2F14+exp(-x%2F14+)(1-exp(-x%2F12))(1-exp(-x%2F10))+dx\n",
    "\n",
    "\n",
    "*remark:*  \n",
    "working directly with the CDF is much more expedient, however carefully working through conditional expectations and the associated events gives some useful insights and another way to check our work   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12)   \n",
    "similar to exercise 11, except we now have $n$ uiform random varaibles $X_i \\in (0,1)$.  \n",
    "$E\\big[\\max X_i\\big]$   \n",
    "\n",
    "solution:  \n",
    "we have   \n",
    "$1=\\sum_{i=1}^n P\\big(X_i \\gt c^*\\big) = n\\cdot (1-c^*)$  \n",
    "$ (1-c^*) = \\frac{1}{n}$  \n",
    "$c^* = \\frac{n-1}{n}$  \n",
    "\n",
    "\n",
    "$E\\big[\\max X_i\\big] \\leq c^* + \\sum_{i=1}^n \\int_{c^*}^\\infty Pr\\{X_i \\gt y\\}dy = c^* + \\sum_{i=1}^n \\int_{c^*}^1 (1-y)\\cdot dy = c^* + \\sum_{i=1}^n \\frac{1}{2}\\big(1-c^*\\big)^2= \\frac{n-1}{n} + \\frac{n}{2}\\big(1-\\frac{n-1}{n}\\big)^2$    \n",
    "\n",
    "$E\\big[\\max X_i\\big] \\leq \\frac{n-1}{n} + \\frac{n}{2}\\big(\\frac{n}{n}-\\frac{n-1}{n}\\big)^2 = \\frac{n-1}{n} + \\frac{n}{2}\\big(\\frac{1}{n}\\big)^2 = \\frac{n-1}{n} + \\frac{n}{2}\\cdot\\frac{1}{n^2}= \\frac{2n-2}{2n} + \\frac{1}{2n} = \\frac{2n-1}{2n} = 1 - \\frac{1}{2n}$    \n",
    "\n",
    "\n",
    "*exact answer in the independent case*  \n",
    "given the homogenization we can easily apply the conditional expectations approach and see  \n",
    "$E\\big[\\max X_i\\big]= E\\big[\\sum_{k=1}^n \\mathbb I_{A_k}X_k\\big]= \\sum_{k=1}^n E\\big[\\mathbb I_{A_k}X_k\\big] = n \\cdot E\\big[\\mathbb I_{A_k}X_k\\big]= n\\cdot E\\Big[X_k \\cdot E\\big[\\mathbb I_{A_k}\\big \\vert X_k = x \\big]\\Big]$  \n",
    "\n",
    "$E\\big[\\max X_i\\big] = n\\cdot E\\Big[X_k \\cdot x^{n-1}\\Big]= n\\cdot \\int_0^1 (1x)\\cdot x^{n-1}dx= n\\cdot \\int_0^1  x^{n}dx = \\frac{n}{n+1} = 1 -\\frac{1}{n+1}$  \n",
    "\n",
    "all in all the upper bound seems to be rather close to the exact amount *and* of course covers non-independent cases as well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13)   \n",
    "an extension of ex 12:  \n",
    "\n",
    "consider two $X_i ~ U(0,1)$ upper bound the maximum and show that it is obtained when $U_2 := 1 - U_1$.  \n",
    "\n",
    "(Note that a more interesting and much stronger statement would be that such a thing is obtained *iff* we have $X_2$ almost surely have that distribution -- however such a thing is not the question at hand.)  \n",
    "\n",
    "solution:  \n",
    "using results from the prior problem we have \n",
    "$E\\big[\\max X_i\\big] \\leq  1 - \\frac{1}{2n}$  with $n=2$ giving us \n",
    "$E\\big[\\max X_i\\big] \\leq  1 - \\frac{1}{4}= \\frac{3}{4}$   \n",
    "\n",
    "\n",
    "we also have, using conditional expectations for this specialized dependent case:   \n",
    "\n",
    "$E\\big[\\max X_i\\big]= n\\cdot E\\Big[X_k \\cdot E\\big[\\mathbb I_{A_k}\\big \\vert X_k = x \\big]\\Big] = 2\\cdot E\\Big[X_k \\cdot E\\big[\\mathbb I_{A_k}\\big \\vert X_k = x \\big]\\Big]$  \n",
    "\n",
    "and with some care we can see  \n",
    "$E\\big[\\mathbb I_{A_k}\\big \\vert X_k = x \\big] = 1 \\text{  if x } \\geq \\frac{1}{2} \\text{ and otherwise} =0 $\n",
    "\n",
    "which gives us \n",
    "\n",
    "$2 \\cdot E\\Big[X_k \\cdot E\\big[\\mathbb I_{A_k}\\big \\vert X_k = x \\big]\\Big] = 2\\cdot\\Big(\\big(\\int_0^\\frac{1}{2}(1x)\\cdot 0 dx\\big)+\\big(\\int_\\frac{1}{2}^1(1x)\\cdot 1dx\\big)  \\Big)= 2 \\cdot\\Big(0 + \\frac{3}{8}\\Big) = \\frac{3}{4}$    \n",
    "\n",
    "*remark:*   \n",
    "This problem likely could have been approached via the CDF method, but it would be somewhat clunky in order to deal with the dependencies.  One of the chief benefits of the conditional expectations approach is that it is quite flexible to accomodate numerous different events and conditioning setups.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 15)   \n",
    "is an open problem of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
