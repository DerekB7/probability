{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your author only focused on a portion of the problems in this chapter due to significant overlap with problems and concepts in other texts covering renewal theory (e.g. Feller volume 1, Gallagher and associated MIT OCW Discrete Stochastic Processes -- approximately 1/4 of that course is focused on renewal processes--, Ross's Stochastic Processes)  \n",
    "\n",
    "While there are some very good items in the chapter,  on the whole the chapter felt rather rushed and a bit lopsided.  Selected problems as well as well as expansions on items from the main chapter, have been included below.  \n",
    "\n",
    "Because simulations seem to tie in especially well with queueing problems, your author has dropped in code as a 'companion' to several of the problems in this chapter.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.) Derive the Renewal Equation   \n",
    "\n",
    "with a renewal proces with $S_n = X_1 + X_2 + ... + X_n$  \n",
    "and $t \\geq 0$  \n",
    "\n",
    "using first step analysis / conditional expectations, we condition on $X_1$  \n",
    "\n",
    "$m(t) = E\\Big[N(t)\\Big] = E\\Big[E\\big[N(t)\\big \\vert X_1\\big]\\Big]$   \n",
    "\n",
    "where \n",
    "$E\\big[N(t)\\big \\vert X_1\\big]$ is a random variable where for $X_1\\big(\\omega\\big)= x_1$  \n",
    "$E\\big[N(t)\\big \\vert X_1\\big] = 1 + m(t-x_1) \\text{  if  } x_1 \\in [0,t], \\text{which has probability measure of }  dF(x_1)$   \n",
    "$E\\big[N(t)\\big \\vert X_1\\big] = 0 \\text{ otherwise, i.e. for } x_1 \\gt t, \\text{which occurs with probability 1-F(t)}$   \n",
    "Thus \n",
    "$m(t) = E\\Big[N(t)\\Big] = E\\Big[E\\big[N(t)\\big \\vert X_1\\big]\\Big] = \\int_0^t \\big(1 + m(t-x_1)\\big)dF(x_1) = F(t) + \\int_0^t  m(t-x_1)dF(x_1)$   \n",
    "\n",
    "but since the $X_i$ are iid, we can more succinctly write this as \n",
    "\n",
    "$m(t) = F(t) + \\int_0^t  m(t-x)dF(x)$   \n",
    "\n",
    "which is the renewal equation  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.) Inspection Paradox \n",
    "for a renewal process with iid interarrival times of $X_i$, we \n",
    "$P\\big(X_{N(t)+1} \\gt x\\big) \\geq 1 - F(x)$  \n",
    "\n",
    "which reads the the complementary cdf of the first arrival after time t is stochastically larger than the complementary cdf of each iid arrival.  In effect, this is adverse sampling.  Technical nit -- this applies for 'regular' renewal processes, not necessarily at the beginning of a delay renewal process.  \n",
    "\n",
    "**proof**  \n",
    "\n",
    "If we orient ourself at time $a$ the time of the prior arrival (at which point the process renewed), we see for any arrival time in \n",
    "\n",
    "$s \\in [a,\\infty)$  \n",
    "\n",
    "$P\\big(X_{N(t)+1} \\gt x\\big) = 1 \\cdot P\\big(X \\gt s\\big \\vert X \\gt a \\big) \\geq P\\big(X \\gt a\\big) \\cdot P\\big(X \\gt s\\big \\vert X \\gt a \\big) =1 - F(x)$   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.)  a queueing prbolem  \n",
    "\n",
    "A room has $n$ machines, each iid exponential with parameter $m$, and a repairman is called as soon as $k\\lt n$ machines break and it takes the repairmean $d$ days to arrive -- and he instantly fixes the broken machines on site and the process probabilistically starts over.  \n",
    "\n",
    "So there are two stages (i.) all machines running and this continues until $k$ break then (ii.) repairman stage  \n",
    "Thus the renewal process is $(i.) \\to (ii.) \\to (i.)$.  However, we could just as easily call the renewal process $(ii.) \\to (i.) \\to (ii.)$ where the very first stage starts in $(i.)$ and hence is a delayed renewal process.  Technically this latter interpretation fits slightly more naturally for question (a) as the repairman is called at the end of each renewal there-- but this is a minor bookkeeping point.  \n",
    "\n",
    "(a) *question:* How often in the long run does the repairman get called?  \n",
    "*answer:* This is a little messy linguistically, but your author's read on this is that it is asking for the expected duration of stage $(i)$ plus expected time of stage $(ii.)$.  The latter has length $d$.  The former, by a Poisson embedding argument, has expected time until absorbtion of \n",
    "\n",
    "$=\\text{expected time until first arrival} +\\text{expected time }\\{1 \\to 2\\} +... + \\text{expected time }\\{k-1 \\to k\\} $  \n",
    "$= \\frac{1}{(n-0)m} +\\frac{1}{(n-1)m} + ... + \\frac{1}{(n-(k-1))m} = \\frac{1}{m}\\sum_{i=0}^{k-1}\\frac{1}{m-i}$  \n",
    "\n",
    "so the total expected length of a renewal cycle is  \n",
    "$E\\big[X\\big] = d + \\frac{1}{m}\\sum_{i=0}^{k-1}\\frac{1}{m-i}$  \n",
    "\n",
    "hence the long-run / time averaged frequency of the repairment being called is given by \n",
    "$\\frac{E[R]}{E[X]}=\\frac{1}{d + \\frac{1}{m}\\sum_{i=0}^{k-1}\\frac{1}{m-i}} =\\frac{m}{md + \\sum_{i=0}^{k-1}\\frac{1}{m-i}}$  \n",
    "\n",
    "where the repairman is called (with reward value 1) exactly once per cycle.  \n",
    "\n",
    "\n",
    "(b) *question:*  What is the distirbution of the total number of broken machines the repairman finds when he arrives?  \n",
    "*answer:* There are several ways to tackle this problem, some quite unpleasant.  Any easy and probablistically satisfying one is as follows: \n",
    "at the beginnong of (ii) we have WP1 $k$ machines broken and at the end we have at most $n$ machines broken.  Making use of memorylessness of exponential arrivals, we get a fresh start on all surviving machines once we've entered this (ii.).  \n",
    "\n",
    "It is convenient to do an affine shift by $k$ such that we consider whether  $0$ or $ 1$ or $2$, or $...$ or  $r$ machines break during the period (where $r=n-k$).  It is a simple translation by $k$ from this to total machines found by the repairman.  \n",
    "\n",
    "Now whether working directly with the independent exponential distributions, or via a Poisson splitting into $r$ streams argument, we find that at time $d$ when the repairman arrives, there is probability of $p = 1-\\exp(-md)$ that a given machine is has broken and probability of $1-p$ that is has broken.  Hence we have a binomial distribution of \n",
    "\n",
    "$\\binom{r}{j}p^i(1-p)^{r-j}$\n",
    "that $j$ machines break during period $(ii)$, or equivalently, the probability is \n",
    "$\\binom{r}{j}p^i(1-p)^{r-j}$ that the repairman finds $k + j$ machines broken for $j \\in\\{0, 1, 2, ..., n-k\\}$  \n",
    "\n",
    "\n",
    "(c) *question:* what fraction of time in the long-run are there more than $k$ broken machines in the room?  \n",
    "*answer:*  This question in effect re-uses items from $(a)$.  It is convenient to focus on the complementary component of less than or equal to $k$ broken machines in the room, calling that expected value $\\bar{Y}$ and noticing that the value we seek is given by $E\\big[R\\big] = \\bar{X} - \\bar{Y}$ (i.e. reward of one at each instant that more than $k$ machines are broken), and the ultimate answer of course is given by \n",
    "\n",
    "$\\frac{E[R]}{E[X]} = \\frac{\\bar{X}-\\bar{Y}}{\\bar{X}} = 1 - \\frac{\\bar{Y}}{\\bar{X}}$  \n",
    "\n",
    "to finish this off:  \n",
    "$\\bar{Y} = \\text{expected value of all of stage (i) } + \\text{expected time until minimum of first arrival and d, while in stage (ii)}$    \n",
    "$= \\frac{1}{m}\\sum_{i=0}^{k-1}\\frac{1}{m-i} + \\text{expected time until minimum of first arrival and d, while in stage (ii)}$  \n",
    "\n",
    "a nice way to calculate $\\text{expected time until minimum of first arrival and d, while in stage (ii)}$ involves Poisson embedding with parameter $mk$ and renewal rewards. \n",
    "\n",
    "$\\text{expected time until minimum of first arrival and d, while in stage (ii)} $  \n",
    "$= \\int_0^d Pr\\{\\text{1st arrival} \\gt x\\} dx = \\int_0^d \\exp(- m\\lambda t )dt = \\frac{1 - \\exp(-dmk)}{m}$  \n",
    "\n",
    "- - - - \n",
    "note that the complementary value here is  \n",
    "$\\int_0^d Pr\\{\\text{1st arrival} \\leq x\\} dx = d-\\big(\\frac{1 - \\exp(-dmk)}{m}\\big)  $  \n",
    "\n",
    "of course for total expected time of $d$ in stage (ii) \n",
    "- - - - \n",
    "Puttin all this together gives   \n",
    "$\\bar{Y} = \\big(\\frac{1}{m}\\sum_{i=0}^{k-1}\\frac{1}{m-i} \\big) + \\frac{1 - \\exp(-dmk)}{m}$  \n",
    "\n",
    "$\\frac{E[R]}{E[X]} = 1 - \\frac{\\bar{\\big(\\frac{1}{m}\\sum_{i=0}^{k-1}\\frac{1}{m-i} \\big) + \\frac{1 - \\exp(-dmk)}{m}}}{ d + \\frac{1}{m}\\sum_{i=0}^{k-1}\\frac{1}{m-i}} = 1 - \\frac{ 1 - \\exp(-dmk)+ \\sum_{i=0}^{k-1}\\frac{1}{m-i}}{ md + \\sum_{i=0}^{k-1}\\frac{1}{m-i}}$  \n",
    "$= \\frac{md + \\sum_{i=0}^{k-1}\\frac{1}{m-i}- 1 + \\exp(-dmk)- \\sum_{i=0}^{k-1}\\frac{1}{m-i}}{ md + \\sum_{i=0}^{k-1}\\frac{1}{m-i}}$  \n",
    "$= \\frac{md  - (1 - \\exp(-dmk))}{ md + \\sum_{i=0}^{k-1}\\frac{1}{m-i}}$  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.)  \n",
    "Each produced item is either defective or acceptable.  (a) Initially each item is inspected and this continues until $k$ consecutive acceptable items are discovered.  At this point the inspection mode changes to (b) inspect items at independently random with probability $\\alpha$, which goes on until a defective item is found and then the process converts back to the process of $(a)$.  Suppose each item is iid defective with probability $q$ -- what portion of items are inspected?  \n",
    "\n",
    "**remark:  we'll set this up as a renewal rewards process, though it is worth consider that this problem consists of nesting various bernouli processes**  \n",
    "\n",
    "The renewal process is cold start at $a \\to b \\to a$ and a renewal occurs immediately upon returning to $a$.  \n",
    "\n",
    "$\\text{portion of items inspected} = \\frac{E[R_1]}{E[X_1]}$  \n",
    "\n",
    "where $E[X_1]$ is expected number of iterations / items in the renewal process, and we get a reward of 1 for each item inspected. \n",
    "\n",
    "$E[X_1] = \\big(\\text{expected iterations from a to b}\\big) + \\big(\\text{expected iterations from b to a}\\big)$   \n",
    "\n",
    "$\\text{expected iterations from a to b}= \\mu = \\frac{1 - (1-q)^k}{q(1-q)^k}$  \n",
    "*rational:*  expected time until a run of $k$ acceptables, with success probability of $(1-q)$.  This calculation is covered in this chapter using an interesting form of renewal reward *and* covered in the Martingales Chapter (with a martingale technique for recovering the variance).  However your author's preferred technique for recovering the expected renewal time is still the on that Feller mentions in chp 13 of volume 1 (i.e. partition events and pass limits using key renewal theorem).  \n",
    "\n",
    "$\\text{expected iterations from b to a} = (q\\alpha)^{-1}$  \n",
    "*rational:*   geometric distribution, indexing at one, that has a probability of stopping / success of $(q\\alpha)$  \n",
    "- - - -\n",
    "$E[R_1] = \\big(\\text{expected items inspected in a }\\big) + \\big(\\text{expected items inspected while in b}\\big)$   \n",
    "$\\text{expected items inspected in a } = \\text{expected iterations from a to b}= \\mu$  \n",
    "*rationale:* all items are inspected in (a)    \n",
    "\n",
    "$\\text{expected items inspected while in b} =  (q)^{-1}$   \n",
    "*rationale:*  if we only count the items inspected, and we count items up to and including the first defective one, this makes the distribution geometric with 'success' parameter $q$ (indexing at 1).  \n",
    "\n",
    "\n",
    "$\\text{portion of items inspected} = \\frac{E[R_1]}{E[X_1]}=\\frac{\\mu +(q)^{-1}}{\\mu + (\\alpha q)^{-1}}=\\frac{q\\mu +1}{q\\mu + \\alpha^{-1}} = \\frac{\\alpha(q\\mu +1)}{\\alpha q\\mu + 1} =  \\frac{\\alpha q\\mu +\\alpha}{\\alpha q\\mu + 1} $   \n",
    "\n",
    "\n",
    "**note: I think there is also a clever way to do this with conditional expectations that I should type up**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#interlude: some code for problem 6 \n",
    "\n",
    "@numba.jit(nopython= True)\n",
    "def my_sim(prob_inspect, prob_defective, k, n_trials):\n",
    "    # for problem 6\n",
    "    alpha = prob_inspect\n",
    "    q = prob_defective\n",
    "    counter_of_inspected = 0\n",
    "    total_iterations = 0 # total iterations \n",
    "    for _ in range(n_trials):\n",
    "        # state 0 \n",
    "        success_run_counter = 0\n",
    "        while True: \n",
    "            my_first_number = np.random.rand()\n",
    "            total_iterations += 1\n",
    "            counter_of_inspected += 1 # all items are inspected while in state 0\n",
    "            if my_first_number <= q: \n",
    "                # i.e. defective \n",
    "                success_run_counter = 0\n",
    "            else:\n",
    "                success_run_counter += 1\n",
    "            if success_run_counter == k:\n",
    "                break\n",
    "        # state 1\n",
    "        while True: \n",
    "            my_first_number = np.random.rand()\n",
    "            my_second_number = np.random.rand()\n",
    "            total_iterations += 1\n",
    "            if my_first_number <= alpha:\n",
    "                counter_of_inspected += 1\n",
    "                if my_second_number <= q:\n",
    "                    break\n",
    "    return counter_of_inspected / total_iterations \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def the_function(q, alpha, k):\n",
    "    mu = (1 -(1-q)**k)/(q*(1-q)**k)\n",
    "    print(\"mu is \", mu)\n",
    "    numerator = alpha * q * mu + alpha \n",
    "    denominator = alpha * q * mu + 1\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1880452052237367\n",
      "mu is  32.25290298461912\n",
      "0.18727620942436682\n"
     ]
    }
   ],
   "source": [
    "q = 0.2\n",
    "alpha = 0.03\n",
    "k = 9\n",
    "sim_results = my_sim(alpha, q, k, n_trials=100000)\n",
    "# print(sim_results[0]/sim_results[1])\n",
    "print(sim_results)\n",
    "print(the_function(q, alpha, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7\n",
    "This is a basic poisson embedding problem   \n",
    "$p_1 = \\text{probability 1 survives } = 1- \\frac{\\lambda_1 }{\\lambda_1 + \\lambda_2} = \\frac{\\lambda_2 }{\\lambda_1 + \\lambda_2}$    \n",
    "$p_2 = \\text{probability 2 survives } = \\frac{\\lambda_1 }{\\lambda_1 + \\lambda_2}$    \n",
    "may need tightened up  \n",
    "\n",
    "$\\text{expected total cost per cycle}  = c\\big(\\frac{1}{\\lambda_1 + \\lambda_2}\\big) + K + c_1 p_1 \\frac{1}{\\lambda_1} +c_2 p_2 \\frac{1}{\\lambda_2} $  \n",
    "\n",
    "$\\text{expected total time per cycle}  = \\big(\\frac{1}{\\lambda_1 + \\lambda_2}\\big) + p_1 \\frac{1}{\\lambda_1} +p_2 \\frac{1}{\\lambda_2} $  \n",
    "\n",
    "per renewal rewards theorem, we thus have  \n",
    "$\\text{expected long-run cost per unit time} = \\frac{\\text{expected total cost per cycle} }{\\text{expected total time per cycle} } = \\frac{c\\big(\\frac{1}{\\lambda_1 + \\lambda_2}\\big) + K + c_1 p_1 \\frac{1}{\\lambda_1} +c_2 p_2 \\frac{1}{\\lambda_2}}{\\big(\\frac{1}{\\lambda_1 + \\lambda_2}\\big) + p_1 \\frac{1}{\\lambda_1} +p_2 \\frac{1}{\\lambda_2} }$   \n",
    "\n",
    "**tbc**.  and it would be good to tie this in with a theorem or proposition related to renewal rewards in this chapter.  A bi-paritition argument would probably do it with the two different failure types... \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 10 \n",
    "this problem is of some interest and still an open item  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 \n",
    "\n",
    "someone rolls a die repeatedly and adds up the numbers:  \n",
    "whats more likely: probability that the sum ever hits 2 or probability that the sum ever hits 102 \n",
    "- - - - -\n",
    "\n",
    "See the two main matrices in 'Feller_chp15_notes.ipynb'. \n",
    "\n",
    "The matrix chosen in the chapter in the proof of Feller-Erdos-Pollard (or what the chapter calls lattice case of Blackwell)  is the first one in that notebook -- the renewal matrix corresponding to Age.  Your author's preference is the second matrix, the renewal matrix corresponding to Residual Life.  Either one works here.  Via 'grabbing' the top left component after two iterations, or direct calculation, we see \n",
    "\n",
    "$P\\big(\\text{sum ever hits 2}\\big) = \\frac{1}{6} + \\frac{1}{36} = \\frac{7}{36}$  \n",
    "\n",
    "however this renewal chain has $\\mu = \\frac{1 + 2 + 3 + 4 + 5 + 6}{6}=  3.5 = \\frac{7}{2}$, which gives an assymptotic estimate of \n",
    "\n",
    "$P\\big(\\text{sum ever hits 102}\\big) \\approx \\frac{1}{\\mu}  = \\frac{2}{7}=\\frac{10}{35}\\gt \\frac{10}{36}\\gt \\frac{7}{36}$\n",
    "\n",
    "the exact calculations are in code, below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.194444444444\n",
      "0.285714285714\n",
      "vs \n",
      "\n",
      "0.194444444444\n",
      "0.285714285714\n",
      "\n",
      "vs lr estimate of \n",
      " 0.2857142857142857\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A = np.zeros((6,6))\n",
    "\n",
    "for k in range(1,A.shape[0]):\n",
    "    A[k,k-1]= 1\n",
    "A[0,:] += 1/6\n",
    "\n",
    "e_1 = np.zeros(6)\n",
    "e_1[0] = 1\n",
    "print(e_1 @ np.linalg.matrix_power(A,2) @ e_1)\n",
    "print(e_1 @ np.linalg.matrix_power(A,102) @ e_1)\n",
    "# note this is extremely close to 1/mu = 1/3.5 which is the steady state estimate \n",
    "\n",
    "B = np.zeros((6,6))\n",
    "for k in range(5):\n",
    "    q = 1/(6-k)\n",
    "    B[k,0] = q\n",
    "    B[k,k+1] = 1-q\n",
    "B[-1,0] = 1\n",
    "print(\"vs \\n\")\n",
    "print(e_1 @ np.linalg.matrix_power(B,2) @ e_1)\n",
    "print(e_1 @ np.linalg.matrix_power(B,102) @ e_1)\n",
    "\n",
    "print(\"\\nvs lr estimate of \\n\", 1/3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 \n",
    "\n",
    "**the end result seems to tie in with example 5.6 on pages 145, 146 which has a very natural tie in with renewal rewards, as well as this particular queueing problem I think... there are more insights to be had in this problem I think**  \n",
    "\n",
    "the resulting chain for $b$ is awfully similar to that used in ex 16 of chapter 5... though it is subtly different I suppose \n",
    "\n",
    "$\\lambda := \\lambda_1 + \\lambda_2 + ... + \\lambda_n$   \n",
    "\n",
    "(a) using the renewal rewards theorem 6.8 from p.174\n",
    "\n",
    "we see that   \n",
    "$\\lim_{t \\to \\infty}\\frac{R(t)}{t} \\to_{as} \\frac{E[R_1]}{E[X_1]} = \\lim_{t \\to \\infty}\\frac{E[R(t)]}{t}$  \n",
    "\n",
    "where $R$ is the 'reward' for the server being busy, and $X_1$ is the time of a renewal interval.  For avoidance of doubt, we define the renewal process to be the server is empty, and the time until a first arrival (hence server is busy) and then time until it is empty again -- there is capacity for only one entity to be processed at a time.  By memorylessness of the exponential arrival and processing times, the entire process probabilistically starts over / \"renews\" after this sequence.  We give a reward only for the second of the two stages of this process.  \n",
    "\n",
    "$E\\big[X_1\\big] = \\big(\\frac{1}{\\lambda}\\big) +\\big(\\frac{\\lambda_1}{\\lambda}\\frac{1}{\\mu_1} + \\frac{\\lambda_2}{\\lambda}\\frac{1}{\\mu_2}+... + \\frac{\\lambda_n}{\\lambda}\\frac{1}{\\mu_n}  \\big) = \\big(\\frac{1}{\\lambda}\\big) +\\big(\\frac{1}{\\lambda}\\sum_{i=1}^n\\frac{\\lambda_i}{\\mu_i}\\big) = \\frac{1}{\\lambda}(1+ \\sum_{i=1}^n\\frac{\\lambda_i}{\\mu_i})$  \n",
    "\n",
    "$E\\big[R_1\\big] = \\big(\\frac{1}{\\lambda}\\sum_{i=1}^n\\frac{\\lambda_i}{\\mu_i}\\big)$  \n",
    "\n",
    "hence the time averaged reward is   \n",
    "$\\lim_{t \\to \\infty}\\frac{E[R(t)]}{t} = \\frac{E[R_1]}{E[X_1]}=\\frac{\\frac{1}{\\lambda}\\sum_{i=1}^n\\frac{\\lambda_i}{\\mu_i}}{\\frac{1}{\\lambda}(1+ \\sum_{i=1}^n\\frac{\\lambda_i}{\\mu_i})}=\\frac{\\sum_{i=1}^n\\frac{\\lambda_i}{\\mu_i}}{1+ \\sum_{i=1}^n\\frac{\\lambda_i}{\\mu_i}}$  \n",
    "\n",
    "\n",
    "(b)\n",
    "note the official problem overlaoded $n$ in (b), so we instead call in $X_m$  \n",
    "\n",
    "\n",
    "in row stochastic form  \n",
    "\n",
    "$\\mathbf A = \n",
    "\\left[\\begin{matrix}\n",
    "q & \\frac{\\lambda_1}{\\lambda+\\mu_1} & \\frac{\\lambda_2}{\\lambda+\\mu_2} & \\frac{\\lambda_3}{\\lambda+\\mu_3} & \\frac{\\lambda_4}{\\lambda + \\mu_2} & \\dots & \\frac{\\lambda_n}{\\lambda + \\mu_n} \\\\\n",
    "\\frac{\\mu_1}{\\lambda+\\mu_1} & \\frac{\\lambda}{\\lambda+\\mu_1} & 0 & 0 & 0 & \\dots & 0\\\\\n",
    "\\frac{\\mu_2}{\\lambda+\\mu_2} & 0 & \\frac{\\lambda}{\\lambda+\\mu_2} & 0 & 0 & \\dots & 0\\\\\n",
    "\\frac{\\mu_3}{\\lambda+\\mu_3} & 0 & 0 & \\frac{\\lambda}{\\lambda+\\mu_3} & 0 & \\dots & 0\\\\\n",
    "\\frac{\\mu_4}{\\lambda+\\mu_4} & 0 & 0 & 0 & \\frac{\\lambda}{\\lambda+\\mu_4} & \\dots & 0\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots  & \\vdots\\\\\\frac{\\mu_n}{\\lambda+\\mu_n} & 0 & 0 & 0 & 0 & \\dots & \\frac{\\lambda}{\\lambda+\\mu_n}\\end{matrix}\\right] = \\left[\\begin{matrix}q & p_1\\frac{\\lambda_1}{\\lambda} & p_2\\frac{\\lambda_2}{\\lambda} & p_3\\frac{\\lambda_3}{\\lambda} & p_4\\frac{\\lambda_4}{\\lambda} & \\dots & p_n\\frac{\\lambda_n}{\\lambda} \\\\\n",
    "1-p_1 & p_1 & 0 & 0 & 0 & \\dots & 0\\\\\n",
    "1-p_2 & 0 & p_2 & 0 & 0 & \\dots & 0\\\\\n",
    "1-p_3 & 0 & 0 & p_3 & 0 & \\dots & 0\\\\\n",
    "1-p_4 & 0 & 0 & 0 & p_4 & \\dots & 0\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1-p_n & 0 & 0 & 0 & 0 & \\dots & p_n\\end{matrix}\\right]$  \n",
    "\n",
    "where $q = 1 - \\sum_{i=1}^n \\frac{\\lambda_i}{\\lambda+\\mu_i} = \\sum_{i=1}^n \\frac{\\mu_i}{\\lambda+\\mu_i}$  \n",
    "\n",
    "being row stochastic, of course $\\mathbf{A1} = \\mathbf 1$  \n",
    "\n",
    "we are to verify that this is in fact a reversible markov chain (note: given the self loops we can be sure that this is aperiodic which is nice and by inspection there is one communicating class)  \n",
    "\n",
    "we have the test for reversibility -- i.e. the detailed balance equations --  \n",
    "\n",
    "$\\pi P_{i,j} = \\pi_j P_{j,i}$  \n",
    "\n",
    "where we have \n",
    "\n",
    "$(1-p_1) = P_{1,0}$  \n",
    "and in general for off diagonal components of the transition matrix    \n",
    "$(1-p_i) = P_{i,0}$  \n",
    "$\\frac{\\lambda_i}{\\lambda} p_i = P_{0,i}$ \n",
    "\n",
    "fixing $\\pi_0:=1$ and normalizing later, \n",
    "\n",
    "this implies for $i \\in \\{1, 2... , n\\}$   \n",
    "$\\pi_i (1-p_i) = 1 \\cdot \\frac{\\lambda_i}{\\lambda} p_i$  \n",
    "$\\pi_i = \\frac{\\lambda_i}{\\lambda}\\frac{p_1}{1-p_1} = \\frac{\\lambda_i}{\\lambda}\\frac{\\frac{\\lambda}{\\lambda + \\mu_i}}{\\frac{\\mu_i}{\\lambda + \\mu_i}} =\\frac{\\lambda_i}{\\lambda}\\frac{\\lambda}{\\mu_i}=\\frac{\\lambda_i}{\\mu_i}$    \n",
    "\n",
    "and we compute a normalizing constant \n",
    "\n",
    "$c^{-1} = 1 + \\sum_{i=1}^n \\frac{\\lambda_i}{\\mu_i}$  \n",
    "\n",
    "hence we have a steady state vector of   \n",
    "\n",
    "$\\mathbf \\pi^T = c\\cdot\\begin{bmatrix}1\\\\ \n",
    "\\frac{\\lambda_1}{\\mu_1}\\\\ \n",
    "\\frac{\\lambda_2}{\\mu_2}\\\\ \n",
    "\\vdots\\\\ \n",
    "\\frac{\\lambda_n}{\\mu_n}\\\\  \n",
    "\\end{bmatrix}^T$\n",
    "\n",
    "**remark:**   \n",
    "$1 - \\pi_0 = \\sum_{i=1}^n \\pi_i= \\frac{\\sum_{i=1}^n\\frac{\\lambda_i}{\\mu_i}}{1+ \\sum_{i=1}^n\\frac{\\lambda_i}{\\mu_i}} = \\frac{E[R_1]}{E[X_1]}=\\lim_{t \\to \\infty}\\frac{E[R(t)]}{t}$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @numba.jit(nopython = True)\n",
    "def run_simulation_part_b(lambda_array, mu_array, r_trials):\n",
    "    # slightly different that typical, we have r_trials being the number of iterations / new arrivals to count\n",
    "    # the goal is to get an estimate of the steady state vector  \n",
    "    n = lambda_array.shape[0]\n",
    "    counter_array = np.zeros(n + 1)\n",
    "    cur_state = np.random.randint(1,n)\n",
    "    # starting at one is a bit odd, but call it a delayed renewal setup      \n",
    "    for _ in range(r_trials):\n",
    "#         assert(cur_state > 0)\n",
    "        departures_batch = np.random.exponential(1/mu_array, n) \n",
    "        arrivals_batch = np.random.exponential(1/lambda_array, n)         \n",
    "        # unfortunately these seem to be throwing errors with numba... \n",
    "        # https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.exponential.html\n",
    "        # recall that beta, the inverse lambda parameter is used here \n",
    "        fastest_arrival_index_shifted_up_one = np.argmin(arrivals_batch) + 1\n",
    "        if np.min(arrivals_batch) < departures_batch[cur_state - 1]:\n",
    "#         if arrivals_batch[cur_state - 1] <= departures_batch[cur_state - 1]:\n",
    "            counter_array[cur_state] += 1\n",
    "            # cur state doesn't change \n",
    "        else:\n",
    "            # i.e. the system has cleared and is empty when next arrival occurs, and arrival is accepted\n",
    "            counter_array[0] += 1\n",
    "            cur_state = fastest_arrival_index_shifted_up_one\n",
    "    return counter_array/r_trials    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.06209694  0.12936863  0.08538329  0.24838777  0.1108874   0.05979705\n",
      "  0.13971812  0.0326826   0.04657271  0.07516998  0.00993551]\n"
     ]
    }
   ],
   "source": [
    "# main simulation for part b\n",
    "n_arrival_types = 10\n",
    "lambdas = np.random.randint(2, 30, n_arrival_types)/2\n",
    "mu_s = np.random.randint(2, 30, n_arrival_types)/2\n",
    "big_lambda = np.sum(lambdas)\n",
    "\n",
    "steady_state_vec = np.zeros(n_arrival_types + 1)\n",
    "steady_state_vec[0] = 1\n",
    "steady_state_vec[1:] = lambdas / mu_s\n",
    "steady_state_vec *= 1/np.sum(steady_state_vec)\n",
    "\n",
    "#normalization \n",
    "print(steady_state_vec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(lambdas)\n",
    "print(mu_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if n_arrival_types == 3:\n",
    "    M = A.subs(l_1, lambdas[0]).subs(l_2, lambdas[1]).subs(l_3, lambdas[2])\n",
    "    M = M.subs(u_1, mu_s[0]).subs(u_2, mu_s[1]).subs(u_3, mu_s[2])\n",
    "    M = np.array(M).astype(np.float64)\n",
    "    ones_v = np.ones(4)\n",
    "    # np.linalg.matrix_power(M, 1000)\n",
    "    Q, R = np.linalg.qr(M - np.identity(4))\n",
    "    steady_state = Q[:,-1] \n",
    "    steady_state = steady_state / np.sum(steady_state)\n",
    "    steady_state\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0628    ,  0.13191   ,  0.08927667,  0.24676   ,  0.10855333,\n",
       "        0.06289333,  0.13163   ,  0.03188   ,  0.04952   ,  0.07578667,\n",
       "        0.00899   ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_result = run_simulation_part_b(lambdas, mu_s, 300000)\n",
    "sim_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16 \n",
    "This is of interest but the question, as written, does not feel complete.  An inhomogenous poisson process doesn't have stationary increments -- e.g. reference \n",
    "http://www.randomservices.org/random/poisson/Nonhomogeneous.html  \n",
    "or http://www.randomservices.org/random/poisson/Compound.html \n",
    "with $p_i(s)$ as the emdedded function.  Yet in the much too short treatment of Poissons in this chapter, stationary and independent increments are held out as being key for a Poisson.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recalling that \n",
    "$\\frac{d}{dt} \\int_{0}^t f(x)dx = f(t)$\n",
    "\n",
    "and an event $A$ positive integer $k$ arrivals occurred at time $t$ for $N_1(t)$, we can run through and modify the inductive argument from page 185 to see \n",
    "**show complementary CDF holds, which is the base case and gives us F_x**  \n",
    "\n",
    "\n",
    "\n",
    "$E\\Big[\\mathbb I_A\\Big] = E\\Big[E\\big[\\mathbb I_A\\big \\vert X_1\\big] \\Big] = \\int_0^t P\\big(N_1(t-x) =k-1\\big \\vert X_1 = x\\big) d F_x $  \n",
    "$= \\int_0^t \\frac{e^{-\\lambda \\int_0^{t-x} p_1(x)}p_1(x)\\lambda(t-x)^{k-1}}{(k-1)!} e^{-x \\lambda \\int_x^t p_1(x)} dx$  \n",
    "\n",
    "$= \\int_0^t \\frac{p_1(x)\\lambda(t-x)^{k-1}}{(k-1)!} e^{-\\lambda \\int_0^{t-x} p_1(t)}e^{- \\lambda \\int_x^t p_1(x)} dx$  \n",
    "$= \\int_0^t \\frac{p_1(x)\\lambda(t-x)^{k-1}}{(k-1)!} e^{-\\lambda( \\int_0^{t-x} p_1(x) +\\int_x^t p_1(x))} dx$  \n",
    "$= \\int_0^t \\frac{p_1(x)\\lambda(t-x)^{k-1}}{(k-1)!} e^{-\\lambda \\int_0^{t} p_1(x) } dx$  \n",
    "$= \\exp\\big(-\\lambda \\int_0^{t} p_1(x) dx\\big)\\lambda^k \\int_0^t \\frac{p_1(x)(t-x)^{k-1}}{(k-1)!}  dx$  \n",
    "** serrious cleanup is needed.... I think it should read** \n",
    "\n",
    "$P_1 = \\int_0^{t} p_1(x) dx$ \n",
    "so  \n",
    "$= \\exp\\big(-\\lambda P_1 dx\\big)(P_1\\lambda)^k \\int_0^t \\frac{(t-x)^{k-1}}{(k-1)!}  dx\\int_x^t p_i(s) ds $   \n",
    "** but even this runs into huge problems... something is very off in here and I'm not toally sure what or how to fix it... **  \n",
    "\n",
    "$= \\exp\\big(-\\lambda P_1 dx\\big)(P_1\\lambda)^k \\frac{t^k}{k!}$   \n",
    "$= \\exp\\big(-\\lambda P_1 dx\\big) \\frac{(P_1\\lambda t)^k}{k!}$   \n",
    "$\\text{Poisson probability of k arrivals at time t with paramenter } P_1\\lambda$  \n",
    "**But many things are still serriously off in here... *** \n",
    "\n",
    "- - -  - - \n",
    "\n",
    "\n",
    "**I think the above breaks, though...**  there should be the hazard rate inside the exponential function but it depends on x and isn't easily removable from the integral...  \n",
    "\n",
    "the above is close but needs cleaned up... furthermore the induction and integral bashing strikes me as quite unpleasant  \n",
    "\n",
    "**THERE ARE TONS OF THINGS WRONG WITH THE ABOVE UNFORTUNATELY... THE INHOMOGENOUS INTEGRAL SEEMS TO CAUSE A MAJOR PROBLEM HERE*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better approach:\n",
    "\n",
    "**lemma on Poisson splitting**: \n",
    "if we have a Poisson process that has (homogenous) probability $\\alpha$ of being accepted at each arrival -- i.e. a bernouli process layered into a Poisson -- then the acceptance 'stream' is a Poisson process with paramater \n",
    "$\\alpha \\lambda$  \n",
    "\n",
    "To prove this we want 3 things: 1) Poisson distributed, 2) stationary increments and 3) independent increments.  In fact it is enough to prove $2$ and $3$ as the Poisson distribution is the only continuous time process possessing 2 and 3-- so we then get 1 for free.  \n",
    "- - - -\n",
    "note: more machinery is involved in the background but an even easier approach is to recognize that the above process has a renewal function of $m_{\\alpha, \\lambda}(t) = \\alpha \\cdot \\lambda \\cdot t$ which is linear in $t$ (and $t$ varies continuously) which is unique to the poisson process over all renewal processes.  \n",
    "\n",
    "in particular a 'regular' poisson has expected value \n",
    "\n",
    "$E\\big[N(t)\\big] = m(t) = \\lambda t = e^{-\\lambda t} \\sum_{k=0}^\\infty \\frac{(\\lambda t)^k}{k!}\\cdot k= e^{-\\lambda t} \\sum_{k=1}^\\infty \\frac{(\\lambda t)^k}{k!}\\cdot k = e^{-\\lambda t} \\sum_{k=1}^\\infty \\frac{(\\lambda t)^k}{k!}\\big(\\sum_{i=1}^k 1\\big)$  \n",
    "\n",
    "when we embedd coin tossing on whether indidividual arrivals are accepted (with probabilty $\\alpha$) we get \n",
    "\n",
    "$E\\big[N_\\alpha(t)\\big] = m_\\alpha(t) = e^{-\\lambda t} \\sum_{k=1}^\\infty \\frac{(\\lambda t)^k}{k!}\\big(\\sum_{i=1}^k \\alpha\\big) = \\alpha \\Big( e^{-\\lambda t} \\sum_{k=1}^\\infty \\frac{(\\lambda t)^k}{k!}\\big(\\sum_{i=1}^k 1\\big)\\Big) = \\alpha \\Big(\\lambda t\\Big)$  \n",
    "\n",
    "or equivalently, where $J$ is poisson distributed and independent of the iid  $\\mathbb I_j$, we have \n",
    "\n",
    "$m_\\alpha(t) = \\alpha \\cdot \\lambda t = E\\big[\\sum_{j=0}^J \\mathbb I_j\\big] =  E\\big[\\mathbb I_j\\big]E\\big[J\\big]$  \n",
    "- - - - \n",
    "*A slightly longer approach:*  \n",
    "we can verify directly that   \n",
    "$P\\big(N_{\\alpha, \\lambda}(t) = 0\\big) = P(N(t) =0) + P(N(t) =1)(1-\\alpha) + P(N(t) =2)(1-\\alpha)^2 + ... $  \n",
    "$=P(N(t) =0)(1-\\alpha)^0 + P(N(t) =1)(1-\\alpha) + P(N(t) =2)(1-\\alpha)^2 + ... $  \n",
    "$=e^{-\\lambda t}\\sum_{k=0}^\\infty \\frac{((1-\\alpha)\\lambda t)^k}{k!}$  \n",
    "$=e^{-\\lambda t \\alpha} \\big(e^{-\\lambda t(1-\\alpha)}\\sum_{k=0}^\\infty \\frac{((1-\\alpha)\\lambda t)^k}{k!}\\big)$  \n",
    "$=e^{-\\lambda t \\alpha}$  \n",
    "\n",
    "but given the time homoegeneity of $\\alpha$ we can easily verify that memorylessness still holds in this 'residual Poisson' -- in effect re-running through the argument on page 186\n",
    "\n",
    "But this is overkill -- in the homogenous case we already have iid bernouli trials layer on with exponential arrival times.  Our expected time until next arrival conditioned on being at time $t+s$ is $=e^{-\\lambda s \\alpha}$ given the memorylessness of the underlying Poisson process and the above work gives us that the waiting time until the next arrival is exponentially distributed irrespective of whether there was an arrival at time $x$ and irrespective of whether that arrival was accepted or rejected.  Hence the process is entirely characterized the the exponential inter-arrival times with parameter $\\lambda \\alpha$, and re-using page 186 or other background material we *know* this is a Poisson counting process.  This proves in the homogenous case that Poisson splitting splits the process into other Poissons.  (Technical nit: it remains to verify that the splits are independent -- this is implied by the fact that the $\\alpha $ stream has countably many arrivals and hence for any $k$ arrivals in that stream we have the same probability estimate of poisson with $r$ arrivals and parameter $(1-\\alpha)\\lambda$.)  \n",
    "\n",
    "We thus know that \n",
    "$N_{\\alpha, \\lambda}(t) $ is Poisson distributed.  We skipped the most direct and cumbersome approach of verifying, which is to use convolutions and show \n",
    "\n",
    "$P\\big(N_{\\alpha, \\lambda}(t) = k\\big)= \\sum_{i= k}^\\infty P\\big(N(t) = i\\big)\\cdot \\alpha^k(1-\\alpha)^{i-k}\\binom{i}{k}= \\sum_{i= k}^\\infty P\\big(N(t) = i\\big)\\cdot \\text{Binomial}\\big(i,k,\\alpha\\big) $  \n",
    "\n",
    "we did this for the $k=0$ case and via memorylessnessness of the underlying process (Poisson and Bernouli -- which are both memoryless) we get the above for free.  \n",
    "\n",
    "\n",
    "*Now for the inhomogenous case*  \n",
    "\n",
    "the problem asks us to examine the case where the acceptance (/hazard) rate changes over time -- such that there is a probability of an arrival at time $s$ being classified as type 1 with probability $p_i(s)$ and that this results in a poisson distributed random variable with mean \n",
    "\n",
    "$E\\big[N_{\\alpha, \\lambda}(t) \\big]=\\lambda \\cdot \\alpha  t  = \\lambda \\int_0^t p_1(s)ds$  \n",
    "\n",
    "i.e. where $\\alpha := \\frac{1}{t}\\int_0^t p_1(s)$  \n",
    "\n",
    "what is interesting is if we use the results from problem 15 (or better: your author's long writeup in Gallagher folder on uniform rivals for Poissons conditioned on $i$ arrivals at time $t$) \n",
    "\n",
    "then we can find that conditoned on \n",
    "$P\\big(N(t) = i\\big)$ each arrivals is uniformly distributed in $[0,t]$ and has acceptance probability of $\\alpha = \\frac{1}{t}\\int_0^t p_1(s)$  \n",
    "\n",
    "hence re-using the convolutional approach, we can see \n",
    "\n",
    "$P\\big(N_{\\alpha, \\lambda}(t) = k\\big)= \\sum_{i= k}^\\infty P\\big(N(t) = i\\big)\\cdot \\text{Binomial}\\big(i,k, \\alpha\\big) $  \n",
    "\n",
    "as before. Thus the resulting random variable *is* Poisson distributed.  Note that we do lose stationarity with this approach.  (Lingering technical nit: we can re-run this for the $1-\\alpha$ case and verify that the two random variables are independent -- as before the point is that the acceptance /rejectance at each moment in time, while not stationary /  time homogenous, is independent of past actions and the duality of countable many arrivals in each stream in a continuous time setting, as well the underlying generator of arrivals being memoryless, means that knowledge of arrivals in one process does not change estimates of arrivals in another process.)  \n",
    "\n",
    "The very ending here could be tightened up a bit.  Overall this is a nice approach of first distilling as much as we can from a homogenous case and then using it to glean insights for an inhomogenous case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
