{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Wald's Equation and Renewal Theory\n",
    "\n",
    "** note this isn't actually from Feller but instead from page 422 of Ross's Intro To Probability Models and ties in with some exercises in Gallagher**  \n",
    "\n",
    "consider applying Wald's Equality where \n",
    "\n",
    "$0 \\lt X$  \n",
    "\n",
    "(recall that $X(\\omega)$ is a time denominated increment)  and $X_i$ are iid\n",
    "\n",
    "$\\bar{ X} \\lt \\infty$  \n",
    "\n",
    "and $S_n = X_1 + X_2 +... + X_n$  \n",
    "\n",
    "and with the stopping rule of stopping at the first cumulative arrival time $\\gt t$ \n",
    "\n",
    "**open item: considering doing this for $\\geq t$, the stopping rule is intact... the issue really seems to be notational** \n",
    "\n",
    "that is, we are considering \n",
    "\n",
    "$S_{N(t)+1}$  \n",
    "\n",
    "where $N(t)$ is the number of arrivals at time $t$ \n",
    "\n",
    "$S_{N(t)+1} = t  + Y(t)$  \n",
    "\n",
    "where $Y(t)$ is the excess at time $t$, i..e. the final $X_i$ arrial is partitioned into the amount up to and including $t$ and the amount after -- the latter is what's counted by $Y(t)$. For avoidance of doubt $Y(t)\\geq 0$ \n",
    "\n",
    "taking expectations of both sides, we have \n",
    "\n",
    "\n",
    "$E\\big[S_{N(t)+1}\\big] = E\\big[t  + Y(t)\\big] = E\\big[X_1 + X_2 +... + X_{N(t)} + X_{N(t)+1}\\big]$  \n",
    "\n",
    "$t + E\\big[ Y(t)\\big] = \\bar{X}E\\big[{N(t)+1}\\big] = \\bar{X}E\\big[{N(t)}\\big]+ \\bar{X}$  \n",
    "\n",
    "$E\\big[{N(t)}\\big]+ 1 = \\frac{t}{\\bar{X}} + \\frac{E\\big[ Y(t)\\big]}{\\bar{X}}$  \n",
    "\n",
    "$E\\big[{N(t)}\\big] = \\frac{t}{\\bar{X}} + \\frac{E\\big[ Y(t)\\big]}{\\bar{X}} - 1$  \n",
    "\n",
    "\n",
    "$\\frac{t}{\\bar{X}}  - 1 \\leq E\\big[{N(t)}\\big] = \\frac{t}{\\bar{X}} + \\frac{E\\big[ Y(t)\\big]}{\\bar{X}} - 1 = \\frac{t}{\\bar{X}} - 1 + \\frac{E\\big[ Y(t)\\big]}{\\bar{X}} $  \n",
    "\n",
    "sometimes the notation $m(t) :=  E\\big[{N(t)}\\big]$ is used instead.  This is then re-written as \n",
    "\n",
    "$\\frac{t}{\\bar{X}}  - 1 \\leq m(t) = \\frac{t}{\\bar{X}}  - 1 + \\frac{E\\big[ Y(t)\\big]}{\\bar{X}}$   \n",
    "\n",
    "**some important special cases** \n",
    "\n",
    "*infinite expected waiting time*\n",
    "we know that it may be possible for expected waiting times to be infinite, if the renewal does not have a second moment.  In such a case, by positivity, we can still be assured that the lower bound is valid.  As for the upper bound, we'd re-run the above argument, but on a truncated $\\hat{X}$, and take limits as needed. Carefully pursuing this (as in Ross, Gallagher, or Feller) then recovers the elementary renewal theorem.  \n",
    "- - - -\n",
    "Of more interest for this writeup are caes where $E\\big[ Y(t)\\big]$ is finite *and* we may estimate it or find a useful upper bound for it. This motivates cases of  \n",
    "\n",
    "\n",
    "*monotonocity in a graph *  \n",
    "\n",
    "note: for *many* special cases of interest we may be about to bound $E\\big[ Y(t)\\big]$ \n",
    "\n",
    "In a particular special case of interest -- when there is *monotonicity* in the graph / underlying countable state markov chain, we **know** that any possible renewal from the starting node (call it 0)  must go through all other nodes, which gives a dominance relation of \n",
    "\n",
    "$\\{\\text{time from 0 }  \\to \\text{ k}\\} + \\{\\text{time from k }  \\to \\text{0}\\} = \\{\\text{time from 0 }  \\to \\text{ 0}\\} $  \n",
    "\n",
    "for $k \\gt 0$  \n",
    "\n",
    "\n",
    "over any legal sample path, and for avoidance of doubt, both quantities being time denominated are real non-negative.  \n",
    "\n",
    "taking expectations we have \n",
    "\n",
    "$E\\big[\\text{time from 0 }  \\to \\text{ k} \\big] + E\\big[\\text{time from k}  \\to \\text{ 0}\\big] = E\\big[\\text{time from 0 }  \\to \\text{ 0}\\big] = \\bar{X} $  \n",
    "\n",
    "$E\\big[\\text{time from 0 }  \\to \\text{ k} \\big] \\leq \\bar{X} $  \n",
    "\n",
    "Thus we can write \n",
    "\n",
    "$E\\big[ Y(t)\\big] = \\sum_{k\\gt 0} p_k E\\big[\\text{time from 0 }  \\to \\text{ k} \\big] \\leq \\sum_{k\\gt 0} p_k \\bar{X} = \\bar{X} \\sum_{k\\gt 0} p_k \\leq \\bar{X} $  \n",
    "\n",
    "where $p_k$ denotes the probability of being in state $k$ at the stopping time.  By definition we have \n",
    "\n",
    "$0 \\leq \\sum_{k\\gt 0} p_k \\leq  p_0 + \\sum_{k\\gt 0} p_k = \\sum_{k\\geq 0} p_k  = 1$  \n",
    "\n",
    "- - - - \n",
    "\n",
    "In these cases of monotonicity, we may then say:  \n",
    "\n",
    "\n",
    "$\\frac{t}{\\bar{X}}  - 1 \\leq m(t) = \\frac{t}{\\bar{X}} -1 + \\frac{E\\big[ Y(t)\\big]}{\\bar{X}} \\leq \\frac{t}{\\bar{X}}-1 + \\frac{\\bar{X}}{\\bar{X}}  = \\frac{t}{\\bar{X}} -1 +1 = \\frac{t}{\\bar{X}}$   \n",
    "\n",
    "So we may conclude that \n",
    "\n",
    "$\\frac{t}{\\bar{X}}  - 1 \\leq m(t) \\leq \\frac{t}{\\bar{X}}$    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternating Renewal Theorem:  **  \n",
    "\n",
    "Suppose that we have iid $X_n$ and iid $Y_n$ where $X_n$ denotes \"on\" time and $Y_n$ is off time. Each random variable denotes an arrival time, and they come in sequence -- i.e. first always $X_i$ then $Y_i$ then $X_{i+1}$ then $Y_{i+1}$ then $X_{i+2}$ and so on.  Further suppose that each random variable has a first moment.  \n",
    "\n",
    "Thus $Z_n := X_n + Y_n$  is a renewal process that starts with the system off, 'turns on' with the arrival denoted by $X$ and then turns off again at the $Y$ arrival -- which constitutes a renewal.  \n",
    "\n",
    "An interesting result: \n",
    "\n",
    "$\\lim_{t \\to \\infty} P\\big[\\text{system is \"on\" at time t}\\big]= \\frac{\\bar{X}}{\\bar{X} + \\bar{Y}} = \\frac{E\\big[X_1\\big]}{E\\big[X_1 + Y_1\\big] }$  \n",
    "\n",
    "\n",
    "we suggest two different proofs / ways of deriving this result.  One using the Renewal Equilibrium theorem (variously called Feller-Erdos-Pollards for arithmetic/lattice cases and Blackwell's theorem for non-arithmetic case), and the other approach is to find that the above conforms to the time averaged result -- hence if there is a limmitting distribution / equilibrium for $Z$ then the above relationship must follow.  (As always the time averaged case has an interpretation in terms problems of 'random incidence' for a finite but sufficiently large $t$.)  \n",
    "\n",
    "\n",
    "**1.)** \n",
    "\n",
    "Approach one: using the key limit theorem, we know the CDF of the age (**2x check on residual life**) is given by \n",
    "\n",
    "$\\lim_{t \\to \\infty} Z_{(t)}\\big(z\\big) =\\frac{Pr\\{Z \\gt (z)\\} }{E\\big[Z_1\\big]} = \\frac{Pr\\{Z \\gt (z)\\} }{E\\big[X_1 + Y_1\\big]} = \\frac{Pr\\{Z \\gt (z)\\} }{E\\big[X_1\\big] + E\\big[Y_1\\big]}$  \n",
    "\n",
    "of course if we integrate over $z$ from 0 to $\\infty$ we get a \n",
    "\n",
    "$\\frac{\\int_0^{\\infty} Pr\\{Z \\gt (z)\\}dz }{E\\big[X_1\\big] + E\\big[Y_1\\big]} = \\frac{E\\big[Z_1\\big] }{E\\big[X_1\\big] + E\\big[Y_1\\big]} =1 $\n",
    "\n",
    "which confirms that this is a valid probability distribution.  \n",
    "\n",
    "However, we can further refine this.  For convenience note that \n",
    "\n",
    "$E\\big[\\mathbb I_{Z_i \\gt z}\\big] = E\\big[\\mathbb I_{Z \\gt z}\\big] = Pr\\{Z \\gt (z)\\} $  \n",
    "\n",
    "The indicator on the left can be further decomposed, taking advantage of the sequential ordering of arrivals.  \n",
    "\n",
    "$\\mathbb I_{Z_i \\gt z} = \\mathbb I_{X_i \\gt z} + \\mathbb I_{\\text{convolution :} X\\leq z * Y \\gt (z-x)}$  \n",
    "\n",
    "taking expectations we have \n",
    "\n",
    "$ Pr\\{Z \\gt (z)\\} = E\\big[\\mathbb I_{Z_i \\gt z}\\big] = E\\big[\\mathbb I_{X_i \\gt z}\\big] + E\\big[\\mathbb I_{\\text{convolution :} X\\leq z * Y \\gt (z-x)}\\big] =  Pr\\{X \\gt (z)\\}  + E\\big[\\mathbb I_{\\text{convolution :} X\\leq z * Y \\gt (z-x)}\\big]$  \n",
    "\n",
    "Now if we only wanted the portion of time that is \"on\", that is given directly by $Pr\\{X \\gt (z)\\}$ -- and if integrate over this, we'd have 'summed' over all probabilities of being on in this equilibrium state, which is given below as:  \n",
    "\n",
    "\n",
    "$\\frac{\\int_0^{\\infty} Pr\\{X \\gt (z)\\} dz}{E\\big[X_1\\big] + E\\big[Y_1\\big]} = \\frac{E\\big[X_1\\big] }{E\\big[X_1\\big] + E\\big[Y_1\\big]} = \\frac{\\bar{X}}{\\bar{X} + \\bar{Y}}$  \n",
    "\n",
    "as desired \n",
    "\n",
    "\n",
    "**2.)** \n",
    "\n",
    "Time averaged approach: As before, we assume each $X_i$ and $Y_i$ have a first moment.  Further to cut to the chase for this argument, assume the random variables have a second moment (or if desired: are  bounded-- which easily sets this up for a truncation argument if needed for full generality)\n",
    "\n",
    "Use the argument at the top of this workbook with a stopping trial that stops at the first $Z$ arrival after some time $t$ and apply Wald's Equality.  However, note that we are modelling a slightly different result, where we have:  \n",
    "\n",
    "$S_n := Z_1 + Z_2 +... + Z_n$  \n",
    "\n",
    "$E\\big[S_{N(t)+1}\\big] = E\\big[t  + R_L(t)\\big] = E\\big[Z_1 + Z_2 +... + Z_{N(t)} + Z_{N(t)+1}\\big] = \\bar{Z}\\big(N(t)\\big) = \\bar{Z}\\cdot m(t)$ \n",
    "\n",
    "where $R_L(t)$ denotes the residual life as a function of time (formerly this was $Y(t)$ though we have a notational overload.)  \n",
    "\n",
    "if we further re-run the argument as before, we get:  \n",
    "\n",
    "$\\frac{t}{\\bar{Z}}  - 1 \\leq m(t) = \\frac{t}{\\bar{Z}}  - 1 + \\frac{E\\big[ R_L(t)\\big]}{\\bar{Z}}$   \n",
    "\n",
    "dividing by $t$  \n",
    "$\\frac{1}{\\bar{Z}}  - \\frac{1}{t} \\leq \\frac{m(t)}{t} = \\frac{1}{\\bar{Z}}  - \\frac{1}{t} + \\frac{E\\big[ R_L(t)\\big]}{\\bar{Z}\\cdot t} \\leq \\frac{1}{\\bar{Z}} + \\frac{E\\big[ R_L(t)\\big]}{\\bar{Z}\\cdot t}$   \n",
    "\n",
    "taking advantage of our assumption (for convenience) that $E\\big[ R_L(t)\\big]$ is bounded we look at the limitting value as $t \\to \\infty$ and see \n",
    "\n",
    "$\\frac{1}{\\bar{Z}} \\leq \\frac{m(t)}{t} \\leq \\frac{1}{\\bar{Z} }  $   \n",
    "\n",
    "as expected.  \n",
    "\n",
    "*However* we are actually interested in estimating $p$ which is the long-run portion of time that the system is \"on\".  So we set up a stopping trial \n",
    "\n",
    "$p(t)\\cdot S_{N(t)+1} = X_1 + X_2 + ... + X_{N(t)}+ X_{N(t)+1}$ \n",
    "\n",
    "where $p(t) := \\frac{X_1 + X_2 + ... + X_{N(t)}+ X_{N(t)+1}}{Z_1 + Z_2 + ... + Z_{N(t)}+ Z_{N(t)+1}}$   \n",
    "\n",
    "for avoidance of doubt: the trials stop on arrival $Z_{N(t)+1}$ -- so there cannot be a 'mismatch' between the $X$'s and $Z$'s -- we always stop on the first renewal after time $t$.  \n",
    "\n",
    "in words: $p(t)$ is distribution of the portion of time that the system is 'on', as a function of time.   It can be a touch confusing since it is a random variable that reflects proportions / probabilities for other random variables.  But ultimately $p(t)$ is just another a random variable taking on values $\\in [0,1]$ -- and it has a CDF which gives the probability of $p(t)$ being less than $x$ for some chosen time $t$. Note that this means for some time $t$ it also has a complementary CDF of $Pr\\{p(t) \\gt x\\}$ and if we integrate over this, we'd recover the expected portion amount of time that the system is on at some time $t$.  While these CDFs exist, they are not easy to tackle head on.  The same holds for the expected value of $p(t)$.  Instead we will be estimating it via setting up a stopping trial and applying Wald's Equality, and with an ultimate goal of passing a limit.  \n",
    "\n",
    "Ultimately we know want to find the probability as $t \\to \\infty$ where we simply show $p = p(\\infty) $.  Technically we are looking at the expected value of this limmitting probability \n",
    "\n",
    "-- as a purely technical point note:  like any probability, $p(t)$ is bounded in $[0,1]$  so if needed we may use bounded convergence theorem here to justify that \n",
    "\n",
    "$p = E\\big[\\lim_{t \\to \\infty}p(t)\\big] = \\lim_{t \\to \\infty}E\\big[p(t)\\big]$\n",
    "\n",
    "- - - - \n",
    "\n",
    "Now we re-run our argument and see:  \n",
    "\n",
    "\n",
    "$E\\big[p(t)\\cdot S_{N(t)+1}\\big] = E\\big[p(t)\\cdot \\big(t  + R_L(t)\\big)\\big]= E\\big[p(t)\\cdot t  + p(t) \\cdot R_L(t)\\big] = E\\big[X_1 + X_2 +... + X_{N(t)} + X_{N(t)+1}\\big] = \\bar{X}\\cdot E\\big[N(t) +1 \\big] $ \n",
    "\n",
    "$= t\\cdot E\\big[p(t)\\big] + E\\big[p(t) \\cdot R_L(t)\\big] =   \\bar{X}\\cdot m(t) +\\bar{X}$  \n",
    "\n",
    "re-arranging terms:   \n",
    "$t\\cdot E\\big[p(t)\\big] + E\\big[p(t) \\cdot R_L(t)\\big]- \\bar{X} =  \\bar{X}\\cdot m(t) $  \n",
    "\n",
    "dividing by $\\bar{X}$  \n",
    "\n",
    "\n",
    "$t \\cdot \\frac{ E\\big[p(t)\\big]}{\\bar{X}}  - 1  \\leq t \\cdot \\frac{ E\\big[p(t)\\big]}{\\bar{X}} + \\frac{E\\big[p(t) \\cdot R_L(t)\\big]}{\\bar{X}} -1 =   m(t) \\leq t \\cdot \\frac{ E\\big[p(t)\\big]}{\\bar{X}} + \\frac{E\\big[p(t) \\cdot R_L(t)\\big]}{\\bar{X}}\\leq t \\cdot \\frac{ E\\big[p(t)\\big]}{\\bar{X}} + \\frac{E\\big[ R_L(t)\\big]}{\\bar{X}}   $  \n",
    "\n",
    "recalling that $p(t)$ is bounded between 0 and 1 and $R_L(t)$ is real non-negative \n",
    "\n",
    "dividing by $t$ we have \n",
    "\n",
    "$ \\frac{ E\\big[p(t)\\big]}{ \\bar{X}}  - \\frac{1}{t}  \\leq  \\frac{ m(t)}{t}  \\leq   \\frac{ E\\big[p(t)\\big]}{\\bar{X}} + \\frac{ E\\big[R_L(t)\\big]}{\\bar{t \\cdot X}}   $ \n",
    "\n",
    "passing limits as before, and taking advantage of boundedness we see as $t \\to \\infty$  \n",
    "\n",
    "$ \\frac{ E\\big[p(t)\\big]}{ \\bar{X}} = \\frac{ p}{ \\bar{X}}   \\leq  \\frac{ m(t)}{t} = \\frac{1}{\\bar{Z} }  \\leq   \\frac{ E\\big[p(t)\\big]}{\\bar{X}} =\\frac{ p}{\\bar{X}}  $ \n",
    "\n",
    "Where we made the subsitution in the middle from the argument that began this second approach. \n",
    "- - - -  \n",
    "\n",
    "We thus see that in the limit, we have a condition where:  \n",
    "\n",
    "$\\frac{1}{\\bar{Z} }  =  \\frac{ p}{\\bar{X}} $\n",
    "\n",
    "or \n",
    "\n",
    "$p = \\frac{\\bar{X}}{\\bar{Z} }  =   \\frac{\\bar{X}}{\\bar{X} + \\bar{Y} } $  \n",
    "\n",
    "as desired  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**follow-up: not just the limitting value-- I want to show that residual life in general must be greater than the expected renewal rate... some inequalities on this are in 2 of Ross's texts** \n",
    "\n",
    "\n",
    "maybe set this up as a point-wise bound using Indicator random variables... \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6\n",
      "   0.6  0.6]\n",
      " [ 0.4  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.4]\n",
      " [ 0.   0.4  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.4  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.4  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.4  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.4  0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.4  0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.4  0.   0.   0.   0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.4  0.   0.   0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.4  0.   0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.4  0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.4  0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.4  0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.4  0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.4\n",
      "   0. ]] \n",
      "\n",
      "[[ 0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6  0.6\n",
      "   0.6  1. ]\n",
      " [ 0.4  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.4  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.4  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.4  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.4  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.4  0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.4  0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.4  0.   0.   0.   0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.4  0.   0.   0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.4  0.   0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.4  0.   0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.4  0.   0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.4  0.   0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.4  0.\n",
      "   0. ]\n",
      " [ 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.4\n",
      "   0. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# python 3.x\n",
    "\n",
    "m = 16\n",
    "p = 0.4\n",
    "\n",
    "A = np.zeros((m,m))\n",
    "B = np.zeros((m,m))\n",
    "\n",
    "A[0,-1]= 1-p\n",
    "A[1,-1]= p\n",
    "\n",
    "B[0,-1]= 1\n",
    "##\n",
    "A[0,0] = 1-p\n",
    "A[1,0] = p\n",
    "B[0,0] = 1-p\n",
    "B[1,0] = p\n",
    "\n",
    "\n",
    "for k in range(0, m-1):\n",
    "    A[k+1,k] = p\n",
    "    A[0,k] = 1-p\n",
    " \n",
    "    B[k+1,k] = p\n",
    "    B[0,k] = 1-p\n",
    "\n",
    "print(A,\"\\n\")\n",
    "print(B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2212203404772941"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_trials = 5000000\n",
    "\n",
    "x = np.zeros(A.shape[0])\n",
    "x[0] = 1\n",
    "\n",
    "onesv = np.ones(A.shape[0])\n",
    "counter = 0 \n",
    "\n",
    "for _ in range(t_trials):\n",
    "    b = A @ x\n",
    "    counter += b[-1] \n",
    "    x = b\n",
    "#     print(b)\n",
    "    \n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1552206.7637444327"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/(counter/t_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1552202.6243591295"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xbar = 0\n",
    "for k in range(1, m):\n",
    "    Xbar += 1/p**k\n",
    "Xbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1552202.6243591297"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-p**(m-1))/((1-p)*p**(m-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2212289307682305"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_trials/Xbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More interesting cases of Monotonicity in Graphs \n",
    "\n",
    "Some bounds on second largest eigenvalue of an absorbing state markov chain \n",
    "\n",
    "referencing the two different forms of Collatz-Wielandt formula for Positive matrices (on p 667 - 669 of Meyers *Matrix Analysis*)  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scipy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9bb4cdd25091>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschur\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scipy' is not defined"
     ]
    }
   ],
   "source": [
    "T, Z = scipy.linalg.schur(A)\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.conj(Z).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1/ 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "value = 0.250184\n",
    "# value = 1/4\n",
    "running_sum = 0\n",
    "for i in range(1, 5+1):\n",
    "    running_sum += i*3*value**i\n",
    "running_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1/(1-value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "0.25/0.250184"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the below is really more in line with the Markov Chains and Matrix Theory but still closely tied in with the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "16*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta = 1/2\n",
    "\n",
    "x = np.array([theta, theta**2, theta**3, theta**4, theta**5, theta**6, 2*theta**7])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = np.arange(x.shape[0]) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter@ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nfl scores problem\n",
    "# 2, 3, 6, 7, 8\n",
    "# 1, 4, 5 are not legal\n",
    "\n",
    "C = np.zeros((8,8))\n",
    "for k in range(8-1):\n",
    "    C[k+1, k] = 1\n",
    "C[0] += 1\n",
    "\n",
    "C[0,0] = 0\n",
    "C[0,4] = 0\n",
    "C[0,3] = 0\n",
    "\n",
    "C\n",
    "\n",
    "print(C)\n",
    "\n",
    "x = np.zeros(8)\n",
    "\n",
    "x[8-2] = 1\n",
    "# score of 2\n",
    "x[8-3] = 1\n",
    "# score of 3\n",
    "x[8-6] = 3\n",
    "# score of 6\n",
    "x[8-7] = 2\n",
    "# score of 7 \n",
    "x[8-8] = 4\n",
    "# score of 8\n",
    "print(x,\"\\n\")\n",
    "\n",
    "for _ in range(30):\n",
    "    b = C @ x\n",
    "    print(x@ b / np.linalg.norm(x)) \n",
    "    print(b,\" \\n\")\n",
    "    x = b\n",
    "#     x = x/np.linalg.norm(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1/0.68233"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "C = np.zeros((5,5))\n",
    "for k in range(5-1):\n",
    "    C[k+1, k] = 1\n",
    "C[0] += 3\n",
    "C\n",
    "eigs = np.linalg.eigvals(C)\n",
    "\n",
    "# C *= 1/abs(eigs[0])\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.linalg.matrix_power(C, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x = np.random.random(5)\n",
    "# x = x / np.linalg.norm(x)\n",
    "x = np.zeros(5)\n",
    "x[0] = 1\n",
    "t_iterations = 100\n",
    "\n",
    "for _ in range(t_iterations):\n",
    "    b = C @ x\n",
    "#     print( np.linalg.norm(b)/np.linalg.norm(x)  )\n",
    "#     x = b/ np.linalg.norm(x)\n",
    "    x = b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the idea then is we want to select some\n",
    "$\\delta \\gt 0$ \n",
    "\n",
    "where for any $\\theta \\in (0,1)$ we still have $0 \\lt \\theta + \\delta \\lt 1$  \n",
    "\n",
    "such that \n",
    "\n",
    "$\\frac{1 - (\\theta + \\delta)^6}{1-(\\delta +\\theta)} \\gt \\frac{1}{1-\\theta}$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta = (- 1 + np.sqrt(5))/ 2\n",
    "\n",
    "C = np.array([[theta, theta**2],[1,0]])\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = theta + 2*theta**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1/mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.linalg.matrix_power(C, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: this comes up again on page 443, XVI -- for a markov chain application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On infinite renewals for non-defective renewal processes  \n",
    "\n",
    "see: my notes under problem 2.28 in chp2 notes to Gallagher's Stochastic Processes \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**comment:**  The problem's result is ultimately clear.  However it feels almost like a trick amongst identities of Generating Functions.  The problem is contained in the 'answers' section in the back, though it took your author a considerable amount of time to parse those answers and recover the missing steps to make things coherent.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Results:**   \n",
    "\n",
    "$U(s) = \\frac{1}{1 - F(s)} = \\sum_{k=0}^\\infty u_k s^k$  \n",
    "this is (3.2) on page 311\n",
    "\n",
    "recall per page 308, that $\\{u_k\\}$ are the probability of there being renewal at $k$ -- for non-transient setups, this sums to $\\infty$ so cannot be a probability.  but the series converges for $\\vert s\\vert \\lt 1$ (recall chapter XI).  \n",
    "\n",
    "The answers in the back note follwing identities\n",
    "\n",
    "(a) $1-F(s) = (1-s) Q(s)$ (to be used first)   \n",
    "(b)$\\bar{X} - Q(s) = (1-s)R(s)$ (to be used a bit later)    \n",
    "\n",
    "(note I chose to use $\\bar{X}$ not $\\mu$ here to avoid visual confusiton with the $u_k$'s)  \n",
    "\n",
    "**Goal: **\n",
    "\n",
    "assuming we have a second moment, show \n",
    "\n",
    "(1.) $\\sum_{k=0}^\\infty \\big(u_k - \\frac{1}{\\bar{X}}\\big)s^k = \\frac{R(s)}{\\bar{X}Q(s)}$  \n",
    "\n",
    "for any $s \\in (0,1)$ \n",
    "\n",
    "(2.) then show it in fact holds for $s:=1$ as well, with no convergence issues.  and at s = 1 \n",
    "(or as I prefer, taking the limit as $s$ approach $1$ from the left)  \n",
    "\n",
    "\n",
    "$\\sum_{k=0}^\\infty \\big(u_k - \\frac{1}{\\bar{X}}\\big)= \\frac{\\sigma^2 - \\bar{X} + \\bar{X}^2}{2 \\bar{X}^2}$ $= \\frac{E\\big[X^2\\big] - \\bar{X}}{2\\bar{X}^2}$  \n",
    "\n",
    "note: the above is interpretted as the total lifetime difference between the probability of a renewal at a particular time, and the steady state probability of renewal at that time \n",
    "\n",
    "**Proof:**  \n",
    "\n",
    "Starting with (1.), we build the relation we want starting with $s\\in(0,1)$ where we have absolute convergence \n",
    "\n",
    "$\\sum_{k=0}^\\infty \\big(u_k - \\frac{1}{\\bar{X}}\\big)s^k = \\big(\\sum_{k=0}^\\infty u_k s^k\\big) -  \\big(\\sum_{k=0}^\\infty \\bar{X}^{-1}s^k \\big) = \\big(U(s)\\big) -\\big(\\frac{\\bar{X}^{-1}}{1-s} \\big) = \\big(\\frac{1}{1 - F(s)}\\big) -\\frac{\\bar{X}^{-1}}{1-s} = \\frac{1}{(1-s) Q(s)} -\\frac{1}{(1-s)\\bar{X}}$ \n",
    "\n",
    "where we used identity (a) on the Right Hand Side \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\sum_{k=0}^\\infty \\big(u_k - \\frac{1}{\\mu}\\big)s^k = \\frac{1}{(1-s) Q(s)} -\\frac{1}{(1-s)\\bar{X}} = \\frac{\\bar{X}}{(1-s) \\bar{X}Q(s)} -\\frac{Q(s)}{(1-s)\\bar{X}Q(s)}= \\frac{\\bar{X} - Q(s)}{(1-s)\\bar{X}Q(s)} = \\frac{(1-s)R(s)}{(1-s)\\bar{X}Q(s)}= \\frac{R(s)}{\\bar{X}Q(s)}$ \n",
    "\n",
    "where we used identity (b), on the Right hand side, and 'divided out' $(1-s)$, an operation which is valid for any $s \\neq 1$  \n",
    "\n",
    "(Note: official solution uses this reduced form to make claims directly about the LHS and RHS at $s =1$... the legacy of the $(1-s)$ that *was* there in my derivation makes me a touch concerned, and instead I prefer to show the relation is true for $1 - \\delta$ small positive $\\delta$.  There might be a slightly simpler or more clean approach to get to these results. As a related point, I have lingering concerns about how I explicitly split the geometric series above -- this is fine as we have absolute convergence for $s = 1 - \\delta$, but techincally that intermediate step doesn't stand on its own if we re-examine it at $s=1$.)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This completes part (1.)\n",
    "\n",
    "\n",
    "- - - -\n",
    "part (2.)  \n",
    "\n",
    "we recall that $Q(1) = \\bar{X}$  (i.e. summing / integrating over the complementary CDF) thus we also know \n",
    "\n",
    "$\\lim_{s \\to 1^{-}}Q(s) = Q(1) = \\bar{X}$  \n",
    "\n",
    "(i.e. approach one as a limit from the left)  \n",
    "\n",
    "and we also recall that \n",
    "\n",
    "$R(1) = \\frac{\\sigma^2 - \\bar{X} + \\bar{X}^2}{2} = \\frac{E\\big[X^2\\big] - \\bar{X}}{2}$   \n",
    "\n",
    "since we have a second moment by assumption, this exists and is finite \n",
    "\n",
    "thus\n",
    "\n",
    "$\\lim_{s \\to 1^{-}}R(s) = \\frac{\\sigma^2 - \\bar{X} + \\bar{X}^2}{2} = \\frac{E\\big[X^2\\big] - \\bar{X}}{2}$  \n",
    "\n",
    "This is discussed and shown in multiple ways in my Absorbing State Markov Chains writeup.  It is also discussed and shown directly on page 266 (i.e. beginning of XI, discussing generating functions).  \n",
    "\n",
    "Finally \n",
    "\n",
    "$\\lim_{s \\to 1^{-}} \\sum_{k=0}^\\infty \\big(u_k - \\frac{1}{\\mu}\\big)s^k =  \\lim_{s \\to 1^{-}}\\frac{R(s)}{\\bar{X}Q(s)} =  \\frac{\\sigma^2 - \\bar{X} + \\bar{X}^2}{2\\bar{X}^2} =  \\frac{E[X^2] - \\bar{X}}{2\\bar{X}^2}$   \n",
    "\n",
    "note: the book makes a slightly stronger claim at the end and states\n",
    "\n",
    "$\\sum_{k=0}^\\infty \\big(u_k - \\frac{1}{\\mu}\\big) = RHS$  \n",
    "\n",
    "i.e. it directly evaluates the LHS at $s=1$.  There are some analytic subtleties in the problem and how I approached it that I don't think I fully grasp, hence I am content to evaluate instead approaching the limit from the left.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
